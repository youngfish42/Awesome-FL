# Federated-Learning-on-Graph-and-Tabular-Data

[![Stars](https://img.shields.io/github/stars/youngfish42/Awesome-Federated-Learning-on-Graph-and-Tabular-Data.svg?color=orange)](https://github.com/youngfish42/Awesome-Federated-Learning-on-Graph-and-Tabular-Data/stargazers) [![Awesome](https://awesome.re/badge-flat.svg)](https://awesome.re) [![License](https://img.shields.io/github/license/youngfish42/Awesome-Federated-Learning-on-Graph-and-Tabular-Data.svg?color=green)](https://github.com/youngfish42/image-registration-resources/blob/master/LICENSE) ![](https://img.shields.io/github/last-commit/youngfish42/Awesome-Federated-Learning-on-Graph-and-Tabular-Data)

---

**Table of Contents**

- [Papers](#Papers)
  - [FL on Graph Data and Graph Neural Networks](#FL-on-Graph-Data-and-Graph-Neural-Networks) [![dblp](https://img.shields.io/badge/dynamic/json?label=dblp&query=%24.result.hits[%27%40total%27]&url=https%3A%2F%2Fdblp.org%2Fsearch%2Fpubl%2Fapi%3Fq%3DFederated%2520graph%257Csubgraph%257Cgnn%26format%3Djson%26h%3D1000)](https://dblp.uni-trier.de/search?q=Federated%20graph%7Csubgraph%7Cgnn) 
  - [FL on Tabular Data](#FL-on-Tabular-Data) [![dblp](https://img.shields.io/badge/dynamic/json?label=dblp&query=%24.result.hits[%27%40total%27]&url=https%3A//dblp.org/search/publ/api%3Fq%3Dfederate%2520tree%257Cboost%257Cbagging%257Cgbdt%257Ctabular%257Cforest%257CXGBoost%26format%3Djson%26h%3D1000)](https://dblp.org/search?q=federate%20tree%7Cboost%7Cbagging%7Cgbdt%7Ctabular%7Cforest%7CXGBoost)
  - [FL in top-tier journal](#FL-in-top-tier-journal)
  - FL in top-tier conference and journal by category
    - [AI](#FL-in-top-AI-conference-and-journal) [ML](#FL-in-top-ML-conference-and-journal) [DM](#FL-in-top-DM-conference-and-journal) [Secure](#FL-in-top-Secure-conference-and-journal) [CV](#FL-in-top-CV-conference-and-journal) [NLP](#FL-in-top-NLP-conference-and-journal) [IR](#FL-in-top-IR-conference-and-journal) [DB](#FL-in-top-DB-conference-and-journal) [Network](#FL-in-top-Network-conference-and-journal) [System](#FL-in-top-System-conference-and-journal) 
- [Framework](#Framework)
- [Datasets](#Datasets)
- [Tutorials](#Tutorials)
- Key Conferences/Workshops/Journals
  - [Workshops](#Workshops) [Special Issues](#Journal-Special-Issues) [Special Tracks](#Conference-Special-Tracks)




# Papers

**Categories**

- Artificial Intelligence (IJCAI, AAAI, AISTATS)
- Machine Learning (NeurIPS, ICML, ICLR, COLT, UAI)
- Data Mining (KDD, WSDM)
- Secure (S&P, CCS, USENIX Security, NDSS)
- Computer Vision (ICCV, CVPR, ECCV, MM)
- Natural Language Processing (ACL, EMNLP, NAACL, COLING)
- Information Retrieval (SIGIR)
- Database (SIGMOD, ICDE, VLDB)
- Network (SIGCOMM, INFOCOM, MOBICOM, NSDI, WWW)
- System (OSDI, SOSP, ISCA, MLSys, TPDS) 



**keywords**

Statistics: :fire: code is available & stars >= 100 | :star: citation >= 50 | :mortar_board: Top-tier venue 

**`kg.`**: Knowledge Graph |   **`data.`**: dataset  |   **`surv.`**: survey





**Update log**

 ![](https://img.shields.io/github/last-commit/youngfish42/Awesome-Federated-Learning-on-Graph-and-Tabular-Data)

- *2022/09/06 -  Add information about FL on Tabular and Graph data*
- *2022/09/05 -  Add some information about top journals and add TPDS papers*
- *2022/08/31 - All papers (including 400+ papers from top conferences and top journals and 100+ papers with graph and tabular data) have been comprehensively sorted out, and information such as publication addresses, links to preprints and source codes of these papers have been compiled. The source code of 280+ papers has been obtained. We hope it can help those who use this project.*  :smiley:
- *2022/07/31 -  Add VLDB papers*
- *2022/07/30 -  Add top-tier system conferences papers and add COLT,UAI,OSDI, SOSP, ISCA, MLSys, AISTATS,WSDM papers*
- *2022/07/28 -  Add a list of top-tier conferences papers and add IJCAI,SIGIR,SIGMOD,ICDE,WWW,SIGCOMM.INFOCOM,WWW papers*
- *2022/07/27 -  add some ECCV 2022 papers*
- *2022/07/22 -  add CVPR 2022 and MM 2020,2021 papers*
- *2022/07/21 - give TL;DR and interpret information(解读) of papers. And add KDD 2022 papers*
- *2022/07/15 - give a list of papers in the field of federated learning in top NLP/Secure conferences. And add ICML 2022 papers*
- *2022/07/14 - give a list of papers in the field of federated learning in top ML/CV/AI/DM conferences from  [innovation-cat](https://github.com/innovation-cat)‘s [Awesome-Federated-Machine-Learning](https://github.com/innovation-cat/Awesome-Federated-Machine-Learning) and find :fire:  papers(code is available & stars >= 100)*
- *2022/07/12 - added information about the last commit time of the federated learning open source framework (can be used to determine the maintenance of the code base)*
- *2022/07/12 - give a list of papers in the field of federated learning in top journals*
- *2022/05/25 - complete the paper and code lists of FL on tabular data and Tree algorithms*
- *2022/05/25 - add the paper list of FL on tabular data and Tree algorithms*
- *2022/05/24 - complete the paper and code lists of FL on graph data and Graph Neural Networks*
- *2022/05/23 - add the paper list of FL on graph data and Graph Neural Networks*
- *2022/05/21 - update all of Federated Learning Framework*



## FL on Graph Data and Graph Neural Networks 

[![dblp](https://img.shields.io/badge/dynamic/json?label=dblp&query=%24.result.hits[%27%40total%27]&url=https%3A%2F%2Fdblp.org%2Fsearch%2Fpubl%2Fapi%3Fq%3DFederated%2520graph%257Csubgraph%257Cgnn%26format%3Djson%26h%3D1000)](https://dblp.uni-trier.de/search?q=Federated%20graph%7Csubgraph%7Cgnn) 

This section partially refers to [DBLP](https://dblp.uni-trier.de/search?q=Federated%20graph%7Csubgraph%7Cgnn) search engine and repositories [Awesome-Federated-Learning-on-Graph-and-GNN-papers](https://github.com/huweibo/Awesome-Federated-Learning-on-Graph-and-GNN-papers) and [Awesome-Federated-Machine-Learning](https://github.com/innovation-cat/Awesome-Federated-Machine-Learning#16-graph-neural-networks).

| Title                                                        | Affiliation       | Venue                  | Year | TL;DR                                               | Materials                                                    |
| ------------------------------------------------------------ | ---------------------- | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| FedWalk: Communication Efficient Federated Unsupervised Node Embedding with Differential Privacy | SJTU | KDD :mortar_board: | 2022 | FedWalk[^FedWalk] | [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539308).] [[PDF](https://arxiv.org/abs/2205.15896)] |
| FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Platform for Federated Graph Learning :fire: | Alibaba | KDD (Best Paper Award) :mortar_board: | 2022 | FederatedScope-GNN [^FederatedScope-GNN] | [[PDF](https://arxiv.org/abs/2204.05562)] [Code](https://github.com/alibaba/FederatedScope) [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539112).] |
| Deep Neural Network Fusion via Graph Matching with Applications to Model Ensemble and Federated Learning | SJTU | ICML :mortar_board: | 2022 | GAMF [^GAMF] | [[PUB.](https://proceedings.mlr.press/v162/liu22k/liu22k.pdf)] [[Code](https://github.com/Thinklab-SJTU/GAMF)] |
| Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting **`kg.`** | ZJU | IJCAI :mortar_board: | 2022 | MaKEr[^MaKEr] | [[PUB](https://www.ijcai.org/proceedings/2022/273).] [[PDF]](https://doi.org/10.48550/arXiv.2205.04692) [[Code](https://github.com/zjukg/maker)] |
| Personalized Federated Learning With a Graph | UTS | IJCAI :mortar_board: | 2022 | SFL[^SFL] | [[PUB](https://www.ijcai.org/proceedings/2022/357).] [[PDF](https://arxiv.org/abs/2203.00829)] [[Code](https://github.com/dawenzi098/SFL-Structural-Federated-Learning)] |
| Vertically Federated Graph Neural Network for Privacy-Preserving Node Classification | ZJU | IJCAI :mortar_board: | 2022 | VFGNN[^VFGNN] | [[PUB](https://www.ijcai.org/proceedings/2022/272).] [[PDF](https://arxiv.org/abs/2005.11903)] |
| SpreadGNN: Decentralized Multi-Task Federated Learning for Graph Neural Networks on Molecular Data | USC | AAAI:mortar_board: | 2022 | SpreadGNN[^SpreadGNN] | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20643).] [PDF](https://arxiv.org/abs/2106.02743) [[Code]](https://github.com/FedML-AI/SpreadGNN) [[解读](https://zhuanlan.zhihu.com/p/429720860)] |
| FedGraph: Federated Graph Learning with Intelligent Sampling | UoA | TPDS :mortar_board:    | 2022 | FedGraph[^FedGraph] | [[PUB.]](https://ieeexplore.ieee.org/abstract/document/9606516/) [Code](https://github.com/cfh19980612/FedGraph) [[解读](https://zhuanlan.zhihu.com/p/442233479)] |
| FedNI: Federated Graph Learning with Network Inpainting for Population-Based Disease Prediction | UESTC | TMI | 2022 | FedNI[^FedNI] | [[PUB](https://ieeexplore.ieee.org/document/9815303).] [[PDF](https://arxiv.org/abs/2112.10166)] |
| FedEgo: Privacy-preserving Personalized Federated Graph Learning with Ego-graphs | SYSU | TOIS | 2022 | FedEgo[^FedEgo] | [PUB.] [PDF](https://arxiv.org/abs/2208.13685) [Code](https://github.com/fedego/fedego) |
| A federated graph neural network framework for privacy-preserving personalization | THU | Nature Communications | 2022 | FedPerGNN[^FedPerGNN] | [[PUB](https://www.nature.com/articles/s41467-022-30714-9).] [[Code](https://github.com/wuch15/FedPerGNN)] [[解读](https://zhuanlan.zhihu.com/p/487383715)] |
| SemiGraphFL: Semi-supervised Graph Federated Learning for Graph Classification. | PKU | PPSN | 2022 | SemiGraphFL[^SemiGraphFL] | [[PUB](https://link.springer.com/chapter/10.1007/978-3-031-14714-2_33).] |
| Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation **`kg.`** | Lehigh University | ACL Workshop           | 2022 | FedR[^FedR] | [[PDF](https://arxiv.org/abs/2203.09553)] [[Code](https://github.com/taokz/FedR)] |
| Power Allocation for Wireless Federated Learning using Graph Neural Networks | Rice University | ICASSP         | 2022 | wirelessfl-pdgnet [^wirelessfl-pdgnet] | [[PUB](https://ieeexplore.ieee.org/document/9747764).] [[PDF](https://arxiv.org/abs/2111.07480)] [[Code](https://github.com/bl166/wirelessfl-pdgnet)] |
| Privacy-Preserving Federated Multi-Task Linear Regression: A One-Shot Linear Mixing Approach Inspired By Graph Regularization | UC | ICASSP | 2022 | multitask-fusion [^multitask-fusion]     | [[PUB](https://ieeexplore.ieee.org/document/9746007).] [PDF](https://www.math.ucla.edu/~harlin/papers/mtl.pdf) [Code](https://github.com/HarlinLee/multitask-fusion) |
| Federated knowledge graph completion via embedding-contrastive learning **`kg.`** | ZJU | Knowl. Based Syst. | 2022 | FedEC[^FedEC] | [[PUB](https://www.sciencedirect.com/science/article/abs/pii/S0950705122007316?via%3Dihub).] |
| Federated Graph Learning with Periodic Neighbour Sampling | HKU | IWQoS | 2022 | PNS-FGL[^PNS-FGL] | [[PUB](https://ieeexplore.ieee.org/document/9812908).] |
| A Privacy-Preserving Subgraph-Level Federated Graph Neural Network via Differential Privacy | Ping An Technology | KSEM                   | 2022 | DP-FedRec[^DP-FedRec] | [[PUB](https://link.springer.com/chapter/10.1007/978-3-031-10989-8_14).] [[PDF]](https://arxiv.org/abs/2206.03492) |
| Graph-Based Traffic Forecasting via Communication-Efficient Federated Learning | SUSTech | WCNC | 2022 | CTFL[^CTFL] | [[PUB](https://ieeexplore.ieee.org/document/9771883).] |
| Federated meta-learning for spatial-temporal prediction | NEU | Neural Comput. Appl. | 2022 | FML-ST[^FML-ST] | [[PUB](https://link.springer.com/article/10.1007/s00521-021-06861-3).] [Code](https://github.com/lwz001/FML-ST) |
| Malicious Transaction Identification in Digital Currency via Federated Graph Deep Learning | BIT | INFOCOM Workshops | 2022 | GraphSniffer [^GraphSniffer] | [[PUB](https://ieeexplore.ieee.org/document/9797992/).] |
| Leveraging Spanning Tree to Detect Colluding Attackers in Federated Learning | Missouri S&T | INFCOM Workshops | 2022 | FL-ST[^FL-ST] | [[PUB](https://ieeexplore.ieee.org/document/9798077).] |
| Federated learning of molecular properties with graph neural networks in a heterogeneous setting | University of Rochester | Patterns | 2022 | FLIT+[^FLIT+] | [[PUB](https://linkinghub.elsevier.com/retrieve/pii/S2666389922001180).] [PDF](https://arxiv.org/abs/2109.07258) [Code](https://github.com/ur-whitelab/fedchem) [Code](https://doi.org/10.5281/zenodo.6485682) |
| Multi-Level Federated Graph Learning and Self-Attention Based Personalized Wi-Fi Indoor Fingerprint Localization | SYSU | IEEE Commun. Lett. | 2022 | ML-FGL[^ML-FGL] | [[PUB](https://ieeexplore.ieee.org/document/9734052).] |
| Decentralized Graph Federated Multitask Learning for Streaming Data | NTNU | CISS                   | 2022 | PSO-GFML[^PSO-GFML] | [[PUB.]](https://doi.org/10.1109/CISS53076.2022.9751160) |
| Dynamic Neural Graphs Based Federated Reptile for Semi-Supervised Multi-Tasking in Healthcare Applications | Oxford | JBHI                   | 2022 | DNG-FR[^DNG-FR] | [[PUB.] ](https://ieeexplore.ieee.org/document/9648036) |
| FedGCN: Federated Learning-Based Graph Convolutional Networks for Non-Euclidean Spatial Data | NUIST | Mathematics | 2022 | FedGCN-NES[^FedGCN-NES] | [[PUB](https://www.mdpi.com/2227-7390/10/6/1000).] |
| Device Sampling for Heterogeneous Federated Learning: Theory, Algorithms, and Implementation. | Purdue | INFOCOM :mortar_board: | 2021 | D2D-FedL[^D2D-FedL] | [[PUB](https://ieeexplore.ieee.org/document/9488906).] [PDF](https://arxiv.org/abs/2101.00787) |
| Federated Graph Classification over Non-IID Graphs           | Emory | NeurIPS :mortar_board: | 2021 | GCFL[^GCFL] | [[PUB.]](https://papers.nips.cc/paper/2021/hash/9c6947bd95ae487c81d4e19d3ed8cd6f-Abstract.html) [[PDF]](https://arxiv.org/abs/2106.13423) [[Code](https://github.com/Oxfordblue7/GCFL)] [[解读](https://zhuanlan.zhihu.com/p/430623053)] |
| Subgraph Federated Learning with Missing Neighbor Generation | Emory; UBC; Lehigh University | NeurIPS :mortar_board: | 2021 | FedSage[^FedSage] | [[PUB.]](https://papers.neurips.cc/paper/2021/hash/34adeb8e3242824038aa65460a47c29e-Abstract.html) [[PDF]](https://arxiv.org/abs/2106.13430) |
| Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling | USC | KDD :mortar_board:                    | 2021 | CNFGNN[^CNFGNN]                          | [[PUB](https://dl.acm.org/doi/10.1145/3447548.3467371).] [PDF](https://arxiv.org/abs/2106.05223) [[Code]](https://github.com/mengcz13/KDD2021_CNFGNN) [[解读](https://zhuanlan.zhihu.com/p/434839878)] |
| Differentially Private Federated Knowledge Graphs Embedding **`kg.`** | BUAA | CIKM                   | 2021 | FKGE[^FKGE] | [[PUB](https://dl.acm.org/doi/10.1145/3459637.3482252).] [[PDF]](https://arxiv.org/abs/2105.07615) [[Code](https://github.com/HKUST-KnowComp/FKGE)] [[解读](https://zhuanlan.zhihu.com/p/437895959)] |
| Decentralized Federated Graph Neural Networks | Blue Elephant Tech | IJCAI Workshop | 2021 | D-FedGNN[^D-FedGNN] | [[PDF]](https://federated-learning.org/fl-ijcai-2021/FTL-IJCAI21_paper_20.pdf) |
| FedSGC: Federated Simple Graph Convolution for Node Classification | HKUST | IJCAI Workshop | 2021 | FedSGC[^FedSGC] | [PDF](https://federated-learning.org/fl-ijcai-2021/FTL-IJCAI21_paper_5.pdf) |
| FL-DISCO: Federated Generative Adversarial Network for Graph-based Molecule Drug Discovery: Special Session Paper | UNM | ICCAD                  | 2021 | FL-DISCO[^FL-DISCO] | [[PUB.]](https://doi.org/10.1109/ICCAD51958.2021.9643440)    |
| FASTGNN: A Topological Information Protected Federated Learning Approach for Traffic Speed Forecasting | UTS | IEEE Trans. Ind. Informatics | 2021 | FASTGNN [^FASTGNN] | [[PUB](https://ieeexplore.ieee.org/document/9340313).] |
| DAG-FL: Direct Acyclic Graph-based Blockchain Empowers On-Device Federated Learning | BUPT; UESTC | ICC                    | 2021 | DAG-FL[^DAG-FL] | [[PUB.]](https://doi.org/10.1109/ICC42927.2021.9500737) [PDF](https://arxiv.org/abs/2104.13092) |
| Graphical Federated Cloud Sharing Markets                    |                   | TSUSC                  | 2021 |          | [[PUB.]](https://doi.org/10.1109/TSUSC.2021.3100010)         |
| Virtual Knowledge Graphs for Federated Log Analysis **`kg.`** |                    | ARES                   | 2021 |             | [[PUB.]](https://doi.org/10.1145/3465481.3465767)            |
| FedE: Embedding Knowledge Graphs in Federated Setting **`kg.`** |                   | IJCKG                  | 2021 | FedE[^FedE] | [[PUB.]](https://doi.org/10.1145/3502223.3502233) [[PDF]](https://arxiv.org/abs/2010.12882) [[Code]](https://github.com/AnselCmy/FedE) |
| Federated Knowledge Graph Embeddings with Heterogeneous Data **`kg.`** |                    | CCKS                   | 2021 |         | [[PUB.]](https://doi.org/10.1007/978-981-16-6471-7_2)        |
| A Graph Federated Architecture with Privacy Preserving Learning | EPFL | SPAWC                  | 2021 | GFL[^GFL] | [[PUB.]](https://doi.org/10.1109/SPAWC51858.2021.9593148) [[PDF]](https://arxiv.org/abs/2104.13215) [[解读](https://zhuanlan.zhihu.com/p/440809332)] |
| Federated Social Recommendation with Graph Neural Network    | UIC | ACM TIST               | 2021 | FeSoG[^FeSoG] | [[PUB](https://dl.acm.org/doi/abs/10.1145/3501815).] [[PDF](https://arxiv.org/abs/2111.10778)] [Code](https://github.com/YangLiangwei/FeSoG) |
| FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks :fire: **`surv.`** |               | ICLR-DPML              | 2021 |  | [[PDF]](https://arxiv.org/abs/2104.07145) [[Code]](https://github.com/FedML-AI/FedGraphNN) [[解读](https://zhuanlan.zhihu.com/p/429220636)] |
| Cluster-driven Graph Federated Learning over Multiple Domains |           | CVPR Workshop          | 2021 |                     | [[PDF]](https://arxiv.org/abs/2104.14628) [[解读](https://zhuanlan.zhihu.com/p/440527314)] |
| Glint: Decentralized Federated Graph Learning with Traffic Throttling and Flow Scheduling |                   | IWQoS                  | 2021 | Glint[^Glint] | [[PUB.]](https://doi.org/10.1109/IWQOS52092.2021.9521331)    |
| A Federated Multigraph Integration Approach for Connectional Brain Template Learning |         | MICCAI Workshop        | 2021 |  | [[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-89847-2_4) |
| Federated Graph Neural Network for Cross-graph Node Classification |                    | CCIS                   | 2021 |      | [[PUB.]](https://doi.org/10.1109/CCIS53392.2021.9754598)     |
| GraFeHTy: Graph Neural Network using Federated Learning for Human Activity Recognition |                   | ICMLA                  | 2021 | GraFeHTy[^GraFeHTy] | [[PUB.]](https://doi.org/10.1109/ICMLA52953.2021.00184)      |
| Distributed Training of Graph Convolutional Networks |  | TSIPN | 2021 |  | [[PUB](https://ieeexplore.ieee.org/document/9303371).] [[PDF](https://arxiv.org/abs/2007.06281)] [[解读](https://zhuanlan.zhihu.com/p/433329525)] |
| FedGNN: Federated Graph Neural Network for Privacy-Preserving Recommendation |    | ICML workshop   | 2021 | FedGNN[^FedGNN] | [[PDF]](https://arxiv.org/abs/2102.04925) [[解读](https://zhuanlan.zhihu.com/p/428783383)] |
| Decentralized federated learning of deep neural networks on non-iid data | | ICML workshop | 2021 | DFL-PENS[^DFL-PENS] | [PDF](https://fl-icml.github.io/2021/papers/FL-ICML21_paper_3.pdf) [PDF](https://arxiv.org/abs/2107.08517) [Code](https://github.com/guskarls/dfl-pens) |
| BiG-Fed: Bilevel Optimization Enhanced Graph-Aided Federated Learning | | ICML workshop | 2021 | BiG-Fed[^BiG-Fed] | [[PDF](https://fl-icml.github.io/2021/papers/FL-ICML21_paper_74.pdf#:~:text=BiG-Fed%3A%20Bilevel%20Optimization%20Enhanced%20Graph-Aided%20Federated%20Learning%20nique,center%20and%20generalized%20learning%20tasks%20for%20local%20clients.)] |
| Decentralized federated learning for electronic health records | UMN | CISS | 2020 | FL-DSGD[^FL-DSGD] | [[PUB](https://ieeexplore.ieee.org/abstract/document/9086196#:~:text=Decentralized%20Federated%20Learning%20for%20Electronic%20Health%20Records%20Abstract:,in%20distributed%20training%20problems%20within%20a%20star%20network.).] [[解读](https://zhuanlan.zhihu.com/p/448738120)] |
| ASFGNN: Automated Separated-Federated Graph Neural Network   | Ant Group | PPNA                   | 2020 | ASFGNN[^ASFGNN] | [[PUB.]](https://doi.org/10.1007/s12083-021-01074-w) [[PDF]](https://arxiv.org/abs/2011.03248) [[解读](https://zhuanlan.zhihu.com/p/431283541)] |
| Decentralized federated learning via sgd over wireless d2d networks | SZU | SPAWC | 2020 | DSGD[^DSGD] | [[PUB](https://ieeexplore.ieee.org/document/9154332).] [PDF](https://arxiv.org/abs/2002.12507) |
| SGNN: A Graph Neural Network Based Federated Learning Approach by Hiding Structure | SDU | BigData | 2019 | SGNN[^SGNN] | [[PUB](https://ieeexplore.ieee.org/document/9005983).] [[PDF]](https://www.researchgate.net/profile/Shijun_Liu3/publication/339482514_SGNN_A_Graph_Neural_Network_Based_Federated_Learning_Approach_by_Hiding_Structure/links/5f48365d458515a88b790595/SGNN-A-Graph-Neural-Network-Based-Federated-Learning-Approach-by-Hiding-Structure.pdf) |
| Learn electronic health records by fully decentralized federated learning | UMN | NeurIPS Workshop | 2019 | FL-DSGD[^FL-DSGD] | [[PDF](https://arxiv.org/abs/1912.01792)] |
| Towards Federated Graph Learning for Collaborative Financial Crimes Detection |        | NeurIPS Workshop       | 2019 |                     | [[PDF]](https://arxiv.org/abs/1909.12946)             |
| Federated learning of predictive models from federated Electronic Health Records :star: | BU | Int. J. Medical Informatics | 2018 | cPDS[^cPDS] | [[PUB](https://www.sciencedirect.com/science/article/abs/pii/S138650561830008X?via%3Dihub).] |
| Federated Graph Contrastive Learning | UTS | preprint | 2022 | | [[PDF](https://arxiv.org/abs/2207.11836)] |
| Federated Graph Machine Learning: A Survey of Concepts, Techniques, and Applications **`surv.`** | University of Virginia | preprint | 2022 | FGML [^FGML] | [[PDF](https://arxiv.org/abs/2207.11812)] |
| FD-GATDR: A Federated-Decentralized-Learning Graph Attention Network for Doctor Recommendation Using EHR |  | preprint | 2022 | FD-GATDR[^FD-GATDR] | [[PDF](https://arxiv.org/abs/2207.05750)] |
| Privacy-preserving Graph Analytics: Secure Generation and Federated Learning |  | preprint | 2022 |  | [[PDF](https://arxiv.org/abs/2207.00048)] |
| Personalized Subgraph Federated Learning |  | preprint | 2022 |  | [[PDF](https://arxiv.org/abs/2206.10206)] |
| Federated Graph Attention Network for Rumor Detection |  | preprint | 2022 |  | [[PDF]](https://arxiv.org/abs/2206.05713) [[Code](https://github.com/baichuanzheng1/fedgat)] |
| FedRel: An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning |  | preprint | 2022 |  | [[PDF]](https://arxiv.org/abs/2206.03420) |
| Privatized Graph Federated Learning                          |                | preprint               | 2022 |                     | [[PDF](https://arxiv.org/abs/2203.07105)]                    |
| Graph-Assisted Communication-Efficient Ensemble Federated Learning |                | preprint               | 2022 |                     | [[PDF](https://arxiv.org/abs/2202.13447)]                    |
| Federated Graph Neural Networks: Overview, Techniques and Challenges **`surv.`** |                | preprint               | 2022 |                     | [[PDF](https://arxiv.org/abs/2202.07256)]                    |
| Decentralized event-triggered federated learning with heterogeneous communication thresholds. | | preprint | 2022 | EF-HC[^EF-HC] | [PDF](https://github.com/ShahryarBQ/EF_HC) |
| More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks |                | preprint               | 2022 |                     | [[PDF](https://arxiv.org/abs/2202.03195)]                    |
| FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks |                | preprint               | 2022 | FedGCN[^FedGCN] | [[PDF](https://arxiv.org/abs/2201.12433)] [[Code](https://github.com/yh-yao/FedGCN)] |
| Federated Learning with Heterogeneous Architectures using Graph HyperNetworks |                | preprint               | 2022 |                     | [[PDF](https://arxiv.org/abs/2201.08459)]                    |
| STFL: A Temporal-Spatial Federated Learning Framework for Graph Neural Networks |                | preprint               | 2021 |  | [[PDF](https://arxiv.org/abs/2111.06750)] [[Code](https://github.com/jw9msjwjnpdrlfw/tsfl)] |
| Graph-Fraudster: Adversarial Attacks on Graph Neural Network Based Vertical Federated Learning |  | preprint | 2021 |  | [[PDF](https://arxiv.org/abs/2110.06468)] [[Code](https://github.com/hgh0545/graph-fraudster)] |
| PPSGCN: A Privacy-Preserving Subgraph Sampling Based Distributed GCN Training Method | | preprint | 2021 | PPSGCN[^PPSGCN] | [PDF](https://arxiv.org/abs/2110.12906) |
| Leveraging a Federation of Knowledge Graphs to Improve Faceted Search in Digital Libraries **`kg.`** |                | preprint               | 2021 |                     | [[PDF]](https://arxiv.org/abs/2107.05447)                    |
| Federated Myopic Community Detection with One-shot Communication |                | preprint               | 2021 |                     | [[PDF]](https://arxiv.org/abs/2106.07255)                    |
| Federated Graph Learning -- A Position Paper **`surv.`**    |                | preprint               | 2021 |                     | [[PDF]](https://arxiv.org/abs/2105.11099)                    |
| A Vertical Federated Learning Framework for Graph Convolutional Network |                | preprint               | 2021 | FedVGCN[^FedVGCN] | [[PDF]](https://arxiv.org/abs/2106.11593)                    |
| FedGL: Federated Graph Learning Framework with Global Self-Supervision |                | preprint               | 2021 | FedGL[^FedGL] | [[PDF]](https://arxiv.org/abs/2105.03170)                    |
| FL-AGCNS: Federated Learning Framework for Automatic Graph Convolutional Network Search |                | preprint               | 2021 | FL-AGCNS[^FL-AGCNS] | [[PDF]](https://arxiv.org/abs/2104.04141)                    |
| Towards On-Device Federated Learning: A Direct Acyclic Graph-based Blockchain Approach |  | preprint | 2021 |  | [[PDF](https://arxiv.org/abs/2104.13092)] |
| A New Look and Convergence Rate of Federated Multi-Task Learning with Laplacian Regularization | | preprint | 2021 | dFedU[^dFedU] | [PDF](https://arxiv.org/abs/2102.07148) [Code](https://github.com/dual-grp/fedu_fmtl) |
| GraphFL: A Federated Learning Framework for Semi-Supervised Node Classification on Graphs |                | preprint               | 2020 | GraphFL[^GraphFL] | [[PDF]](https://arxiv.org/abs/2012.04187) [[解读](https://zhuanlan.zhihu.com/p/431479904)] |
| Improving Federated Relational Data Modeling via Basis Alignment and Weight Penalty **`kg.`** |                | preprint               | 2020 | FedAlign-KG[^FedAlign-KG] | [[PDF]](https://arxiv.org/abs/2011.11369)                    |
| Federated Dynamic GNN with Secure Aggregation                |                | preprint               | 2020 |                     | [[PDF]](https://arxiv.org/abs/2009.07351)                    |
| GraphFederator: Federated Visual Analysis for Multi-party Graphs |  | preprint | 2020 |  | [[PDF](https://arxiv.org/abs/2008.11989)] |
| Privacy-Preserving Graph Neural Network for Node Classification |                | preprint               | 2020 |                     | [[PDF]](https://arxiv.org/abs/2005.11903)                    |
| Peer-to-peer federated learning on graphs                    | UC | preprint               | 2019 | P2P-FLG[^P2P-FLG] | [[PDF]](https://arxiv.org/abs/1901.11173) [[解读](https://zhuanlan.zhihu.com/p/441944011)] |




### Private Graph Neural Networks (todo)

- [Arxiv 2021] Privacy-Preserving Graph Convolutional Networks for Text Classification. [[PDF]](https://arxiv.org/abs/2102.09604)
- [Arxiv 2021] GraphMI: Extracting Private Graph Data from Graph Neural Networks. [[PDF]](https://arxiv.org/abs/2106.02820)
- [Arxiv 2021] Towards Representation Identical Privacy-Preserving Graph Neural Network via Split Learning. [[PDF]](https://arxiv.org/abs/2107.05917)
- [Arxiv 2020] Locally Private Graph Neural Networks. [[PDF]](https://arxiv.org/abs/2006.05535)



## FL on Tabular Data

[![dblp](https://img.shields.io/badge/dynamic/json?label=dblp&query=%24.result.hits[%27%40total%27]&url=https%3A//dblp.org/search/publ/api%3Fq%3Dfederate%2520tree%257Cboost%257Cbagging%257Cgbdt%257Ctabular%257Cforest%257CXGBoost%26format%3Djson%26h%3D1000)](https://dblp.org/search?q=federate%20tree%7Cboost%7Cbagging%7Cgbdt%7Ctabular%7Cforest%7CXGBoost)

This section refers to [DBLP](https://dblp.org/search?q=federate%20tree%7Cboost%7Cbagging%7Cgbdt%7Ctabular%7Cforest) search engine.

| Title                                                        | Affiliation                       | Venue                       | Year | TL;DR                       | Materials                                                    |
| ------------------------------------------------------------ | --------------------------------- | --------------------------- | ---- | --------------------------- | ------------------------------------------------------------ |
| Federated Functional Gradient Boosting                       | University of Pennsylvania        | AISTATS :mortar_board:      | 2022 | FFGB[^FFGB]                 | [[PUB](https://proceedings.mlr.press/v151/shen22a.html).] [[PDF](https://arxiv.org/abs/2103.06972)] [[Code](https://github.com/shenzebang/Federated-Learning-Pytorch)] |
| Federated Random Forests can improve local performance of predictive models for various healthcare applications | University of Marburg             | Bioinform.                  | 2022 | FRF[^FRF]                   | [[PUB](https://academic.oup.com/bioinformatics/article-abstract/38/8/2278/6525214).] [[Code](https://featurecloud.ai/)] |
| Federated Forest                                             | JD                                | TBD                         | 2022 | FF[^FF]                     | [[PUB](https://ieeexplore.ieee.org/document/9088965).] [[PDF](https://arxiv.org/abs/1905.10053)] |
| Fed-GBM: a cost-effective federated gradient boosting tree for non-intrusive load monitoring | The University of Sydney          | e-Energy                    | 2022 | Fed-GBM[^Fed-GBM]           | [[PUB](https://dl.acm.org/doi/10.1145/3538637.3538840).]     |
| BOFRF: A Novel Boosting-Based Federated Random Forest Algorithm on Horizontally Partitioned Data | METU                              | IEEE Access                 | 2022 | BOFRF[^BOFRF]               | [[PUB](https://ieeexplore.ieee.org/document/9867984/).]      |
| eFL-Boost: Efficient Federated Learning for Gradient Boosting Decision Trees | kobe-u                            | IEEE Access                 | 2022 | eFL-Boost [^eFL-Boost]      | [[PUB](https://ieeexplore.ieee.org/document/9761890).]       |
| Random Forest Based on Federated Learning for Intrusion Detection | Malardalen University             | AIAI                        | 2022 | FL-RF[^FL-RF]               | [[PUB](https://link.springer.com/chapter/10.1007/978-3-031-08333-4_11).] |
| Cross-silo federated learning based decision trees           | ETH Zürich                        | SAC                         | 2022 | FL-DT[^FL-DT]               | [[PUB](https://dl.acm.org/doi/10.1145/3477314.3507149).]     |
| Leveraging Spanning Tree to Detect Colluding Attackers in Federated Learning | Missouri S&T                      | INFCOM Workshops            | 2022 | FL-ST[^FL-ST]               | [[PUB](https://ieeexplore.ieee.org/document/9798077).]       |
| VF2Boost: Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning | PKU                               | SIGMOD :mortar_board:       | 2021 | VF2Boost[^VF2Boost]         | [[PUB](https://dl.acm.org/doi/10.1145/3448016.3457241).]     |
| SecureBoost: A Lossless Federated Learning Framework :fire:  | UC                                | IEEE Intell. Syst.          | 2021 | SecureBoost[^SecureBoost]   | [[PUB](https://ieeexplore.ieee.org/document/9440789/).] [PDF](https://arxiv.org/abs/1901.08755)  [Slides](https://fate.readthedocs.io/en/latest/resources/SecureBoost-ijcai2019-workshop.pdf) [Code](https://github.com/FederatedAI/FATE/tree/master/python/federatedml/ensemble/secureboost) [[解读](https://zhuanlan.zhihu.com/p/456772543)] [[解读](https://zhuanlan.zhihu.com/p/545739311)] [UC](https://github.com/Koukyosyumei/AIJack). |
| A Blockchain-Based Federated Forest for SDN-Enabled In-Vehicle Network Intrusion Detection System | CNU                               | IEEE Access                 | 2021 | BFF-IDS[^BFF-IDS]           | [[PUB](https://ieeexplore.ieee.org/document/9471858).]       |
| Research on privacy protection of multi source data based on improved gbdt federated ensemble method with different metrics | NCUT                              | Phys. Commun.               | 2021 | I-GBDT[^I-GBDT]             | [[PUB](https://www.sciencedirect.com/science/article/pii/S1874490721000847).] |
| Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Vertical Federated Learning | UCAS; CAS                         | IEEE BigData                | 2021 | Fed-EINI[^Fed-EINI]         | [[PUB](https://ieeexplore.ieee.org/document/9671749).] [[PDF](https://arxiv.org/abs/2105.09540)] |
| Gradient Boosting Forest: a Two-Stage Ensemble Method Enabling Federated Learning of GBDTs | THU                               | ICONIP                      | 2021 | GBF-Cen[^GBF-Cen]           | [[PUB](https://link.springer.com/chapter/10.1007/978-3-030-92270-2_7).] |
| A k-Anonymised Federated Learning Framework with Decision Trees | Umeå University                   | DPM/CBT @ESORICS            | 2021 | KA-FL[^KA-FL]               | [[PUB](https://link.springer.com/chapter/10.1007/978-3-030-93944-1_7).] |
| AF-DNDF: Asynchronous Federated Learning of Deep Neural Decision Forests | Chalmers                          | SEAA                        | 2021 | AF-DNDF[^AF-DNDF]           | [[PUB](https://ieeexplore.ieee.org/document/9582575).]       |
| Compression Boosts Differentially Private Federated Learning | Univ. Grenoble Alpes              | EuroS&P                     | 2021 | CB-DP[^CB-DP]               | [[PUB](https://ieeexplore.ieee.org/document/9581200).] [[PDF](https://arxiv.org/abs/2011.05578)] |
| Practical Federated Gradient Boosting Decision Trees         | NUS; UWA                          | AAAI :mortar_board:         | 2020 | SimFL[^SimFL]               | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/5895).] [[PDF](https://arxiv.org/abs/1911.04206)] [[Code](https://github.com/Xtra-Computing/SimFL)] |
| Privacy Preserving Vertical Federated Learning for Tree-based Models | NUS                               | VLDB :mortar_board:         | 2020 | Pivot-DT[^Pivot-DT]         | [[PUB](http://vldb.org/pvldb/vol13/p2090-wu.pdf).] [PDF](https://arxiv.org/abs/2008.06170) [[Video](https://www.youtube.com/watch?v=sjii8oVCqiY)] [Code](https://github.com/nusdbsystem/pivot) |
| Boosting Privately: Federated Extreme Gradient Boosting for Mobile Crowdsensing | Xidian University                 | ICDCS                       | 2020 | FEDXGB [^FEDXGB]            | [[PUB](https://ieeexplore.ieee.org/document/9355600).] [[PDF](https://arxiv.org/abs/1907.10218)] |
| FedCluster: Boosting the Convergence of Federated Learning via Cluster-Cycling | University of Utah                | IEEE BigData                | 2020 | FedCluster[^FedCluster]     | [[PUB](https://ieeexplore.ieee.org/document/9377960).] [[PDF](https://arxiv.org/abs/2009.10748)] |
| New Approaches to Federated XGBoost Learning for Privacy-Preserving Data Analysis | kobe-u                            | ICONIP                      | 2020 | FL-XGBoost[^FL-XGBoost]     | [[PUB](https://link.springer.com/chapter/10.1007/978-3-030-63833-7_47).] |
| Bandwidth Slicing to Boost Federated Learning Over Passive Optical Networks | Chalmers University of Technology | IEEE Communications Letters | 2020 | FL-PON[^FL-PON]             | [[PUB](https://ieeexplore.ieee.org/document/9044640).]       |
| DFedForest: Decentralized Federated Forest                   | UFRJ                              | Blockchain                  | 2020 | DFedForest[^DFedForest]     | [[PUB](https://ieeexplore.ieee.org/document/9284805/).]      |
| Straggler Remission for Federated Learning via Decentralized Redundant Cayley Tree | Stevens Institute of Technology   | LATINCOM                    | 2020 | DRC-tree[^DRC-tree]         | [[PUB](https://ieeexplore.ieee.org/document/9282334).]       |
| Federated Soft Gradient Boosting Machine for Streaming Data  | Sinovation Ventures AI Institute  | Federated Learning          | 2020 | Fed-sGBM[^Fed-sGBM]         | [[PUB](https://link.springer.com/chapter/10.1007/978-3-030-63076-8_7).] [[解读](https://www.leiphone.com/category/academic/4tVdYDuYTA293NCy.html)] |
| Federated Learning of Deep Neural Decision Forests           | Fraunhofer-Chalmers Centre        | LOD                         | 2019 | FL-DNDF[^FL-DNDF]           | [[PUB](https://link.springer.com/chapter/10.1007/978-3-030-37599-7_58).] |
| Statistical Detection of Adversarial examples in Blockchain-based Federated Forest In-vehicle Network Intrusion Detection Systems |                                   | preprint                    | 2022 |                             | [[PDF](https://arxiv.org/abs/2207.04843)]                    |
| Hercules: Boosting the Performance of Privacy-preserving Federated Learning |                                   | preprint                    | 2022 | Hercules[^Hercules]         | [[PDF](https://arxiv.org/abs/2207.04620)]                    |
| FedGBF: An efficient vertical federated learning framework via gradient boosting and bagging |                                   | preprint                    | 2022 | FedGBF[^FedGBF]             | [[PDF](https://arxiv.org/abs/2204.00976)]                    |
| A Fair and Efficient Hybrid Federated Learning Framework based on XGBoost for Distributed Power Prediction. | THU                               | preprint                    | 2022 | HFL-XGBoost[^HFL-XGBoost]   | [PDF](https://arxiv.org/abs/2201.02783)                      |
| An Efficient and Robust System for Vertically Federated Random Forest |                                   | preprint                    | 2022 |                             | [[PDF](https://arxiv.org/abs/2201.10761)]                    |
| Efficient Batch Homomorphic Encryption for Vertically Federated XGBoost. | BUAA                              | preprint                    | 2021 | EBHE-VFXGB[^EBHE-VFXGB]     | [PDF](https://arxiv.org/abs/2112.04261)                      |
| Guess what? You can boost Federated Learning for free        |                                   | preprint                    | 2021 |                             | [[PDF](https://arxiv.org/abs/2110.11486)]                    |
| SecureBoost+ : A High Performance Gradient Boosting Tree Framework for Large Scale Vertical Federated Learning :fire: |                                   | preprint                    | 2021 | SecureBoost+[^SecureBoost+] | [[PDF](https://arxiv.org/abs/2110.10927)] [[Code](https://github.com/FederatedAI/FATE)] |
| Fed-TGAN: Federated Learning Framework for Synthesizing Tabular Data |                                   | preprint                    | 2021 | Fed-TGAN[^Fed-TGAN]         | [[PDF](https://arxiv.org/abs/2108.07927)]                    |
| FedXGBoost: Privacy-Preserving XGBoost for Federated Learning | TUM                               | preprint                    | 2021 | FedXGBoost[^FedXGBoost]     | [PDF](https://arxiv.org/abs/2106.10662)                      |
| An Efficient Learning Framework For Federated XGBoost Using Secret Sharing And Distributed Optimization. | Tongji University                 | preprint                    | 2021 | MP-FedXGB[^MP-FedXGB]       | [PDF](https://arxiv.org/abs/2105.05717) [Code](https://github.com/HikariX/MP-FedXGB) |
| A Tree-based Federated Learning Approach for Personalized Treatment Effect Estimation from Heterogeneous Data Sources |                                   | preprint                    | 2021 |                             | [[PDF](https://arxiv.org/abs/2103.06261)] [[Code](https://github.com/ellenxtan/ifedtree)] |
| Adaptive Histogram-Based Gradient Boosted Trees for Federated Learning |                                   | preprint                    | 2020 |                             | [[PDF](https://arxiv.org/abs/2012.06670)]                    |
| FederBoost: Private Federated Learning for GBDT              | ZJU                               | preprint                    | 2020 | FederBoost [^FederBoost]    | [[PDF](https://arxiv.org/abs/2011.02796)]                    |
| Privacy Preserving Text Recognition with Gradient-Boosting for Federated Learning |                                   | preprint                    | 2020 |                             | [[PDF](https://arxiv.org/abs/2007.07296)] [[Code](https://github.com/rand2ai/fedboost)] |
| Cloud-based Federated Boosting for Mobile Crowdsensing       |                                   | preprint                    | 2020 |                             | [[arxiv](https://arxiv.org/abs/2005.05304)]                  |
| Federated Extra-Trees with Privacy Preserving                |                                   | preprint                    | 2020 |                             | [[PDF](https://arxiv.org/abs/2002.07323.pdf)]                |
| Bandwidth Slicing to Boost Federated Learning in Edge Computing |                                   | preprint                    | 2019 |                             | [[PDF](https://arxiv.org/abs/1911.07615)]                    |
| Revocable Federated Learning: A Benchmark of Federated Forest |                                   | preprint                    | 2019 |                             | [[PDF](https://arxiv.org/abs/1911.03242)]                    |
| The Tradeoff Between Privacy and Accuracy in Anomaly Detection Using Federated XGBoost | CUHK                              | preprint                    | 2019 | F-XGBoost[^F-XGBoost]       | [PDF](https://arxiv.org/abs/1907.07157) [Code](https://github.com/Raymw/Federated-XGBoost) |



## FL in top-tier journal

List of papers in the field of federated learning in Nature(and its sub-journals), Cell, Science(and Science Advances) and PANS refers to [WOS](https://www.webofscience.com/wos/woscc/summary/ed3f4552-5450-4de7-bf2c-55d01e20d5de-4301299e/relevance/1) search engine.

| Title                                                        | Affiliation | Venue                 | Year | TL;DR                 | Materials                                                    |
| ------------------------------------------------------------ | ----------- | --------------------- | ---- | --------------------- | ------------------------------------------------------------ |
| Federated disentangled representation learning for unsupervised brain anomaly detection | TUM         | Nat. Mach. Intell.    | 2022 | FedDis[^FedDis]       | [[PUB](https://www.nature.com/articles/s42256-022-00515-2).] [PDF](https://arxiv.org/abs/2103.03705) [[PDF](https://doi.org/https://doi.org/10.21203/rs.3.rs-722389/v1)] [Code](https://github.com/albarqounilab/FedDis-NMI) [Code](https://doi.org/10.5281/zenodo.6604161) |
| Shifting machine learning for healthcare from development to deployment and from models to data |             | Nat. Biomed. Eng.     | 2022 |                       | [[PUB](https://www.nature.com/articles/s41551-022-00898-y).] |
| A federated graph neural network framework for privacy-preserving personalization | THU         | Nat. Commun.          | 2022 | FedPerGNN[^FedPerGNN] | [[PUB](https://www.nature.com/articles/s41467-022-30714-9).] [[Code](https://github.com/wuch15/FedPerGNN)] [[解读](https://zhuanlan.zhihu.com/p/487383715)] |
| Communication-efficient federated learning via knowledge distillation |             | Nat. Commun.          | 2022 |                       | [[PUB](https://www.nature.com/articles/s41467-022-29763-x).] [PDF](https://arxiv.org/abs/2108.13323) [Code](https://github.com/wuch15/FedKD) [Code](https://zenodo.org/record/6383473) |
| Lead federated neuromorphic learning for wireless edge artificial intelligence |             | Nat. Commun.          | 2022 |                       | [[PUB](https://www.nature.com/articles/s41467-022-32020-w).] [Code ](https://github.com/GOGODD/FL-EDGE-COMPUTING/releases/tag/federated_learning) [[解读](https://zhuanlan.zhihu.com/p/549087420)] |
| Advancing COVID-19 diagnosis with privacy-preserving collaboration in artificial intelligence |             | Nat. Mach. Intell.    | 2021 |                       | [[PUB](https://www.nature.com/articles/s42256-021-00421-z).] [PDF](https://arxiv.org/abs/2111.09461) [Code](https://github.com/HUST-EIC-AI-LAB/UCADI) |
| Federated learning for predicting clinical outcomes in patients with COVID-19 |             | Nat. Med.             | 2021 |                       | [[PUB](https://www.nature.com/articles/s41591-021-01506-3).] [Code](https://www.nature.com/articles/s41591-021-01506-3#code-availability) |
| Adversarial interference and its mitigations in privacy-preserving collaborative machine learning |             | Nat. Mach. Intell.    | 2021 |                       | [[PUB](https://www.nature.com/articles/s42256-021-00390-3).] |
| Swarm Learning for decentralized and confidential clinical machine learning :star: |             | Nature :mortar_board: | 2021 |                       | [[PUB](https://www.nature.com/articles/s41586-021-03583-3).] [Code ](https://github.com/HewlettPackard/swarm-learning) [Software](https://myenterpriselicense.hpe.com) [[解读](https://zhuanlan.zhihu.com/p/379434722)] |
| End-to-end privacy preserving deep learning on multi-institutional medical imaging |             | Nat. Mach. Intell.    | 2021 |                       | [[PUB](https://www.nature.com/articles/s42256-021-00337-8).] [Code](https://github.com/gkaissis/PriMIA) [Code](https://doi.org/10.5281/zenodo.4545599) [[解读](https://zhuanlan.zhihu.com/p/484801505)] |
| Communication-efficient federated learning                   |             | PANS.                 | 2021 |                       | [[PUB](https://www.pnas.org/doi/full/10.1073/pnas.2024789118).] [[Code](https://github.com/mzchen0/Communication-Efficient-Federated-Learning)] [Code](https://code.ihub.org.cn/projects/4394/repository/revisions/master/show/PNAS) |
| Breaking medical data sharing boundaries by using synthesized radiographs |             | Science. Advances.    | 2020 |                       | [[PUB](https://www.science.org/doi/10.1126/sciadv.abb7973).] [Code](https://github.com/peterhan91/Thorax_GAN) |
| Secure, privacy-preserving and federated machine learning in medical imaging :star: |             | Nat. Mach. Intell.    | 2020 |                       | [[PUB](https://www.nature.com/articles/s42256-020-0186-1).]  |





## FL in top AI conference and journal

In this section, we will summarize Federated Learning papers accepted by top AI(Artificial Intelligence) conference and journal, Including [IJCAI](https://dblp.org/db/conf/ijcai/index.html)(International Joint Conference on Artificial Intelligence), [AAAI](https://dblp.uni-trier.de/db/conf/aaai/index.html)(AAAI Conference on Artificial Intelligence), [AISTATS](https://dblp.uni-trier.de/db/conf/aistats/index.html)(Artificial Intelligence and Statistics).

- [IJCAI](https://dblp.uni-trier.de/search?q=federate%20venue%3AIJCAI%3A) 2022,[2021](https://ijcai-21.org/program-main-track/#),2020,[2019](https://www.ijcai19.org/accepted-papers.html)
- [AAAI](https://dblp.uni-trier.de/search?q=federate%20venue%3AAAAI%3A) [2022](https://aaai.org/Conferences/AAAI-22/wp-content/uploads/2021/12/AAAI-22_Accepted_Paper_List_Main_Technical_Track.pdf),[2021](https://aaai.org/Conferences/AAAI-21/wp-content/uploads/2020/12/AAAI-21_Accepted-Paper-List.Main_.Technical.Track_.pdf),[2020](https://aaai.org/Conferences/AAAI-20/wp-content/uploads/2020/01/AAAI-20-Accepted-Paper-List.pdf)
- [AISTATS](https://dblp.uni-trier.de/search?q=federate%20venue%3AAISTATS%3A) [2022](http://proceedings.mlr.press/v151/), [2021](http://proceedings.mlr.press/v130/),[2020](http://proceedings.mlr.press/v108/)



| Title                                                        | Affiliation                                                  | Venue   | Year | TL;DR                     | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------- | ---- | ------------------------- | ------------------------------------------------------------ |
| Towards Understanding Biased Client Selection in Federated Learning. | CMU                                                          | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/jee-cho22a.html).] [code](https://proceedings.mlr.press/v151/jee-cho22a/jee-cho22a-supp.zip) |
| FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning | KAUST                                                        | AISTATS | 2022 | FLIX[^FLIX]               | [[PUB](https://proceedings.mlr.press/v151/gasanov22a.html).] [PDF](https://arxiv.org/abs/2111.11556) [code](https://proceedings.mlr.press/v151/gasanov22a/gasanov22a-supp.zip) |
| Sharp Bounds for Federated Averaging (Local SGD) and Continuous Perspective. | Stanford                                                     | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/glasgow22a.html).] [PDF](https://arxiv.org/abs/2111.03741) [Code](https://github.com/hongliny/sharp-bounds-for-fedavg-and-continuous-perspective) |
| Federated Reinforcement Learning with Environment Heterogeneity. | PKU                                                          | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/jin22a.html).] [PDF](https://arxiv.org/abs/2204.02634) [Code](https://github.com/pengyang7881187/fedrl) |
| Federated Myopic Community Detection with One-shot Communication | Purdue                                                       | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/ke22a.html).] [PDF](https://arxiv.org/abs/2106.07255) |
| Asynchronous Upper Confidence Bound Algorithms for Federated Linear Bandits. | University of Virginia                                       | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/li22e.html).] [PDF](https://arxiv.org/abs/2110.01463) [Code](https://github.com/cyrilli/Async-LinUCB) |
| Towards Federated Bayesian Network Structure Learning with Continuous Optimization. | CMU                                                          | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/ng22a.html).] [PDF](https://arxiv.org/abs/2110.09356) [Code](https://github.com/ignavierng/notears-admm) |
| Federated Learning with Buffered Asynchronous Aggregation    | Meta AI                                                      | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/nguyen22b.html).] [PDF](https://arxiv.org/abs/2106.06639) [video](https://www.youtube.com/watch?v=Ui-OGUAieNY&ab_channel=FederatedLearningOneWorldSeminar) |
| Differentially Private Federated Learning on Heterogeneous Data. | Stanford                                                     | AISTATS | 2022 | DP-SCAFFOLD[^DP-SCAFFOLD] | [[PUB](https://proceedings.mlr.press/v151/noble22a.html).] [PDF](https://arxiv.org/abs/2111.09278) [Code](https://github.com/maxencenoble/Differential-Privacy-for-Heterogeneous-Federated-Learning) |
| SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification | Princeton                                                    | AISTATS | 2022 | SparseFed[^SparseFed]     | [[PUB](https://proceedings.mlr.press/v151/panda22a.html).] [PDF](https://arxiv.org/abs/2112.06274) [Code](https://github.com/sparsefed/sparsefed) [video](https://www.youtube.com/watch?v=TXG7ZScheas&ab_channel=GoogleTechTalks) |
| Basis Matters: Better Communication-Efficient Second Order Methods for Federated Learning | KAUST                                                        | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/qian22a.html).] [PDF](https://arxiv.org/abs/2111.01847) |
| Federated Functional Gradient Boosting.                      | University of Pennsylvania                                   | AISTATS | 2022 |                           | [[PUB](https://proceedings.mlr.press/v151/shen22a.html).] [PDF](https://arxiv.org/abs/2103.06972) [Code](https://github.com/shenzebang/Federated-Learning-Pytorch) |
| QLSD: Quantised Langevin Stochastic Dynamics for Bayesian Federated Learning. | Criteo AI Lab                                                | AISTATS | 2022 | QLSD[^QLSD]               | [[PUB](https://proceedings.mlr.press/v151/vono22a.html).] [PDF](https://arxiv.org/abs/2106.00797) [Code](https://proceedings.mlr.press/v151/vono22a/vono22a-supp.zip) [video](https://www.youtube.com/watch?v=fY8V184It1g&ab_channel=FederatedLearningOneWorldSeminar) |
| Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting **`kg.`** | ZJU                                                          | IJCAI   | 2022 | MaKEr[^MaKEr]             | [[PUB](https://www.ijcai.org/proceedings/2022/273).] [[PDF]](https://doi.org/10.48550/arXiv.2205.04692) [[Code](https://github.com/zjukg/maker)] |
| Personalized Federated Learning With a Graph                 | UTS                                                          | IJCAI   | 2022 | SFL[^SFL]                 | [[PUB](https://www.ijcai.org/proceedings/2022/357).] [[PDF](https://arxiv.org/abs/2203.00829)] [[Code](https://github.com/dawenzi098/SFL-Structural-Federated-Learning)] |
| Vertically Federated Graph Neural Network for Privacy-Preserving Node Classification | ZJU                                                          | IJCAI   | 2022 | VFGNN[^VFGNN]             | [[PUB](https://www.ijcai.org/proceedings/2022/272).] [[PDF](https://arxiv.org/abs/2005.11903)] |
| Adapt to Adaptation: Learning Personalization for Cross-Silo Federated Learning |                                                              | IJCAI   | 2022 |                           | [[PUB](https://www.ijcai.org/proceedings/2022/301).] [PDF](https://arxiv.org/abs/2110.08394) [Code](https://github.com/ljaiverson/pFL-APPLE) |
| Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning |                                                              | IJCAI   | 2022 | Fed-ET[^Fed-ET]           | [[PUB](https://www.ijcai.org/proceedings/2022/399).] [PDF](https://arxiv.org/abs/2204.12703) |
| Private Semi-Supervised Federated Learning.                  |                                                              | IJCAI   | 2022 |                           | [[PUB](https://www.ijcai.org/proceedings/2022/279).]         |
| Continual Federated Learning Based on Knowledge Distillation. |                                                              | IJCAI   | 2022 |                           | [[PUB](https://doi.org/10.24963/ijcai.2022/306).]            |
| Federated Learning on Heterogeneous and Long-Tailed Data via Classifier Re-Training with Federated Features |                                                              | IJCAI   | 2022 | CReFF[^CReFF]             | [[PUB](https://www.ijcai.org/proceedings/2022/308).] [PDF](https://arxiv.org/abs/2204.13399) [Code](https://github.com/shangxinyi/CReFF-FL) |
| Federated Multi-Task Attention for Cross-Individual Human Activity Recognition |                                                              | IJCAI   | 2022 |                           | [[PUB](https://www.ijcai.org/proceedings/2022/475).]         |
| Personalized Federated Learning with Contextualized Generalization. |                                                              | IJCAI   | 2022 |                           | [[PUB](https://www.ijcai.org/proceedings/2022/311).] [PDF](https://arxiv.org/abs/2106.13044) |
| Shielding Federated Learning: Robust Aggregation with Adaptive Client Selection. |                                                              | IJCAI   | 2022 |                           | [[PUB](https://www.ijcai.org/proceedings/2022/106).] [PDF](https://arxiv.org/abs/2204.13256) |
| FedCG: Leverage Conditional GAN for Protecting Privacy and Maintaining Competitive Performance in Federated Learning |                                                              | IJCAI   | 2022 | FedCG[^FedCG]             | [[PUB](https://www.ijcai.org/proceedings/2022/324).] [PDF](https://arxiv.org/abs/2111.08211) [Code](https://github.com/FederatedAI/research/tree/main/publications/FedCG) |
| FedDUAP: Federated Learning with Dynamic Update and Adaptive Pruning Using Shared Data on the Server. |                                                              | IJCAI   | 2022 | FedDUAP[^FedDUAP]         | [[PUB](https://www.ijcai.org/proceedings/2022/385).] [PDF](https://arxiv.org/abs/2204.11536) |
| Towards Verifiable Federated Learning **`surv.`**            |                                                              | IJCAI   | 2022 |                           | [[PUB](https://www.ijcai.org/proceedings/2022/792).] [PDF](https://arxiv.org/abs/2202.08310) |
| HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images | CUHK; BUAA                                                   | AAAI    | 2022 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/19993).] [PDF](https://arxiv.org/abs/2112.10775) [[Code]](https://github.com/med-air/HarmoFL) [[解读](https://zhuanlan.zhihu.com/p/472555067)] |
| Federated Learning for Face Recognition with Gradient Correction | BUPT                                                         | AAAI    | 2022 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20095).] [PDF](https://arxiv.org/abs/2112.07246) |
| SpreadGNN: Decentralized Multi-Task Federated Learning for Graph Neural Networks on Molecular Data | USC                                                          | AAAI    | 2022 | SpreadGNN[^SpreadGNN]     | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20643).] [PDF](https://arxiv.org/abs/2106.02743) [[Code]](https://github.com/FedML-AI/SpreadGNN) [[解读](https://zhuanlan.zhihu.com/p/429720860)] |
| SmartIdx: Reducing Communication Cost in Federated Learning by Exploiting the CNNs Structures | HIT; PCL                                                     | AAAI    | 2022 | SmartIdx[^SmartIdx]       | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20345).] [Code](https://github.com/wudonglei99/smartidx) |
| Bridging between Cognitive Processing Signals and Linguistic Features via a Unified Attentional Network | TJU                                                          | AAAI    | 2022 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/19878).] [PDF](https://arxiv.org/abs/2112.08831) |
| Seizing Critical Learning Periods in Federated Learning      | SUNY-Binghamton University                                   | AAAI    | 2022 | FedFIM[^FedFIM]           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20859).] [PDF](https://arxiv.org/abs/2109.05613) |
| Coordinating Momenta for Cross-silo Federated Learning       | University of Pittsburgh                                     | AAAI    | 2022 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20853).] [PDF](https://arxiv.org/abs/2102.03970) |
| FedProto: Federated Prototype Learning over Heterogeneous Devices | UTS                                                          | AAAI    | 2022 | FedProto[^FedProto]       | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20819).] [PDF](https://arxiv.org/abs/2105.00243) [[Code]](https://github.com/yuetan031/fedproto) |
| FedSoft: Soft Clustered Federated Learning with Proximal Local Updating | CMU                                                          | AAAI    | 2022 | FedSoft[^FedSoft]         | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20785).] [PDF](https://arxiv.org/abs/2112.06053) [Code](https://github.com/ycruan/FedSoft) |
| Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better | The University of Texas at Austin                            | AAAI    | 2022 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20555).] [PDF](https://arxiv.org/abs/2112.09824) [[Code]](https://github.com/bibikar/feddst) |
| FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition | National Taiwan University                                   | AAAI    | 2022 | FedFR[^FedFR]             | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20057).] [PDF](https://arxiv.org/abs/2112.12496) [[Code]](https://github.com/jackie840129/fedfr) |
| SplitFed: When Federated Learning Meets Split Learning       | CSIRO                                                        | AAAI    | 2022 | SplitFed[^SplitFed]       | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20825).] [PDF](https://arxiv.org/abs/2004.12088) [[Code]](https://github.com/chandra2thapa/SplitFed-When-Federated-Learning-Meets-Split-Learning) |
| Efficient Device Scheduling with Multi-Job Federated Learning | Soochow University                                           | AAAI    | 2022 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/21235).] [PDF](https://arxiv.org/abs/2112.05928) |
| Implicit Gradient Alignment in Distributed and Federated Learning | IIT Kanpur                                                   | AAAI    | 2022 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20597).] [PDF](https://arxiv.org/abs/2106.13897) |
| Federated Nearest Neighbor Classification with a Colony of Fruit-Flies | IBM Research                                                 | AAAI    | 2022 | FlyNNFL[^FlyNNFL]         | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/20775).] [PDF](https://arxiv.org/abs/2112.07157) [Code](https://github.com/rithram/flynn) |
| Federated Learning with Sparsification-Amplified Privacy and Adaptive Optimization |                                                              | IJCAI   | 2021 |                           | [[PUB](https://www.ijcai.org/proceedings/2021/202).] [PDF](https://arxiv.org/abs/2008.01558) [Video](https://papertalk.org/papertalks/35198) |
| Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning |                                                              | IJCAI   | 2021 |                           | [[PUB](https://www.ijcai.org/proceedings/2021/352).] [PDF](https://arxiv.org/abs/2106.12300) |
| FedSpeech: Federated Text-to-Speech with Continual Learning  |                                                              | IJCAI   | 2021 | FedSpeech[^FedSpeech]     | [[PUB](https://www.ijcai.org/proceedings/2021/527).] [PDF](https://arxiv.org/abs/2110.07216) |
| Practical One-Shot Federated Learning for Cross-Silo Setting |                                                              | IJCAI   | 2021 | FedKT[^FedKT]             | [[PUB](https://www.ijcai.org/proceedings/2021/205).] [PDF](https://arxiv.org/abs/2010.01017) [Code](https://github.com/QinbinLi/FedKT) |
| Federated Model Distillation with Noise-Free Differential Privacy |                                                              | IJCAI   | 2021 | FEDMD-NFDP[^FEDMD-NFDP]   | [[PUB](https://www.ijcai.org/proceedings/2021/216).] [PDF](https://arxiv.org/abs/2202.08310) [Video](https://papertalk.org/papertalks/35184) |
| LDP-FL: Practical Private Aggregation in Federated Learning with Local Differential Privacy |                                                              | IJCAI   | 2021 | LDP-FL[^LDP-FL]           | [[PUB](https://www.ijcai.org/proceedings/2021/217).] [PDF](https://arxiv.org/abs/2007.15789) |
| Federated Learning with Fair Averaging. :fire:               |                                                              | IJCAI   | 2021 | FedFV[^FedFV]             | [[PUB](https://www.ijcai.org/proceedings/2021/223).] [PDF](https://arxiv.org/abs/2104.14937) [Code](https://github.com/WwZzz/easyFL) |
| H-FL: A Hierarchical Communication-Efficient and Privacy-Protected Architecture for Federated Learning. |                                                              | IJCAI   | 2021 | H-FL[^H-FL]               | [[PUB](https://www.ijcai.org/proceedings/2021/67).] [PDF](https://arxiv.org/abs/2106.00275) |
| Communication-efficient and Scalable Decentralized Federated Edge Learning. |                                                              | IJCAI   | 2021 |                           | [[PUB](https://www.ijcai.org/proceedings/2021/720).]         |
| Secure Bilevel Asynchronous Vertical Federated Learning with Backward Updating | Xidian University; JD Tech                                   | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17301).] [PDF](https://arxiv.org/abs/2103.00958) [video](https://slideslive.com/38947765/secure-bilevel-asynchronous-vertical-federated-learning-with-backward-updating) |
| FedRec++: Lossless Federated Recommendation with Explicit Feedback | SZU                                                          | AAAI    | 2021 | FedRec++[^FedRec++]       | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/16546).] [video](https://slideslive.com/38947798/fedrec-lossless-federated-recommendation-with-explicit-feedback) |
| Federated Multi-Armed Bandits                                | University of Virginia                                       | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17156).] [PDF](https://arxiv.org/abs/2101.12204) [[Code]](https://github.com/ShenGroup/FMAB) [video](https://slideslive.com/38947985/federated-multiarmed-bandits) |
| On the Convergence of Communication-Efficient Local SGD for Federated Learning | Temple University; University of Pittsburgh                  | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/16920).] [video](https://slideslive.com/38948341/on-the-convergence-of-communicationefficient-local-sgd-for-federated-learning) |
| FLAME: Differentially Private Federated Learning in the Shuffle Model | Renmin University of China; Kyoto University                 | AAAI    | 2021 | FLAME_D[^FLAME_D]         | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17053).] [PDF](https://arxiv.org/abs/2009.08063) [video](https://slideslive.com/38948496/flame-differentially-private-federated-learning-in-the-shuffle-model) [[Code]](https://github.com/Rachelxuan11/FLAME) |
| Toward Understanding the Influence of Individual Clients in Federated Learning | SJTU; The University of Texas at Dallas                      | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17263).] [PDF](https://arxiv.org/abs/2012.10936) [video](https://slideslive.com/38948549/toward-understanding-the-influence-of-individual-clients-in-federated-learning) |
| Provably Secure Federated Learning against Malicious Clients | Duke University                                              | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/16849).] [PDF](https://arxiv.org/abs/2102.01854) [video](https://www.youtube.com/watch?v=LP4uqW18yA0&ab_channel=PurdueCERIAS) [slides](https://people.duke.edu/~zg70/code/Secure_Federated_Learning.pdf) |
| Personalized Cross-Silo Federated Learning on Non-IID Data   | Simon Fraser University; McMaster University                 | AAAI    | 2021 | FedAMP[^FedAMP]           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/16960).] [PDF](https://arxiv.org/abs/2007.03797) [video](https://slideslive.com/38948676/personalized-crosssilo-federated-learning-on-noniid-data) [UC.](https://github.com/TsingZ0/PFL-Non-IID) |
| Model-Sharing Games: Analyzing Federated Learning under Voluntary Participation | Cornell University                                           | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/16669).] [PDF](https://arxiv.org/abs/2010.00753) [[Code]](https://github.com/kpdonahue/model_sharing_games) [video](https://slideslive.com/38948684/modelsharing-games-analyzing-federated-learning-under-voluntary-participation) |
| Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning | University of Nevada; IBM Research                           | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17291).] [PDF](https://arxiv.org/abs/2102.00655) [video](https://slideslive.com/38949098/curse-or-redemption-how-data-heterogeneity-affects-the-robustness-of-federated-learning) |
| Game of Gradients: Mitigating Irrelevant Clients in Federated Learning | IIT Bombay; IBM Research                                     | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17093).] [PDF](https://arxiv.org/abs/2110.12257) [Code](https://github.com/nlokeshiisc/sfedavg-aaai21) [video](https://slideslive.com/38949109/game-of-gradients-mitigating-irrelevant-clients-in-federated-learning) [Supplementary](https://github.com/nlokeshiisc/SFedAvg-AAAI21) |
| Federated Block Coordinate Descent Scheme for Learning Global and Personalized Models | CUHK; Arizona State University                               | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17240).] [PDF](https://arxiv.org/abs/2012.13900) [video](https://slideslive.com/38949195/federated-block-coordinate-descent-scheme-for-learning-global-and-personalized-models) [[Code]](https://github.com/REIYANG/FedBCD) |
| Addressing Class Imbalance in Federated Learning             | Northwestern University                                      | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17219).] [PDF](https://arxiv.org/abs/2008.06217) [video](https://slideslive.com/38949283/adressing-class-imbalance-in-federated-learning) [[Code]](https://github.com/balanced-fl/Addressing-Class-Imbalance-FL) [[解读]](https://zhuanlan.zhihu.com/p/443009189) |
| Defending against Backdoors in Federated Learning with Robust Learning Rate | The University of Texas at Dallas                            | AAAI    | 2021 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/17118).] [PDF](https://arxiv.org/abs/2007.03767) [video](https://slideslive.com/38949344/defending-against-backdoors-in-federated-learning-with-robust-learning-rate) [[Code]](https://github.com/TinfoilHat0/Defending-Against-Backdoors-with-Robust-Learning-Rate) |
| Free-rider Attacks on Model Aggregation in Federated Learning | Accenture Labs                                               | AISTAT  | 2021 |                           | [[PUB](http://proceedings.mlr.press/v130/fraboni21a/fraboni21a.pdf).] [PDF](https://arxiv.org/abs/2006.11901) [Code](https://github.com/Accenture/Labs-Federated-Learning) [video](https://papertalk.org/papertalks/27640) [Supplementary](http://proceedings.mlr.press/v130/fraboni21a/fraboni21a-supp.pdf) |
| Federated f-differential privacy                             | University of Pennsylvania                                   | AISTAT  | 2021 |                           | [[PUB](http://proceedings.mlr.press/v130/zheng21a/zheng21a.pdf).] [[Code]](https://github.com/enosair/federated-fdp) [video](https://papertalk.org/papertalks/27595) [Supplementary](http://proceedings.mlr.press/v130/zheng21a/zheng21a-supp.pdf) |
| Federated learning with compression: Unified analysis and sharp guarantees :fire: | The Pennsylvania State University; The University of Texas at Austin | AISTAT  | 2021 |                           | [[PUB](http://proceedings.mlr.press/v130/haddadpour21a/haddadpour21a.pdf).] [PDF](https://arxiv.org/abs/2007.01154) [[Code]](https://github.com/MLOPTPSU/FedTorch) [video](https://papertalk.org/papertalks/27584) [Supplementary](http://proceedings.mlr.press/v130/haddadpour21a/haddadpour21a-supp.pdf) |
| Shuffled Model of Differential Privacy in Federated Learning | UCLA; Google                                                 | AISTAT  | 2021 |                           | [[PUB](http://proceedings.mlr.press/v130/girgis21a/girgis21a.pdf).] [video](https://papertalk.org/papertalks/27565) [Supplementary](http://proceedings.mlr.press/v130/girgis21a/girgis21a-supp.pdf) |
| Convergence and Accuracy Trade-Offs in Federated Learning and Meta-Learning | Google                                                       | AISTAT  | 2021 |                           | [[PUB](http://proceedings.mlr.press/v130/charles21a/charles21a.pdf).] [PDF](https://arxiv.org/abs/2103.05032) [video](https://papertalk.org/papertalks/27559) [Supplementary](http://proceedings.mlr.press/v130/charles21a/charles21a-supp.pdf) |
| Federated Multi-armed Bandits with Personalization           | University of Virginia; The Pennsylvania State University    | AISTAT  | 2021 |                           | [[PUB](http://proceedings.mlr.press/v130/shi21c/shi21c.pdf).] [PDF](https://arxiv.org/abs/2102.13101) [[Code]](https://github.com/ShenGroup/PF_MAB) [video](https://papertalk.org/papertalks/27521) [Supplementary](http://proceedings.mlr.press/v130/shi21c/shi21c-supp.pdf) |
| Towards Flexible Device Participation in Federated Learning  | CMU; SYSU                                                    | AISTAT  | 2021 |                           | [[PUB](http://proceedings.mlr.press/v130/ruan21a/ruan21a.pdf).] [PDF](https://arxiv.org/abs/2006.06954) [video](https://papertalk.org/papertalks/27467) [Supplementary](http://proceedings.mlr.press/v130/ruan21a/ruan21a-supp.pdf) |
| Federated Meta-Learning for Fraudulent Credit Card Detection |                                                              | IJCAI   | 2020 |                           | [[PUB](https://www.ijcai.org/proceedings/2020/642).] [Video](https://www.ijcai.org/proceedings/2020/video/23994) |
| A Multi-player Game for Studying Federated Learning Incentive Schemes |                                                              | IJCAI   | 2020 | FedGame[^FedGame]         | [[PUB](https://www.ijcai.org/proceedings/2020/769).] [Code](https://github.com/benggggggggg/fedgame)[[解读](https://zhuanlan.zhihu.com/p/353868739)] |
| Practical Federated Gradient Boosting Decision Trees         | NUS; UWA                                                     | AAAI    | 2020 | SimFL[^SimFL]             | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/5895).] [PDF](https://arxiv.org/abs/1911.04206) [[Code]](https://github.com/Xtra-Computing/PrivML) |
| Federated Learning for Vision-and-Language Grounding Problems | PKU; Tencent                                                 | AAAI    | 2020 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/6824).] |
| Federated Latent Dirichlet Allocation: A Local Differential Privacy Based Framework | BUAA                                                         | AAAI    | 2020 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/6096).] |
| Federated Patient Hashing                                    | Cornell University                                           | AAAI    | 2020 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/6121).] |
| Robust Federated Learning via Collaborative Machine Teaching | Symantec Research Labs; KAUST                                | AAAI    | 2020 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/5826).] [PDF](https://arxiv.org/abs/1905.02941) |
| FedVision: An Online Visual Object Detection Platform Powered by Federated Learning | WeBank                                                       | AAAI    | 2020 |                           | [[PUB](https://ojs.aaai.org/index.php/AAAI/article/view/7021).] [PDF](https://arxiv.org/abs/2001.06202) [Code](https://github.com/FederatedAI/FedVision) |
| FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization | UC Santa Barbara; UT Austin                                  | AISTAT  | 2020 |                           | [[PUB](http://proceedings.mlr.press/v108/reisizadeh20a/reisizadeh20a.pdf).] [PDF](https://arxiv.org/abs/1909.13014) [video](https://papertalk.org/papertalks/7961) [Supplementary](http://proceedings.mlr.press/v108/reisizadeh20a/reisizadeh20a-supp.pdf) |
| How To Backdoor Federated Learning :fire:                    | Cornell Tech                                                 | AISTAT  | 2020 |                           | [[PUB](http://proceedings.mlr.press/v108/bagdasaryan20a/bagdasaryan20a.pdf).] [PDF](https://arxiv.org/abs/1807.00459) [video](https://papertalk.org/papertalks/8046) [[Code]](https://github.com/ebagdasa/backdoor_federated_learning) [Supplementary](http://proceedings.mlr.press/v108/bagdasaryan20a/bagdasaryan20a-supp.pdf) |
| Federated Heavy Hitters Discovery with Differential Privacy  | RPI; Google                                                  | AISTAT  | 2020 |                           | [[PUB](http://proceedings.mlr.press/v108/zhu20a/zhu20a.pdf).] [PDF](https://arxiv.org/abs/1902.08534) [video](https://papertalk.org/papertalks/8129) [Supplementary](http://proceedings.mlr.press/v108/zhu20a/zhu20a-supp.pdf) |
| Multi-Agent Visualization for Explaining Federated Learning  | WeBank                                                       | IJCAI   | 2019 |                           | [[PUB](https://www.ijcai.org/proceedings/2019/960).] [Video](https://youtu.be/NPGf_OJrzOg) |





## FL in top ML conference and journal

In this section, we will summarize Federated Learning papers accepted by top ML(machine learning) conference and journal, Including [NeurIPS](https://dblp.uni-trier.de/db/conf/nips/index.html)(Annual Conference on Neural Information Processing Systems), [ICML](https://dblp.uni-trier.de/db/conf/icml/index.html)(International Conference on Machine Learning), [ICLR](https://dblp.uni-trier.de/db/conf/iclr/index.html)(International Conference on Learning Representations), [COLT](https://dblp.org/db/conf/colt/index.html)(Annual Conference Computational Learning Theory) and [UAI](https://dblp.org/db/conf/uai/index.html)(Conference on Uncertainty in Artificial Intelligence).

- [NeurIPS](https://dblp.uni-trier.de/search?q=federate%20venue%3ANeurIPS%3A) [2021](https://papers.nips.cc/paper/2021), [2020](https://papers.nips.cc/paper/2020), [2018](https://papers.nips.cc/paper/2018), [2017](https://papers.nips.cc/paper/2017)
- [ICML](https://dblp.uni-trier.de/search?q=federate%20venue%3AICML%3A) [2022](https://icml.cc/Conferences/2022/Schedule?type=Poster), [2021](https://icml.cc/Conferences/2021/Schedule?type=Poster), [2020](https://icml.cc/Conferences/2020/Schedule?type=Poster), [2019](https://icml.cc/Conferences/2019/Schedule?type=Poster)
- [ICLR](https://dblp.uni-trier.de/search?q=federate%20venue%3AICLR%3A) [2022](https://openreview.net/group?id=ICLR.cc/2022/Conference),[2021](https://openreview.net/group?id=ICLR.cc/2021/Conference), [2020](https://openreview.net/group?id=ICLR.cc/2020/Conference)
- [COLT](https://dblp.org/search?q=federated%20venue%3ACOLT%3A) NULL
- [UAI](https://dblp.org/search?q=federated%20venue%3AUAI%3A) 2021



| Title                                                        | Affiliation                                                  | Venue          | Year | TL;DR                       | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ | -------------- | ---- | --------------------------- | ------------------------------------------------------------ |
| Fast Composite Optimization and Statistical Recovery in Federated Learning | SJTU                                                         | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/bao22b.html).] [PDF](https://arxiv.org/abs/2207.08204) [Code](https://github.com/MingruiLiu-ML-Lab/Federated-Sparse-Learning) |
| Personalization Improves Privacy-Accuracy Tradeoffs in Federated Learning | NYU                                                          | ICML           | 2022 | PPSGD[^PPSGD]               | [[PUB](https://proceedings.mlr.press/v162/bietti22a.html).] [PDF](https://arxiv.org/abs/2202.05318) [Code](https://github.com/albietz/ppsgd) |
| The Fundamental Price of Secure Aggregation in Differentially Private Federated Learning :fire: | Stanford; Google Research                                    | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/chen22c.html).] [PDF](https://arxiv.org/abs/2203.03761) [code](https://github.com/google-research/federated/tree/master/private_linear_compression) [slides](https://icml.cc/media/icml-2022/Slides/17529.pdf) |
| The Poisson Binomial Mechanism for Unbiased Federated Learning with Secure Aggregation | Stanford; Google Research                                    | ICML           | 2022 | PBM[^PBM]                   | [[PUB](https://proceedings.mlr.press/v162/chen22s.html).] [PDF](https://arxiv.org/abs/2207.09916) [Code](https://github.com/WeiNingChen/pbm) |
| DisPFL: Towards Communication-Efficient Personalized Federated Learning via Decentralized Sparse Training | USTC                                                         | ICML           | 2022 | DisPFL[^DisPFL]             | [[PUB](https://proceedings.mlr.press/v162/dai22b.html).] [PDF](https://arxiv.org/abs/2206.00187) [Code](https://github.com/rong-dai/DisPFL) |
| FedNew: A Communication-Efficient and Privacy-Preserving Newton-Type Method for Federated Learning | University of Oulu                                           | ICML           | 2022 | FedNew[^FedNew]             | [[PUB](https://proceedings.mlr.press/v162/elgabli22a.html).] [PDF](https://arxiv.org/abs/2206.08829) [code](https://github.com/aelgabli/FedNew) |
| DAdaQuant: Doubly-adaptive quantization for communication-efficient Federated Learning | University of Cambridge                                      | ICML           | 2022 | DAdaQuant[^DAdaQuant]       | [[PUB](https://proceedings.mlr.press/v162/honig22a.html).] [PDF](https://arxiv.org/abs/2111.00465) [slides](https://icml.cc/media/icml-2022/Slides/16009.pdf) [Code](https://media.icml.cc/Conferences/ICML2022/supplementary/honig22a-supp.zip) |
| Accelerated Federated Learning with Decoupled Adaptive Optimization | Auburn University                                            | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/jin22e.html).] [PDF](https://arxiv.org/abs/2207.07223) |
| Federated Reinforcement Learning: Linear Speedup Under Markovian Sampling | Georgia Tech                                                 | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/khodadadian22a.html).] [PDF](https://arxiv.org/abs/2206.10185) |
| Multi-Level Branched Regularization for Federated Learning   | Seoul National University                                    | ICML           | 2022 | FedMLB[^FedMLB]             | [[PUB](https://proceedings.mlr.press/v162/kim22a.html).] [PDF](https://arxiv.org/abs/2207.06936) [Code](https://github.com/jinkyu032/FedMLB) [Page](http://cvlab.snu.ac.kr/research/FedMLB/) |
| FedScale: Benchmarking Model and System Performance of Federated Learning at Scale :fire: | University of Michigan                                       | ICML           | 2022 | FedScale[^FedScale]         | [[PUB](https://proceedings.mlr.press/v162/lai22a.html).] [PDF](https://arxiv.org/abs/2105.11367) [code](https://github.com/SymbioticLab/FedScale) |
| Federated Learning with Positive and Unlabeled Data          | XJTU                                                         | ICML           | 2022 | FedPU[^FedPU]               | [[PUB](https://proceedings.mlr.press/v162/lin22b.html).] [PDF](https://arxiv.org/abs/2106.10904) [Code](https://github.com/littlesunlxy/fedpu-torch) |
| Deep Neural Network Fusion via Graph Matching with Applications to Model Ensemble and Federated Learning | SJTU                                                         | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/liu22k.html).] [code](https://github.com/Thinklab-SJTU/GAMF) |
| Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering | University of Michigan                                       | ICML           | 2022 | Orchestra[^Orchestra]       | [[PUB](https://proceedings.mlr.press/v162/lubana22a.html).] [PDF](https://arxiv.org/abs/2205.11506) [code](https://github.com/akhilmathurs/orchestra) |
| Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring | USTC                                                         | ICML           | 2022 | DFL[^DFL]                   | [[PUB](https://proceedings.mlr.press/v162/luo22b.html).] [PDF](https://arxiv.org/abs/2206.06818) [Code](https://github.com/luozhengquan/DFL) [slides](https://icml.cc/media/icml-2022/Slides/16881.pdf) [[解读](https://www.bilibili.com/read/cv17092678)] |
| Architecture Agnostic Federated Learning for Neural Networks | The University of Texas at Austin                            | ICML           | 2022 | FedHeNN[^FedHeNN]           | [[PUB](https://proceedings.mlr.press/v162/makhija22a.html).] [PDF](https://proceedings.mlr.press/v162/zhang22p.html) [Slides](https://icml.cc/media/icml-2022/Slides/16926.pdf) |
| Personalized Federated Learning through Local Memorization   | Inria                                                        | ICML           | 2022 | KNN-PER[^KNN-PER]           | [[PUB](https://proceedings.mlr.press/v162/marfoq22a.html).] [PDF](https://arxiv.org/abs/2111.09360) [code](https://github.com/omarfoq/knn-per) |
| Proximal and Federated Random Reshuffling                    | KAUST                                                        | ICML           | 2022 | ProxRR[^ProxRR]             | [[PUB](https://proceedings.mlr.press/v162/mishchenko22a.html).] [PDF](https://arxiv.org/abs/2102.06704) [code](https://github.com/konstmish/rr_prox_fed) |
| Federated Learning with Partial Model Personalization        | University of Washington                                     | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/pillutla22a.html).] [PDF](https://arxiv.org/abs/2204.03809) [code](https://github.com/krishnap25/FL_partial_personalization) |
| Generalized Federated Learning via Sharpness Aware Minimization | University of South Florida                                  | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/qu22a.html).] [PDF](https://arxiv.org/abs/2206.02618) |
| FedNL: Making Newton-Type Methods Applicable to Federated Learning | KAUST                                                        | ICML           | 2022 | FedNL[^FedNL]               | [[PUB](https://proceedings.mlr.press/v162/safaryan22a.html).] [PDF](https://arxiv.org/abs/2106.02969) [video](https://www.youtube.com/watch?v=_VYCEWT17R0&ab_channel=FederatedLearningOneWorldSeminar) [slides](https://icml.cc/media/icml-2022/Slides/17084.pdf) |
| Federated Minimax Optimization: Improved Convergence Analyses and Algorithms | CMU                                                          | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/sharma22c.html).] [PDF](https://arxiv.org/abs/2203.04850) [slides](https://icml.cc/media/icml-2022/Slides/17435.pdf) |
| Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning | Hong Kong Baptist University                                 | ICML           | 2022 | VFL[^VFL]                   | [[PUB](https://proceedings.mlr.press/v162/tang22d.html).] [PDF](https://arxiv.org/abs/2206.02465) [code](https://github.com/wizard1203/VHL) [[解读](https://zhuanlan.zhihu.com/p/548508633)] |
| FedNest: Federated Bilevel, Minimax, and Compositional Optimization | University of Michigan                                       | ICML           | 2022 | FedNest[^FedNest]           | [[PUB](https://proceedings.mlr.press/v162/tarzanagh22a.html).] [PDF](https://arxiv.org/abs/2205.02215) [code](https://github.com/mc-nya/FedNest) |
| EDEN: Communication-Efficient and Robust Distributed Mean Estimation for Federated Learning | VMware Research                                              | ICML           | 2022 | EDEN[^EDEN]                 | [[PUB](https://proceedings.mlr.press/v162/vargaftik22a.html).] [PDF](https://arxiv.org/abs/2108.08842) [code](https://github.com/amitport/EDEN-Distributed-Mean-Estimation) |
| Communication-Efficient Adaptive Federated Learning          | Pennsylvania State University                                | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/wang22o.html).] [PDF](https://arxiv.org/abs/2205.02719) |
| ProgFed: Effective, Communication, and Computation Efficient Federated Learning by Progressive Training | CISPA Helmholz Center for Information Security               | ICML           | 2022 | ProgFed[^ProgFed]           | [[PUB](https://proceedings.mlr.press/v162/wang22y.html).] [PDF](https://arxiv.org/abs/2110.05323) [slides](https://icml.cc/media/icml-2022/Slides/16194_hmjFNsN.pdf) [code](https://github.com/a514514772/ProgFed) |
| Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification :fire: | University of Maryland                                       | ICML           | 2022 | breaching[^breaching]       | [[PUB](https://proceedings.mlr.press/v162/wen22a.html).] [PDF](https://arxiv.org/abs/2202.00580) [code](https://github.com/JonasGeiping/breaching) |
| Anarchic Federated Learning                                  | The Ohio State University                                    | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/yang22r.html).] [PDF](https://arxiv.org/abs/2108.09875) |
| QSFL: A Two-Level Uplink Communication Optimization Framework for Federated Learning | Nankai University                                            | ICML           | 2022 | QSFL[^QSFL]                 | [[PUB](https://proceedings.mlr.press/v162/yi22a.html).] [code](https://github.com/LipingYi/QSFL) |
| Bitwidth Heterogeneous Federated Learning with Progressive Weight Dequantization | KAIST                                                        | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/yoon22a.html).] [PDF](https://arxiv.org/abs/2202.11453) |
| Neural Tangent Kernel Empowered Federated Learning           | NC State University                                          | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/yue22a.html).] [PDF](https://arxiv.org/abs/2110.03681) [code](https://github.com/KAI-YUE/ntk-fed) |
| Understanding Clipping for Federated Learning: Convergence and Client-Level Differential Privacy | UMN                                                          | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/zhang22b.html).] [PDF](https://arxiv.org/abs/2106.13673) |
| Personalized Federated Learning via Variational Bayesian Inference | CAS                                                          | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/zhang22o.html).] [PDF](https://arxiv.org/abs/2206.07977) [slides](https://icml.cc/media/icml-2022/Slides/17302.pdf) [UC.](https://github.com/AllenBeau/pFedBayes) |
| Federated Learning with Label Distribution Skew via Logits Calibration | ZJU                                                          | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/zhang22p.html).]   |
| Neurotoxin: Durable Backdoors in Federated Learning          | Southeast University;Princeton                               | ICML           | 2022 | Neurotoxin[^Neurotoxin]     | [[PUB](https://proceedings.mlr.press/v162/zhang22w.html).] [PDF](https://arxiv.org/abs/2206.10341) [code](https://github.com/jhcknzzm/Federated-Learning-Backdoor/) |
| Resilient and Communication Efficient Learning for Heterogeneous Federated Systems | Michigan State University                                    | ICML           | 2022 |                             | [[PUB](https://proceedings.mlr.press/v162/zhu22e.html).]     |
| Bayesian Framework for Gradient Leakage                      | ETH Zurich                                                   | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=f2lrIbGx3x7).] [PDF](https://arxiv.org/abs/2111.04706) [[Code]](https://github.com/eth-sri/bayes-framework-leakage) |
| Federated Learning from only unlabeled data with class-conditional-sharing clients | The University of Tokyo; CUHK                                | ICLR           | 2022 | FedUL[^FedUL]               | [[PUB](https://openreview.net/forum?id=WHA8009laxu).] [[Code]](https://github.com/lunanbit/FedUL) |
| FedChain: Chained Algorithms for Near-Optimal Communication Cost in Federated Learning | CMU; University of Illinois at Urbana-Champaign; University of Washington | ICLR           | 2022 | FedChain[^FedChain]         | [[PUB](https://openreview.net/forum?id=ZaVVVlcdaN).] [PDF](https://arxiv.org/abs/2108.06869.) |
| Acceleration of Federated Learning with Alleviated Forgetting in Local Training | THU                                                          | ICLR           | 2022 | FedReg[^FedReg]             | [[PUB](https://openreview.net/forum?id=541PxiEKN3F).] [PDF](https://arxiv.org/abs/2203.02645) [[Code]](https://github.com/Zoesgithub/FedReg) |
| FedPara: Low-rank Hadamard Product for Communicatkion-Efficient Federated Learning | POSTECH                                                      | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=d71n4ftoCBy).] [PDF](https://arxiv.org/abs/2108.06098) [[Code]](https://github.com/South-hw/FedPara_ICLR22) |
| An Agnostic Approach to Federated Learning with Class Imbalance | University of Pennsylvania                                   | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=Xo0lbDt975).] [[Code]](https://github.com/shenzebang/Federated-Learning-Pytorch) |
| Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization | Michigan State University; The University of Texas at Austin | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=_QLmakITKg).] [PDF](https://arxiv.org/abs/2203.09747) [[Code]](https://github.com/illidanlab/SplitMix) |
| Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models :fire: | University of Maryland; NYU                                  | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=fwzUgo0FM9v).] [PDF](https://arxiv.org/abs/2110.13057) [[Code]](https://github.com/lhfowl/robbing_the_fed) [[Code]](https://github.com/JonasGeiping/breaching) |
| ZeroFL: Efficient On-Device Training for Federated Learning with Local Sparsity | University of Cambridge; University of Oxford                | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=2sDQwC_hmnM).] [PDF](https://arxiv.org/abs/2208.02507) |
| Diverse Client Selection for Federated Learning via Submodular Maximization | Intel; CMU                                                   | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=nwKXyFvaUm).] [[Code]](https://github.com/melodi-lab/divfl) |
| Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank? | Purdue                                                       | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=B7ZbqNLDn-_).] [PDF](https://arxiv.org/abs/2202.00280) [[Code]](https://github.com/shams-sam/FedOptim) |
| Diurnal or Nocturnal? Federated Learning of Multi-branch Networks from Periodically Shifting Distributions :fire: | University of Maryland; Google                               | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=E4EE_ohFGz).] [[Code]](https://github.com/google-research/federated/tree/7525c36324cb022bc05c3fce88ef01147cae9740/periodic_distribution_shift) |
| Towards Model Agnostic Federated Learning Using Knowledge Distillation | EPFL                                                         | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=lQI_mZjvBxj).] [PDF](https://arxiv.org/abs/2110.15210) [Code](https://github.com/AfoninAndrei/ICLR2022) |
| Divergence-aware Federated Self-Supervised Learning          | NTU; SenseTime                                               | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=oVE1z8NlNe).] [PDF](https://arxiv.org/abs/2204.04385) [Code](https://github.com/EasyFL-AI/EasyFL) |
| What Do We Mean by Generalization in Federated Learning? :fire: | Stanford; Google                                             | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=VimqQq-i_Q).] [PDF](https://arxiv.org/abs/2110.14216) [[Code]](https://github.com/google-research/federated/tree/master/generalization) |
| FedBABU: Toward Enhanced Representation for Federated Image Classification | KAIST                                                        | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=HuaYQfggn5u).] [PDF](https://arxiv.org/abs/2106.06042) [[Code]](https://github.com/jhoon-oh/FedBABU) |
| Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing | EPFL                                                         | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=jXKKDEi5vJt).] [PDF](https://arxiv.org/abs/2006.09365) [[Code]](https://github.com/liehe/byzantine-robust-noniid-optimizer) |
| Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters | Aibee                                                        | ICLR Spotlight | 2022 |                             | [[PUB](https://openreview.net/forum?id=7l1IjZVddDW).] [PDF](https://arxiv.org/abs/2201.12467) [Page](https://irvingmeng.github.io/projects/privacyface/) [[解读](https://zhuanlan.zhihu.com/p/484920301)] |
| Hybrid Local SGD for Federated Learning with Heterogeneous Communications | University of Texas; Pennsylvania State University           | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=H0oaWl6THa).]         |
| On Bridging Generic and Personalized Federated Learning for Image Classification | The Ohio State University                                    | ICLR           | 2022 | Fed-RoD[^Fed-RoD]           | [[PUB](https://openreview.net/forum?id=I1hQbx10Kxn).] [PDF](https://arxiv.org/abs/2107.00778) [[Code]](https://github.com/hongyouc/Fed-RoD) |
| Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond | KAIST; MIT                                                   | ICLR           | 2022 |                             | [[PUB](https://openreview.net/forum?id=LdlwbBP2mlq).] [PDF](https://arxiv.org/abs/2110.10342) |
| Constrained differentially private federated learning for low-bandwidth devices |                                                              | UAI            | 2021 |                             | [[PUB](https://proceedings.mlr.press/v161/kerkouche21a.html).] [PDF](https://arxiv.org/abs/2103.00342) |
| Federated stochastic gradient Langevin dynamics              |                                                              | UAI            | 2021 |                             | [[PUB](https://proceedings.mlr.press/v161/mekkaoui21a.html).] [PDF](https://arxiv.org/abs/2004.11231) |
| Federated Learning Based on Dynamic Regularization           | BU; ARM                                                      | ICLR           | 2021 |                             | [[PUB](https://openreview.net/forum?id=B7v4QMR6Z9w).] [PDF](https://arxiv.org/abs/2111.04263) [Code](https://github.com/AntixK/FedDyn) |
| Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning | The Ohio State University                                    | ICLR           | 2021 |                             | [[PUB](https://openreview.net/forum?id=jDdzh5ul-d).] [PDF](https://arxiv.org/abs/2101.11203) |
| HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients | Duke University                                              | ICLR           | 2021 | HeteroFL[^HeteroFL]         | [[PUB](https://openreview.net/forum?id=TNkPBBYFkXg).] [PDF](https://arxiv.org/abs/2010.01264) [[Code]](https://github.com/dem123456789/HeteroFL-Computation-and-Communication-Efficient-Federated-Learning-for-Heterogeneous-Clients) |
| FedMix: Approximation of Mixup under Mean Augmented Federated Learning | KAIST                                                        | ICLR           | 2021 | FedMix[^FedMix]             | [[PUB](https://openreview.net/forum?id=Ogga20D2HO-).] [PDF](https://arxiv.org/abs/2107.00233) |
| Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms :fire: | CMU; Google                                                  | ICLR           | 2021 |                             | [[PUB](https://openreview.net/forum?id=GFsU8a0sGB).] [PDF](https://arxiv.org/abs/2010.05273) [[Code]](https://github.com/alshedivat/fedpa) |
| Adaptive Federated Optimization :fire:                       | Google                                                       | ICLR           | 2021 |                             | [[PUB](https://openreview.net/forum?id=LkFG3lB13U5).] [PDF](https://arxiv.org/abs/2003.00295) [[Code]](https://github.com/google-research/federated/tree/master/optimization) |
| Personalized Federated Learning with First Order Model Optimization | Stanford; NVIDIA                                             | ICLR           | 2021 | FedFomo[^FedFomo]           | [[PUB](https://openreview.net/forum?id=ehJqJQk9cw).] [PDF](https://arxiv.org/abs/2012.08565) [Code](https://github.com/NVlabs/FedFomo) [UC.](https://github.com/TsingZ0/PFL-Non-IID) |
| FedBN: Federated Learning on Non-IID Features via Local Batch Normalization :fire: | Princeton                                                    | ICLR           | 2021 | FedBN[^FedBN]               | [[PUB](https://openreview.net/forum?id=6YEQUn0QICG).] [PDF](https://arxiv.org/abs/2102.07623) [[Code]](https://github.com/med-air/FedBN) |
| FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning | The Ohio State University                                    | ICLR           | 2021 | FedBE[^FedBE]               | [[PUB](https://openreview.net/forum?id=dgtpE6gKjHn).] [PDF](https://arxiv.org/abs/2009.01974) [Code](https://github.com/hongyouc/fedbe) |
| Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning | KAIST                                                        | ICLR           | 2021 |                             | [[PUB](https://openreview.net/forum?id=ce6CFXBh30h).] [PDF](https://arxiv.org/abs/2006.12097) [[Code]](https://github.com/wyjeong/FedMatch) |
| KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation | ZJU                                                          | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/feng21f.html).] [PDF](https://arxiv.org/abs/2011.09757) [Code](https://github.com/FengHZ/KD3A) [[解读](https://mp.weixin.qq.com/s/gItgiZmKUxg0ltaeOVdnRw)] |
| Gradient Disaggregation: Breaking Privacy in Federated Learning by Reconstructing the User Participant Matrix | Harvard University                                           | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/lam21b.html).] [PDF](https://arxiv.org/abs/2106.06089) [video](https://slideslive.com/38958558/gradient-disaggregation-breaking-privacy-in-federated-learning-by-reconstructing-the-user-participant-matrix) [[Code]](https://github.com/gdisag/gradient_disaggregation) |
| FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning Analysis | PKU; Princeton                                               | ICML           | 2021 | FL-NTK[^FL-NTK]             | [[PUB](http://proceedings.mlr.press/v139/huang21c.html).] [PDF](https://arxiv.org/abs/2105.05001) [video](https://slideslive.com/38959650/flntk-a-neural-tangent-kernelbased-framework-for-federated-learning-analysis) |
| Personalized Federated Learning using Hypernetworks :fire:   | Bar-Ilan University; NVIDIA                                  | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/shamsian21a.html).] [PDF](https://arxiv.org/abs/2103.04628) [[Code]](https://github.com/AvivSham/pFedHN) [Page](https://avivsham.github.io/pfedhn/) [video](https://slideslive.com/38959583/personalized-federated-learning-using-hypernetworks) [[解读](https://zhuanlan.zhihu.com/p/431130945)] |
| Federated Composite Optimization                             | Stanford; Google                                             | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/yuan21d.html).] [PDF](https://arxiv.org/abs/2011.08474) [[Code]](https://github.com/hongliny/FCO-ICML21) [video](https://www.youtube.com/watch?v=tKDbc60XJks&ab_channel=FederatedLearningOneWorldSeminar) [slides](https://hongliny.github.io/files/FCO_ICML21/FCO_ICML21_slides.pdf) |
| Exploiting Shared Representations for Personalized Federated Learning | University of Texas at Austin; University of Pennsylvania    | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/collins21a.html).] [PDF](https://arxiv.org/abs/2102.07078) [[Code]](https://github.com/lgcollins/FedRep) [video](https://slideslive.com/38959519/exploiting-shared-representations-for-personalized-federated-learning) |
| Data-Free Knowledge Distillation for Heterogeneous Federated Learning :fire: | Michigan State University                                    | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/zhu21b.html).] [PDF](https://arxiv.org/abs/2105.10056) [[Code]](https://github.com/zhuangdizhu/FedGen) [video](https://slideslive.com/38959429/datafree-knowledge-distillation-for-heterogeneous-federated-learning) |
| Federated Continual Learning with Weighted Inter-client Transfer | KAIST                                                        | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/yoon21b.html).] [PDF](https://arxiv.org/abs/2003.03196) [[Code]](https://github.com/wyjeong/FedWeIT) [video](https://slideslive.com/38959323/federated-continual-learning-with-weighted-interclient-transfer) |
| Federated Deep AUC Maximization for Hetergeneous Data with a Constant Communication Complexity | The University of Iowa                                       | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/yuan21a.html).] [PDF](https://arxiv.org/abs/2102.04635) [Code](https://github.com/optimization-ai/icml2021_feddeepauc_codasca) [Code](https://libauc.org/) [video](https://slideslive.com/38959235/federated-deep-auc-maximization-for-hetergeneous-data-with-a-constant-communication-complexity) |
| Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning | The University of Tokyo                                      | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/murata21a.html).] [PDF](https://arxiv.org/abs/2102.03198) [video](https://slideslive.com/38959169/biasvariance-reduced-local-sgd-for-less-heterogeneous-federated-learning) |
| Federated Learning of User Verification Models Without Sharing Embeddings | Qualcomm                                                     | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/hosseini21a.html).] [PDF](https://arxiv.org/abs/2104.08776) [video](https://slideslive.com/38958858/federated-learning-of-user-verification-models-without-sharing-embeddings) |
| Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning | Accenture                                                    | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/fraboni21a.html).] [PDF](https://arxiv.org/abs/2105.05883) [[Code]](https://github.com/Accenture//Labs-Federated-Learning/tree/clustered_sampling) [video](https://slideslive.com/38959618/clustered-sampling-lowvariance-and-improved-representativity-for-clients-selection-in-federated-learning) |
| Ditto: Fair and Robust Federated Learning Through Personalization | CMU; Facebook AI                                             | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/li21h.html).] [PDF](https://arxiv.org/abs/2012.04221) [[Code]](https://github.com/litian96/ditto) [video](https://slideslive.com/38955195/ditto-fair-and-robust-federated-learning-through-personalization) |
| Heterogeneity for the Win: One-Shot Federated Clustering     | CMU                                                          | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/dennis21a.html).] [PDF](https://arxiv.org/abs/2103.00697) [video](https://slideslive.com/38959380/heterogeneity-for-the-win-oneshot-federated-clustering) |
| The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation :fire: | Google                                                       | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/kairouz21a.html).] [PDF](https://arxiv.org/abs/2102.06387) [CODE](https://github.com/google-research/federated/tree/master/distributed_dp) [video](https://slideslive.com/38959306/the-distributed-discrete-gaussian-mechanism-for-federated-learning-with-secure-aggregation) |
| Debiasing Model Updates for Improving Personalized Federated Training | BU; Arm                                                      | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/acar21a.html).] [Code](https://github.com/venkatesh-saligrama/Personalized-Federated-Learning) [video](https://slideslive.com/38959212/debiasing-model-updates-for-improving-personalized-federated-training) |
| One for One, or All for All: Equilibria and Optimality of Collaboration in Federated Learning | Toyota; Berkeley; Cornell University                         | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/blum21a.html).] [PDF](https://arxiv.org/abs/2103.03228) [[Code]](https://github.com/rlphilli/Collaborative-Incentives) [video](https://slideslive.com/38959135/one-for-one-or-all-for-all-equilibria-and-optimality-of-collaboration-in-federated-learning) |
| CRFL: Certifiably Robust Federated Learning against Backdoor Attacks | UIUC; IBM                                                    | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/xie21a.html).] [PDF](https://arxiv.org/abs/2106.08283) [[Code]](https://github.com/AI-secure/CRFL) [video](https://slideslive.com/38959047/crfl-certifiably-robust-federated-learning-against-backdoor-attacks) |
| Federated Learning under Arbitrary Communication Patterns    | Indiana University; Amazon                                   | ICML           | 2021 |                             | [[PUB](http://proceedings.mlr.press/v139/avdiukhin21a.html).] [video](https://slideslive.com/38959048/federated-learning-under-arbitrary-communication-patterns) |
| Sageflow: Robust Federated Learning against Both Stragglers and Adversaries | KAIST                                                        | NeurIPS        | 2021 | Sageflow[^Sageflow]         | [[PUB](https://proceedings.neurips.cc/paper/2021/hash/076a8133735eb5d7552dc195b125a454-Abstract.html).] |
| CAFE: Catastrophic Data Leakage in Vertical Federated Learning | Rensselaer Polytechnic Institute; IBM Research               | NeurIPS        | 2021 | CAFE[^CAFE]                 | [[PUB.]](https://papers.nips.cc/paper/2021/hash/08040837089cdf46631a10aca5258e16-Abstract.html) [[Code]](https://github.com/DeRafael/CAFE) |
| Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee | NUS                                                          | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/080acdcce72c06873a773c4311c2e464-Abstract.html) [PDF](https://arxiv.org/abs/2110.14074) [[Code]](https://github.com/flint-xf-fan/Byzantine-Federeated-RL) |
| Optimality and Stability in Federated Learning: A Game-theoretic Approach | Cornell University                                           | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/09a5e2a11bea20817477e0b1dfe2cc21-Abstract.html) [PDF](https://arxiv.org/abs/2106.09580) [[Code] ](https://github.com/kpdonahue/model_sharing_games) |
| QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning | UCLA                                                         | NeurIPS        | 2021 | QuPeD[^QuPeD]               | [[PUB.]](https://papers.nips.cc/paper/2021/hash/1dba3025b159cd9354da65e2d0436a31-Abstract.html) [PDF](https://arxiv.org/abs/2107.13892) [[Code](https://github.com/zkhku/fedsage)] [[解读](https://zhuanlan.zhihu.com/p/430789355)] |
| The Skellam Mechanism for Differentially Private Federated Learning :fire: | Google Research; CMU                                         | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.neurips.cc/paper/2021/hash/285baacbdf8fda1de94b19282acd23e2-Abstract.html) [PDF](https://arxiv.org/abs/2110.04995) [Code](https://github.com/google-research/federated/tree/master/distributed_dp) |
| No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data | NUS; Huawei                                                  | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.neurips.cc/paper/2021/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html) [PDF](https://arxiv.org/abs/2106.05001) |
| STEM: A Stochastic Two-Sided Momentum Algorithm Achieving Near-Optimal Sample and Communication Complexities for Federated Learning | UMN                                                          | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.neurips.cc/paper/2021/hash/3016a447172f3045b65f5fc83e04b554-Abstract.html) [PDF](https://arxiv.org/abs/2106.10435) |
| Subgraph Federated Learning with Missing Neighbor Generation | Emory;  UBC; Lehigh University                               | NeurIPS        | 2021 | FedSage[^FedSage]           | [[PUB.]](https://papers.neurips.cc/paper/2021/hash/34adeb8e3242824038aa65460a47c29e-Abstract.html) [PDF](https://arxiv.org/abs/2106.13430) [Code](https://github.com/zkhku/fedsage) [[解读](https://zhuanlan.zhihu.com/p/423555171)] |
| Evaluating Gradient Inversion Attacks and Defenses in Federated Learning :fire: | Princeton                                                    | NeurIPS        | 2021 | GradAttack[^GradAttack]     | [[PUB.]](https://papers.neurips.cc/paper/2021/hash/3b3fff6463464959dcd1b68d0320f781-Abstract.html) [PDF](https://arxiv.org/abs/2112.00059) [Code](https://github.com/Princeton-SysML/GradAttack) |
| Personalized Federated Learning With Gaussian Processes      | Bar-Ilan University                                          | NeurIPS        | 2021 |                             | [[PUB](https://proceedings.neurips.cc/paper/2021/hash/46d0671dd4117ea366031f87f3aa0093-Abstract.html).] [PDF](https://arxiv.org/abs/2106.15482) [[Code]](https://github.com/IdanAchituve/pFedGP) |
| Differentially Private Federated Bayesian Optimization with Distributed Exploration | MIT; NUS                                                     | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/4c27cea8526af8cfee3be5e183ac9605-Abstract.html) [PDF](https://arxiv.org/abs/2110.14153) [[Code]](https://github.com/daizhongxiang/Differentially-Private-Federated-Bayesian-Optimization) |
| Parameterized Knowledge Transfer for Personalized Federated Learning | PolyU                                                        | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/5383c7318a3158b9bc261d0b6996f7c2-Abstract.html) [PDF](https://arxiv.org/abs/2111.02862) |
| Federated Reconstruction: Partially Local Federated Learning :fire: | Google Research                                              | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/5d44a2b0d85aa1a4dd3f218be6422c66-Abstract.html) [PDF](https://arxiv.org/abs/2102.03448) [Code](https://github.com/google-research/federated/tree/master/reconstruction) [UC.](https://github.com/KarhouTam/FedRecon) |
| Fast Federated Learning in the Presence of Arbitrary Device Unavailability | THU; Princeton; MIT                                          | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/64be20f6dd1dd46adf110cf871e3ed35-Abstract.html) [PDF](https://arxiv.org/abs/2106.04159) [[Code]](https://github.com/hmgxr128/MIFA_code/) |
| FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective | Duke University; Accenture Labs                              | NeurIPS        | 2021 | FL-WBC[^FL-WBC]             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/692baebec3bb4b53d7ebc3b9fabac31b-Abstract.html) [PDF](https://arxiv.org/abs/2110.13864) [[Code]](https://github.com/jeremy313/FL-WBC) |
| FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout | KAUST; Samsung AI Center                                     | NeurIPS        | 2021 | FjORD[^FjORD]               | [[PUB.]](https://papers.nips.cc/paper/2021/hash/6aed000af86a084f9cb0264161e29dd3-Abstract.html) [PDF](https://arxiv.org/abs/2102.13451) |
| Linear Convergence in Federated Learning: Tackling Client Heterogeneity and Sparse Gradients | University of Pennsylvania                                   | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/7a6bda9ad6ffdac035c752743b7e9d0e-Abstract.html) [PDF](https://arxiv.org/abs/2102.07053) [Video](https://papertalk.org/papertalks/35898) |
| Federated Multi-Task Learning under a Mixture of Distributions | INRIA; Accenture Labs                                        | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/82599a4ec94aca066873c99b4c741ed8-Abstract.html) [PDF](https://arxiv.org/abs/2108.10252) [[Code]](https://github.com/omarfoq/FedEM) |
| Federated Graph Classification over Non-IID Graphs           | Emory                                                        | NeurIPS        | 2021 | GCFL[^GCFL]                 | [[PUB.]](https://papers.nips.cc/paper/2021/hash/9c6947bd95ae487c81d4e19d3ed8cd6f-Abstract.html) [PDF](https://arxiv.org/abs/2106.13423) [Code](https://github.com/Oxfordblue7/GCFL) [[解读](https://zhuanlan.zhihu.com/p/430623053)] [[解读](https://zhuanlan.zhihu.com/p/430718887)] |
| Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing | CMU; Hewlett Packard Enterprise                              | NeurIPS        | 2021 | FedEx[^FedEx]               | [[PUB.]](https://papers.nips.cc/paper/2021/hash/a0205b87490c847182672e8d371e9948-Abstract.html) [PDF](https://arxiv.org/abs/2106.04502) [[Code]](https://github.com/mkhodak/fedex) |
| On Large-Cohort Training for Federated Learning :fire:       | Google; CMU                                                  | NeurIPS        | 2021 | Large-Cohort[^Large-Cohort] | [[PUB.]](https://papers.nips.cc/paper/2021/hash/ab9ebd57177b5106ad7879f0896685d4-Abstract.html) [PDF](https://arxiv.org/abs/2106.07820) [[Code]](https://github.com/google-research/federated/tree/f4e26c1b9b47ac320e520a8b9943ea2c5324b8c2/large_cohort) |
| DeepReduce: A Sparse-tensor Communication Framework for Federated Deep Learning | KAUST; Columbia University; University of Central Florida    | NeurIPS        | 2021 | DeepReduce[^DeepReduce]     | [[PUB.]](https://papers.nips.cc/paper/2021/hash/b0ab42fcb7133122b38521d13da7120b-Abstract.html) [PDF](https://arxiv.org/abs/2102.03112) [[Code]](https://github.com/hangxu0304/DeepReduce) |
| PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization | Huawei                                                       | NeurIPS        | 2021 | PartialFed[^PartialFed]     | [[PUB.]](https://papers.nips.cc/paper/2021/hash/c429429bf1f2af051f2021dc92a8ebea-Abstract.html) [Video](https://papertalk.org/papertalks/37327) |
| Federated Split Task-Agnostic Vision Transformer for COVID-19 CXR Diagnosis | KAIST                                                        | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/ceb0595112db2513b9325a85761b7310-Abstract.html) [PDF](https://arxiv.org/abs/2111.01338) |
| Addressing Algorithmic Disparity and Performance Inconsistency in Federated Learning | THU; Alibaba; Weill Cornell Medicine                         | NeurIPS        | 2021 | FCFL[^FCFL ]                | [[PUB.]](https://papers.nips.cc/paper/2021/hash/db8e1af0cb3aca1ae2d0018624204529-Abstract.html) [PDF](https://arxiv.org/abs/2108.08435) [[Code]](https://github.com/cuis15/FCFL) |
| Federated Linear Contextual Bandits                          | The Pennsylvania State University; Facebook; University of Virginia | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/e347c51419ffb23ca3fd5050202f9c3d-Abstract.html) [PDF](https://arxiv.org/abs/2110.14177) [Code](https://github.com/Ruiquan5514/Federated-Linear-Contextual-Bandits) |
| Few-Round Learning for Federated Learning                    | KAIST                                                        | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/f065d878ccfb4cc4f4265a4ff8bafa9a-Abstract.html) |
| Breaking the centralized barrier for cross-device federated learning | EPFL; Google Research                                        | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/f0e6be4ce76ccfa73c5a540d992d0756-Abstract.html) [[Code]](https://fedjax.readthedocs.io/en/latest/fedjax.algorithms.html#module-fedjax.algorithms.mime) [Video](https://papertalk.org/papertalks/37564) |
| Federated-EM with heterogeneity mitigation and variance reduction | Ecole Polytechnique; Google Research                         | NeurIPS        | 2021 | Federated-EM[^Federated-EM] | [[PUB.]](https://papers.nips.cc/paper/2021/hash/f740c8d9c193f16d8a07d3a8a751d13f-Abstract.html) [PDF](https://arxiv.org/abs/2111.02083) |
| Delayed Gradient Averaging: Tolerate the Communication Latency for Federated Learning | MIT; Amazon; Google                                          | NeurIPS        | 2021 |                             | [[PUB](https://proceedings.neurips.cc/paper/2021/hash/fc03d48253286a798f5116ec00e99b2b-Abstract.html).] [Page](https://dga.hanlab.ai/) [Slides](https://dga.hanlab.ai/assets/dga_slides.pdf) |
| FedDR – Randomized Douglas-Rachford Splitting Algorithms for Nonconvex Federated Composite Optimization | University of North Carolina at Chapel Hill; IBM Research    | NeurIPS        | 2021 | FedDR[^FedDR]               | [[PUB.]](https://papers.nips.cc/paper/2021/hash/fe7ee8fc1959cc7214fa21c4840dff0a-Abstract.html) [PDF](https://arxiv.org/abs/2103.03452) [[Code]](https://github.com/unc-optimization/FedDR) |
| Gradient Inversion with Generative Image Prior               | Pohang University of Science and Technology; University of Wisconsin-Madison; University of Washington | NeurIPS        | 2021 |                             | [[PUB.]](https://papers.nips.cc/paper/2021/hash/fa84632d742f2729dc32ce8cb5d49733-Abstract.html) [PDF](https://arxiv.org/abs/2110.14962) [[Code]](https://github.com/ml-postech/gradient-inversion-generative-image-prior) |
| Federated Adversarial Domain Adaptation                      | BU; Columbia University; Rutgers University                  | ICLR           | 2020 |                             | [[PUB](https://openreview.net/forum?id=HJezF3VYPB).] [PDF](https://arxiv.org/abs/1911.02054) [Code](https://drive.google.com/file/d/1OekTpqB6qLfjlE2XUjQPm3F110KDMFc0/view?usp=sharing) |
| DBA: Distributed Backdoor Attacks against Federated Learning | ZJU; IBM Research                                            | ICLR           | 2020 |                             | [[PUB](https://openreview.net/forum?id=rkgyS0VFvr).] [[Code]](https://github.com/AI-secure/DBA) |
| Fair Resource Allocation in Federated Learning :fire:        | CMU; Facebook AI                                             | ICLR           | 2020 | fair-flearn[^fair-flearn]   | [[PUB](https://openreview.net/forum?id=ByexElSYDr).] [PDF](https://arxiv.org/abs/1905.10497) [[Code]](https://github.com/litian96/fair_flearn) |
| Federated Learning with Matched Averaging :fire:             | University of Wisconsin-Madison; IBM Research                | ICLR           | 2020 | FedMA[^FedMA]               | [[PUB](https://openreview.net/forum?id=BkluqlSFDS).] [PDF](https://arxiv.org/abs/2002.06440) [[Code]](https://github.com/IBM/FedMA) |
| Differentially Private Meta-Learning                         | CMU                                                          | ICLR           | 2020 |                             | [[PUB](https://openreview.net/forum?id=rJgqMRVYvr).] [PDF](https://proceedings.mlr.press/v162/zhang22p.html) |
| Generative Models for Effective ML on Private, Decentralized Datasets :fire: | Google                                                       | ICLR           | 2020 |                             | [[PUB](https://openreview.net/forum?id=SJgaRA4FPH).] [PDF](https://arxiv.org/abs/1911.06679) [[Code](https://github.com/google-research/federated/tree/master/gans)] |
| On the Convergence of FedAvg on Non-IID Data :fire:          | PKU                                                          | ICLR           | 2020 |                             | [[PUB](https://openreview.net/forum?id=HJxNAnVtDS).] [PDF](https://arxiv.org/abs/1907.02189#:~:text=%EE%80%80On%20the%20Convergence%20of%20FedAvg%20on%20Non-IID%20Data%EE%80%81.,of%20the%20total%20devices%20and%20averages%20the%20) [[Code]](https://github.com/lx10077/fedavgpy) [[解读](https://zhuanlan.zhihu.com/p/500005337)] |
| FedBoost: A Communication-Efficient Algorithm for Federated Learning | Google                                                       | ICML           | 2020 | FedBoost[^FedBoost]         | [[PUB](http://proceedings.mlr.press/v119/hamer20a.html).] [Video](https://slideslive.com/38928463/fedboost-a-communicationefficient-algorithm-for-federated-learning?ref=speaker-16993-latest) |
| FetchSGD: Communication-Efficient Federated Learning with Sketching | UC Berkeley; Johns Hopkins University; Amazon                | ICML           | 2020 | FetchSGD[^FetchSGD]         | [[PUB](http://proceedings.mlr.press/v119/rothchild20a.html).] [PDF](https://arxiv.org/abs/2007.07682) [Video](https://slideslive.com/38928454/fetchsgd-communicationefficient-federated-learning-with-sketching) [[Code]](https://github.com/kiddyboots216/CommEfficient) |
| SCAFFOLD: Stochastic Controlled Averaging for Federated Learning | EPFL; Google                                                 | ICML           | 2020 | SCAFFOLD[^SCAFFOLD]         | [[PUB](http://proceedings.mlr.press/v119/karimireddy20a.html).] [PDF](https://arxiv.org/abs/1910.06378) [Video](https://slideslive.com/38927610/scaffold-stochastic-controlled-averaging-for-federated-learning) [UC.](https://github.com/ki-ljl/Scaffold-Federated-Learning) [UC.](https://github.com/ramshi236/Accelerated-Federated-Learning-Over-MAC-in-Heterogeneous-Networks) [[解读](https://zhuanlan.zhihu.com/p/538941775)] |
| Federated Learning with Only Positive Labels                 | Google                                                       | ICML           | 2020 |                             | [[PUB](http://proceedings.mlr.press/v119/yu20f.html).] [PDF](https://arxiv.org/abs/2004.10342) [Video](https://slideslive.com/38928322/federated-learning-with-only-positive-labels) |
| From Local SGD to Local Fixed-Point Methods for Federated Learning | Moscow Institute of Physics and Technology; KAUST            | ICML           | 2020 |                             | [[PUB](http://proceedings.mlr.press/v119/malinovskiy20a.html).] [PDF](https://arxiv.org/abs/2004.01442) [Slides](https://icml.cc/media/Slides/icml/2020/virtual(no-parent)-15-18-00UTC-6590-from_local_sgd.pdf) [Video](https://slideslive.com/38928320/from-local-sgd-to-local-fixed-point-methods-for-federated-learning) |
| Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization | KAUST                                                        | ICML           | 2020 |                             | [[PUB](http://proceedings.mlr.press/v119/li20g.html).] [PDF](https://arxiv.org/abs/2002.11364) [Slide](https://icml.cc/media/Slides/icml/2020/virtual(no-parent)-15-19-00UTC-6191-acceleration_fo.pdf) [Video](https://slideslive.com/38927921/acceleration-for-compressed-gradient-descent-in-distributed-optimization) |
| Differentially-Private Federated Linear Bandits              | MIT                                                          | NeurIPS        | 2020 |                             | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/47a658229eb2368a99f1d032c8848542-Abstract.html).] [PDF](https://arxiv.org/abs/2010.11425) [[Code]](https://github.com/abhimanyudubey/private_federated_linear_bandits) |
| Federated Principal Component Analysis                       | University of Cambridge; Quine Technologies                  | NeurIPS        | 2020 |                             | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/47a658229eb2368a99f1d032c8848542-Abstract.html).] [PDF](https://arxiv.org/abs/1907.08059) [[Code]](https://github.com/andylamp/federated_pca) |
| FedSplit: an algorithmic framework for fast federated optimization | UC Berkeley                                                  | NeurIPS        | 2020 | FedSplit[^FedSplit]         | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/4ebd440d99504722d80de606ea8507da-Abstract.html).] [PDF](https://arxiv.org/abs/2005.05238) |
| Federated Bayesian Optimization via Thompson Sampling        | NUS; MIT                                                     | NeurIPS        | 2020 | fbo[^fbo]                   | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/6dfe08eda761bd321f8a9b239f6f4ec3-Abstract.html).] [PDF](https://arxiv.org/abs/2010.10154) [Code](https://github.com/daizhongxiang/Federated_Bayesian_Optimization) |
| Lower Bounds and Optimal Algorithms for Personalized Federated Learning | KAUST                                                        | NeurIPS        | 2020 |                             | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/187acf7982f3c169b3075132380986e4-Abstract.html).] [PDF](https://arxiv.org/abs/2010.02372) |
| Robust Federated Learning: The Case of Affine Distribution Shifts | UC Santa Barbara; MIT                                        | NeurIPS        | 2020 | RobustFL[^RobustFL]         | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/f5e536083a438cec5b64a4954abc17f1-Abstract.html).] [PDF](https://arxiv.org/abs/2006.08907) [Code](https://github.com/farzanfarnia/RobustFL) |
| An Efficient Framework for Clustered Federated Learning      | UC Berkeley; DeepMind                                        | NeurIPS        | 2020 | ifca[^ifca]                 | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/e32cc80bf07915058ce90722ee17bb71-Abstract.html).] [PDF](https://arxiv.org/abs/2006.04088) [[Code]](https://github.com/jichan3751/ifca) |
| Distributionally Robust Federated Averaging :fire:           | Pennsylvania State University                                | NeurIPS        | 2020 | DRFA[^DRFA]                 | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/ac450d10e166657ec8f93a1b65ca1b14-Abstract.html).] [PDF](https://arxiv.org/abs/2102.12660) [[Code]](https://github.com/MLOPTPSU/FedTorch) |
| Personalized Federated Learning with Moreau Envelopes :fire: | The University of Sydney                                     | NeurIPS        | 2020 |                             | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/f4f1f13c8289ac1b1ee0ff176b56fc60-Abstract.html).] [PDF](https://arxiv.org/abs/2006.08848) [[Code]](https://github.com/CharlieDinh/pFedMe) |
| Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach | MIT; UT Austin                                               | NeurIPS        | 2020 | Per-FedAvg[^Per-FedAvg]     | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html).] [PDF](https://arxiv.org/abs/2002.07948) [UC.](https://github.com/ki-ljl/Per-FedAvg) [UC.](https://github.com/KarhouTam/Per-FedAvg) |
| Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge | USC                                                          | NeurIPS        | 2020 | FedGKT[^FedGKT]             | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/a1d4c20b182ad7137ab3606f0e3fc8a4-Abstract.html).] [PDF](https://arxiv.org/abs/2007.14513) [[Code]](https://github.com/FedML-AI/FedML/tree/master/fedml_experiments/distributed/fedgkt) [[解读](https://zhuanlan.zhihu.com/p/536901871)] |
| Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization :fire: | CMU; Princeton                                               | NeurIPS        | 2020 | FedNova[^FedNova]           | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/564127c03caab942e503ee6f810f54fd-Abstract.html).] [PDF](https://arxiv.org/abs/2007.07481) [Code](https://github.com/JYWa/FedNova) [UC.](https://github.com/carbonati/fl-zoo) |
| Attack of the Tails: Yes, You Really Can Backdoor Federated Learning | University of Wisconsin-Madison                              | NeurIPS        | 2020 |                             | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html).] [PDF](https://arxiv.org/abs/2007.05084) |
| Federated Accelerated Stochastic Gradient Descent            | Stanford                                                     | NeurIPS        | 2020 | FedAc[^FedAc]               | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/39d0a8908fbe6c18039ea8227f827023-Abstract.html).] [PDF](https://arxiv.org/abs/2006.08950) [[Code]](https://github.com/hongliny/FedAc-NeurIPS20) [Video](https://youtu.be/K28zpAgg3HM) |
| Inverting Gradients - How easy is it to break privacy in federated learning? :fire: | University of Siegen                                         | NeurIPS        | 2020 |                             | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/c4ede56bbd98819ae6112b20ac6bf145-Abstract.html).] [PDF](https://arxiv.org/abs/2003.14053) [[Code]](https://github.com/JonasGeiping/invertinggradients) |
| Ensemble Distillation for Robust Model Fusion in Federated Learning | EPFL                                                         | NeurIPS        | 2020 | FedDF[^FedDF]               | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/18df51b97ccd68128e994804f3eccc87-Abstract.html).] [PDF](https://arxiv.org/abs/2006.07242) [Code](https://github.com/epfml/federated-learning-public-code/tree/master/codes/FedDF-code) |
| Throughput-Optimal Topology Design for Cross-Silo Federated Learning | INRIA                                                        | NeurIPS        | 2020 |                             | [[PUB](https://proceedings.neurips.cc/paper/2020/hash/e29b722e35040b88678e25a1ec032a21-Abstract.html).] [PDF](https://arxiv.org/abs/2010.12229) [[Code]](https://github.com/omarfoq/communication-in-cross-silo-fl) |
| Bayesian Nonparametric Federated Learning of Neural Networks :fire: | IBM                                                          | ICML           | 2019 |                             | [[PUB](http://proceedings.mlr.press/v97/yurochkin19a.html).] [PDF](https://arxiv.org/abs/1905.12022) [[Code]](https://github.com/IBM/probabilistic-federated-neural-matching) |
| Analyzing Federated Learning through an Adversarial Lens :fire: | Princeton; IBM                                               | ICML           | 2019 |                             | [[PUB](http://proceedings.mlr.press/v97/bhagoji19a.html).] [PDF](https://arxiv.org/abs/1811.12470) [[Code]](https://github.com/inspire-group/ModelPoisoning) |
| Agnostic Federated Learning                                  | Google                                                       | ICML           | 2019 |                             | [[PUB](http://proceedings.mlr.press/v97/mohri19a.html).] [PDF](https://arxiv.org/abs/1902.00146) |
| cpSGD: Communication-efficient and differentially-private distributed SGD | Princeton; Google                                            | NeurIPS        | 2018 |                             | [[PUB](https://papers.nips.cc/paper/2018/hash/21ce689121e39821d07d04faab328370-Abstract.html).] [PDF ](https://arxiv.org/abs/1805.10559) |
| Federated Multi-Task Learning :fire:                         | Stanford; USC; CMU                                           | NeurIPS        | 2017 |                             | [[PUB](https://papers.nips.cc/paper/2017/hash/6211080fa89981f66b1a0c9d55c61d0f-Abstract.html).] [PDF](https://arxiv.org/abs/1705.10467) [[Code]](https://github.com/gingsmith/fmtl) |



## FL in top DM conference and journal

In this section, we will summarize Federated Learning papers accepted by top DM(Data Mining) conference and journal, Including [KDD](https://dblp.uni-trier.de/db/conf/kdd/index.html)(ACM SIGKDD Conference on Knowledge Discovery and Data Mining) and [WSDM](https://dblp.uni-trier.de/db/conf/wsdm/index.html)(Web Search and Data Mining).

- [KDD](https://dblp.uni-trier.de/search?q=federate%20venue%3AKDD%3A) 2022([Research Track](https://kdd.org/kdd2022/paperRT.html), [Applied Data Science track](https://kdd.org/kdd2022/paperADS.html)) , [2021](https://kdd.org/kdd2021/accepted-papers/index),[2020](https://www.kdd.org/kdd2020/accepted-papers)
- [WSDM](https://dblp.uni-trier.de/search?q=federate%20venue%3AWSDM%3A) 2022, 2021, 2019



| Title                                                        | Affiliation                                                | Venue                  | Year | TL;DR                                                 | Materials                                                    |
| ------------------------------------------------------------ | ---------------------------------------------------------- | ---------------------- | ---- | ----------------------------------------------------- | ------------------------------------------------------------ |
| Collaboration Equilibrium in Federated Learning              | THU                                                        | KDD                    | 2022 | CE[^CE]                                               | [[PDF](https://arxiv.org/abs/2108.07926)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539237).] [Code](https://github.com/cuis15/learning-to-collaborate) |
| Connected Low-Loss Subspace Learning for a Personalization in Federated Learning | Ulsan National Institute of Science and Technology         | KDD                    | 2022 | SuPerFed [^SuPerFed]                                  | [[PDF](https://arxiv.org/abs/2109.07628)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539254).] [Code](https://github.com/vaseline555/superfed) |
| FedMSplit: Correlation-Adaptive Federated Multi-Task Learning across Multimodal Split Networks | University of Virginia                                     | KDD                    | 2022 | FedMSplit[^FedMSplit]                                 | [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539384).]     |
| Communication-Efficient Robust Federated Learning with Noisy Labels | University of Pittsburgh                                   | KDD                    | 2022 | Comm-FedBiO[^Comm-FedBiO]                             | [[PDF](https://arxiv.org/abs/2206.05558)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539328).] |
| FLDetector: Detecting Malicious Clients in Federated Learning via Checking Model-Updates Consistency | USTC                                                       | KDD                    | 2022 | FLDetector[^FLDetector]                               | [[PDF](https://arxiv.org/abs/2207.09209)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539231).] [Code](https://github.com/zaixizhang/FLDetector) |
| Practical Lossless Federated Singular Vector Decomposition Over Billion-Scale Data | HKUST                                                      | KDD                    | 2022 | FedSVD [^FedSVD]                                      | [[PDF](https://arxiv.org/abs/2105.08925)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539402).] [Code](https://github.com/Di-Chai/FedEval) |
| FedWalk: Communication Efficient Federated Unsupervised Node Embedding with Differential Privacy | SJTU                                                       | KDD                    | 2022 | FedWalk[^FedWalk]                                     | [[PDF](https://arxiv.org/abs/2205.15896)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539308).] |
| FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Platform for Federated Graph Learning :fire: | Alibaba                                                    | KDD (Best Paper Award) | 2022 | FederatedScope-GNN [^FederatedScope-GNN]              | [[PDF](https://arxiv.org/abs/2204.05562)] [Code](https://github.com/alibaba/FederatedScope) [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539112).] |
| Fed-LTD: Towards Cross-Platform Ride Hailing via Federated Learning to Dispatch | BUAA                                                       | KDD                    | 2022 | Fed-LTD [^Fed-LTD]                                    | [[PDF](https://hufudb.com/static/paper/2022/SIGKDD2022_Fed-LTD%20Towards%20Cross-Platform%20Ride%20Hailing%20via.pdf)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539047).] <br />[[解读](https://zhuanlan.zhihu.com/p/544183874)] |
| Felicitas: Federated Learning in Distributed Cross Device Collaborative Frameworks | USTC                                                       | KDD                    | 2022 | Felicitas [^Felicitas]                                | [[PDF](https://arxiv.org/abs/2202.08036)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539039).] |
| No One Left Behind: Inclusive Federated Learning over Heterogeneous Devices | Renmin University of China                                 | KDD                    | 2022 | InclusiveFL [^InclusiveFL]                            | [[PDF](https://arxiv.org/abs/2202.08036)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539086).] |
| FedAttack: Effective and Covert Poisoning Attack on Federated Recommendation via Hard Sampling | THU                                                        | KDD                    | 2022 | FedAttack [^FedAttack]                                | [[PDF](https://arxiv.org/abs/2202.04975)] [[PUB](https://dl.acm.org/doi/10.1145/3534678.3539119).] [code](https://github.com/wuch15/FedAttack) |
| PipAttack: Poisoning Federated Recommender Systems for Manipulating Item Promotion | The University of Queensland                               | WSDM                   | 2022 | PipAttack[^PipAttack]                                 | [[PDF](https://arxiv.org/abs/2110.10926)] [[PUB](https://dl.acm.org/doi/10.1145/3488560.3498386).] |
| Fed2: Feature-Aligned Federated Learning                     | George Mason University; Microsoft; University of Maryland | KDD                    | 2021 | Fed2[^Fed2]                                           | [PDF](https://arxiv.org/abs/2111.14248) [[PUB](https://dl.acm.org/doi/10.1145/3447548.3467309).] |
| FedRS: Federated Learning with Restricted Softmax for Label Distribution Non-IID Data | Nanjing University                                         | KDD                    | 2021 | FedRS[^FedRS]                                         | [Code](https://github.com/lxcnju/FedRepo) [[PUB](https://dl.acm.org/doi/10.1145/3447548.3467254).] |
| Federated Adversarial Debiasing for Fair and Trasnferable Representations | Michigan State University                                  | KDD                    | 2021 | FADE[^FADE]                                           | [Page](https://jyhong.gitlab.io/publication/fade2021kdd/) [Code](https://github.com/illidanlab/FADE) [Slides](https://jyhong.gitlab.io/publication/fade2021kdd/slides.pdf) [[PUB](https://dl.acm.org/doi/10.1145/3447548.3467281).] |
| Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling | USC                                                        | KDD                    | 2021 | CNFGNN[^CNFGNN]                                       | [[PUB](https://dl.acm.org/doi/pdf/10.1145/3447548.3467371).] [[Code]](https://github.com/mengcz13/KDD2021_CNFGNN) [[解读](https://zhuanlan.zhihu.com/p/434839878)] |
| AsySQN: Faster Vertical Federated Learning Algorithms with Better Computation Resource Utilization | Xidian University;JD Tech                                  | KDD                    | 2021 | AsySQN[^AsySQN]                                       | [PDF](https://arxiv.org/abs/2109.12519) [[PUB](https://dl.acm.org/doi/10.1145/3447548.3467169).] |
| FLOP: Federated Learning on Medical Datasets using Partial Networks | Duke University                                            | KDD                    | 2021 | FLOP[^FLOP]                                           | [PDF](https://arxiv.org/abs/2102.05218.pdf) [[PUB](https://dl.acm.org/doi/10.1145/3447548.3467185).] [[Code]](https://github.com/jianyizhang123/FLOP) |
| A Practical Federated Learning Framework for Small Number of Stakeholders | ETH Zürich                                                 | WSDM                   | 2021 | Federated-Learning-source[^Federated-Learning-source] | [[PUB](https://dl.acm.org/doi/10.1145/3437963.3441702).] [Code](https://github.com/MTC-ETH/Federated-Learning-source) |
| Federated Deep Knowledge Tracing                             | USTC                                                       | WSDM                   | 2021 | FDKT[^FDKT]                                           | [[PUB](https://dl.acm.org/doi/10.1145/3437963.3441747).] [[Code](https://github.com/hxwujinze/federated-deep-knowledge-tracing)] |
| FedFast: Going Beyond Average for Faster Training of Federated Recommender Systems | University College Dublin                                  | KDD                    | 2020 | FedFast[^FedFast]                                     | [[PUB](https://dl.acm.org/doi/10.1145/3394486.3403176).] [video](https://papertalk.org/papertalks/23422) |
| Federated Doubly Stochastic Kernel Learning for Vertically Partitioned Data | JD Tech                                                    | KDD                    | 2020 | FDSKL[^FDSKL]                                         | [[PUB](https://dl.acm.org/doi/10.1145/3394486.3403298).] [PDF](https://arxiv.org/abs/2008.06197) [video](https://papertalk.org/papertalks/23301) |
| Federated Online Learning to Rank with Evolution Strategies  | Facebook AI Research                                       | WSDM                   | 2019 | FOLtR-ES[^FOLtR-ES]                                   | [[PUB](https://dl.acm.org/doi/10.1145/3289600.3290968).] [Code](http://github.com/facebookresearch/foltr-es) |



## FL in top Secure conference and journal

In this section, we will summarize Federated Learning papers accepted by top Secure conference and journal, Including [S&P](https://dblp.uni-trier.de/db/conf/sp/index.html)(IEEE Symposium on Security and Privacy), [CCS](https://dblp.uni-trier.de/db/conf/ccs/index.html)(Conference on Computer and Communications Security), [USENIX Security](https://dblp.uni-trier.de/db/conf/uss/index.html)(Usenix Security Symposium) and [NDSS](https://dblp.uni-trier.de/db/conf/ndss/index.html)(Network and Distributed System Security Symposium).

- [S&P](https://dblp.uni-trier.de/search?q=federate%20venue%3AIEEE%20Symposium%20on%20Security%20and%20Privacy%3A) [2022](https://www.ieee-security.org/TC/SP2022/program-papers.html), [2019](https://www.ieee-security.org/TC/SP2019/program-papers.html)
- [CCS](https://dblp.uni-trier.de/search?q=federate%20venue%3ACCS%3A) [2021](https://sigsac.org/ccs/CCS2021/accepted-papers.html), [2019](https://www.sigsac.org/ccs/CCS2019/index.php/program/accepted-papers/), 2017
- [USENIX Security](https://dblp.uni-trier.de/search?q=federate%20venue%3AUSENIX%20Security%20Symposium%3A) [2022](https://www.usenix.org/conference/usenixsecurity22/technical-sessions), [2020](https://www.usenix.org/conference/usenixsecurity20/technical-sessions)
- [NDSS](https://dblp.uni-trier.de/search?q=federate%20venue%3ANDSS%3A) [2022](https://www.ndss-symposium.org/ndss2022/accepted-papers/), [2021](https://www.ndss-symposium.org/ndss2021/accepted-papers/)


| Title                                                        | Affiliation                                                  | Venue | Year | TL;DR                                               | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ----- | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy | Fudan University| S&P | 2023 | PEA[^PEA] | [[PDF](https://arxiv.org/abs/2208.08662)] |
| Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning | University of Massachusetts | S&P | 2022 |  | [[PUB](https://ieeexplore.ieee.org/document/9833647/).] [Video](https://www.youtube.com/watch?v=tQv3CpxIyvs) |
| SIMC: ML Inference Secure Against Malicious Clients at Semi-Honest Cost | Microsoft Research | USENIX Security | 2022 | SIMC[^SIMC] | [[PUB](https://www.usenix.org/conference/usenixsecurity22/presentation/chandran).] [PDF](https://eprint.iacr.org/2021/1538) [code](https://github.com/shahakash28/simc) |
| Efficient Differentially Private Secure Aggregation for Federated Learning via Hardness of Learning with Errors | University of Vermont | USENIX Security | 2022 |  | [[PUB](https://www.usenix.org/conference/usenixsecurity22/presentation/stevens).] [Slides](https://www.usenix.org/system/files/sec22_slides-stevens.pdf) |
| Label Inference Attacks Against Vertical Federated Learning | ZJU | USENIX Security | 2022 |  | [[PUB](https://www.usenix.org/conference/usenixsecurity22/presentation/fu-chong).] [Slides](https://www.usenix.org/system/files/sec22_slides-fu-chong.pdf) [code](https://github.com/FuChong-cyber/label-inference-attacks) |
| FLAME: Taming Backdoors in Federated Learning | Technical University of Darmstadt | USENIX Security | 2022 | FLAME[^FLAME] | [[PUB](https://www.usenix.org/conference/usenixsecurity22/presentation/nguyen).] [Slides](https://www.usenix.org/system/files/sec22_slides-nguyen.pdf) [PDF](https://arxiv.org/abs/2101.02281) |
|Local and Central Differential Privacy for Robustness and Privacy in Federated Learning| University at Buffalo, SUNY| NDSS | 2022 |  | [[PUB](https://www.ndss-symposium.org/ndss-paper/auto-draft-204/).] [PDF](https://arxiv.org/abs/2009.03561) [UC.](https://github.com/wenzhu23333/Differential-Privacy-Based-Federated-Learning) |
|Interpretable Federated Transformer Log Learning for Cloud Threat Forensics|University of the Incarnate Word | NDSS | 2022 |  | [[PUB](https://www.ndss-symposium.org/ndss-paper/auto-draft-236/).] [UC.](https://github.com/cyberthreat-datasets/ctdd-2021-os-syslogs) |
|FedCRI: Federated Mobile Cyber-Risk Intelligence| Technical University of Darmstadt| NDSS | 2022 | FedCRI[^FedCRI] | [[PUB](https://www.ndss-symposium.org/ndss-paper/auto-draft-229/).] |
|DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection| Technical University of Darmstadt| NDSS | 2022 |DeepSight[^DeepSight]|[[PUB](https://www.ndss-symposium.org/ndss-paper/auto-draft-205/).] [PDF](https://arxiv.org/abs/2201.00763)|
| Private Hierarchical Clustering in Federated Networks | NUS | CCS             | 2021 |                                                              | [[PUB](https://dl.acm.org/doi/10.1145/3460120.3484822).] [PDF](https://arxiv.org/abs/2105.09057) |
| FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping | Duke University | NDSS            | 2021 |  | [[PUB](https://www.ndss-symposium.org/ndss-paper/fltrust-byzantine-robust-federated-learning-via-trust-bootstrapping/).] [PDF](https://arxiv.org/abs/2012.13995) [code](https://people.duke.edu/~zg70/code/fltrust.zip) [Video](https://www.youtube.com/watch?v=zhhdPgKPCN0&list=PLfUWWM-POgQvaqlGPwlOa0JR3bryB1KCS&index=2) [Slides](https://people.duke.edu/~zg70/code/Secure_Federated_Learning.pdf) |
| POSEIDON: Privacy-Preserving Federated Neural Network Learning | EPFL | NDSS            | 2021 |  | [[PUB](https://www.ndss-symposium.org/ndss-paper/poseidon-privacy-preserving-federated-neural-network-learning/).] [Video](https://www.youtube.com/watch?v=kX6-PMzxZ3c&list=PLfUWWM-POgQvaqlGPwlOa0JR3bryB1KCS&index=1) |
| Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning | University of Massachusetts Amherst | NDSS            | 2021 |  | [[PUB](https://www.ndss-symposium.org/ndss-paper/manipulating-the-byzantine-optimizing-model-poisoning-attacks-and-defenses-for-federated-learning/).] [code](https://github.com/vrt1shjwlkr/NDSS21-Model-Poisoning) [Video](https://www.youtube.com/watch?v=G2VYRnLqAXE&list=PLfUWWM-POgQvaqlGPwlOa0JR3bryB1KCS&index=3) |
| Local Model Poisoning Attacks to Byzantine-Robust Federated Learning | The Ohio State University | USENIX Security | 2020 |  | [[PUB](https://www.usenix.org/conference/usenixsecurity20/presentation/fang).] [PDF](https://arxiv.org/abs/1911.11815) [code](https://people.duke.edu/~zg70/code/fltrust.zip) [Video](https://www.youtube.com/watch?v=SQ12UpYrUVU&feature=emb_imp_woyt) [Slides](https://www.usenix.org/system/files/sec20_slides_fang.pdf) |
| A Reliable and Accountable Privacy-Preserving Federated Learning Framework using the Blockchain | University of Kansas | CCS (Poster)   | 2019 |                                                              | [[PUB](https://dl.acm.org/doi/10.1145/3319535.3363256).] |
| IOTFLA : A Secured and Privacy-Preserving Smart Home Architecture Implementing Federated Learning | Université du Québéc á Montréal | S&P （Workshop）  | 2019 |                                                              | [[PUB](https://ieeexplore.ieee.org/document/8844592).] |
| Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning :fire: | University of Massachusetts Amherst | S&P             | 2019 |  | [[PUB](https://ieeexplore.ieee.org/document/8835245).] [[PUB](https://www.computer.org/csdl/proceedings-article/sp/2019/666000a739/1dlwhtj4r7O).] [Video](https://www.youtube.com/watch?v=qwrLuqnuiqI) [Video](https://youtu.be/lzJY4BjCxTc) [Slides](https://www.ieee-security.org/TC/SP2019/SP19-Slides-pdfs/Milad_Nasr_-_08-Milad_Nasr-Comprehensive_Privacy_Analysis_of_Deep_Learning_(1).pdf) [code](https://github.com/privacytrustlab/ml_privacy_meter) |
| Practical Secure Aggregation for Privacy Preserving Machine Learning | Google | CCS | 2017 | | [[PUB](https://dl.acm.org/doi/10.1145/3133956.3133982).] [PDF](https://eprint.iacr.org/2017/281) [[解读](https://zhuanlan.zhihu.com/p/445656765)] [UC.](https://github.com/ammartahir24/SecureAggregation) [UC.](https://github.com/Chen-Junbao/SecureAggregation) [UC](https://github.com/corentingiraud/federated-learning-secure-aggregation). |

 


## FL in top CV conference and journal

In this section, we will summarize Federated Learning papers accepted by top CV(computer vision) conference and journal, Including [CVPR](https://dblp.uni-trier.de/db/conf/cvpr/index.html)(Computer Vision and Pattern Recognition), [ICCV](https://dblp.uni-trier.de/db/conf/iccv/index.html)(IEEE International Conference on Computer Vision), [ECCV](https://dblp.uni-trier.de/db/conf/eccv/index.html)(European Conference on Computer Vision), [MM](https://dblp.org/db/conf/mm/index.html)(ACM International Conference on Multimedia).

- [CVPR](https://dblp.uni-trier.de/search?q=federate%20venue%3ACVPR%3A) [2022](https://openaccess.thecvf.com/CVPR2022), [2021](https://openaccess.thecvf.com/CVPR2021?day=all)
- [ICCV](https://dblp.uni-trier.de/search?q=federate%20venue%3AICCV%3A) [2021](https://openaccess.thecvf.com/ICCV2021?day=all)
- [ECCV](https://dblp.uni-trier.de/search?q=federate%20venue%3AECCV%3A) 2022, [2020](https://www.ecva.net/papers.php)
- [MM](https://dblp.uni-trier.de/search?q=federate%20venue%3AACM%20Multimedia%3A) [2021](https://2021.acmmm.org/main-track-list), [2020](https://2020.acmmm.org/main-track-list.html)

| Title                                                        | Affiliation                                                  | Venue | Year | TL;DR                                       | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ----- | ---- | ------------------------------------------- | ------------------------------------------------------------ |
| FedX: Unsupervised Federated Learning with Cross Knowledge Distillation | KAIST                                                        | ECCV  | 2022 | FedX[^FedX]                                 | [PUB.] [[PDF](https://arxiv.org/abs/2207.09158)] [Code](https://github.com/sungwon-han/fedx) |
| Personalizing Federated Medical Image Segmentation via Local Calibration | Xiamen University                                            | ECCV  | 2022 | LC-Fed [^LC-Fed]                            | [PUB.] [[PDF](https://arxiv.org/abs/2207.04655)] [Code](https://github.com/jcwang123/fedlc) |
| Improving Generalization in Federated Learning by Seeking Flat Minima | Politecnico di Torino                                        | ECCV  | 2022 | FedSAM [^FedSAM]                            | [PUB.] [[PDF](https://arxiv.org/abs/2203.11834)] [Code](https://github.com/debcaldarola/fedsam) |
| ATPFL: Automatic Trajectory Prediction Model Design Under Federated Learning Framework | HIT                                                          | CVPR  | 2022 | ATPFL [^ATPFL]                              | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_ATPFL_Automatic_Trajectory_Prediction_Model_Design_Under_Federated_Learning_Framework_CVPR_2022_paper.html).] |
| Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning | Stanford                                                     | CVPR  | 2022 | ViT-FL [^ViT-FL]                            | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Qu_Rethinking_Architecture_Design_for_Tackling_Data_Heterogeneity_in_Federated_Learning_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Qu_Rethinking_Architecture_Design_CVPR_2022_supplemental.pdf)] [[PDF](http://arxiv.org/abs/2106.06047)] [[Code]](https://github.com/Liangqiong/ViT-FL-main) [video](https://www.youtube.com/watch?v=Ae1CDi0_Nok&ab_channel=StanfordMedAI) |
| FedCorr: Multi-Stage Federated Learning for Label Noise Correction | Singapore University of Technology and Design                | CVPR  | 2022 | FedCorr[^FedCorr]                           | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Xu_FedCorr_Multi-Stage_Federated_Learning_for_Label_Noise_Correction_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Xu_FedCorr_Multi-Stage_Federated_CVPR_2022_supplemental.pdf)] [[PDF](http://arxiv.org/abs/2204.04677)] [[Code]](https://github.com/xu-jingyi/fedcorr) [video](https://www.youtube.com/watch?v=GA22ct1LgRA&ab_channel=ZihanChen) |
| FedCor: Correlation-Based Active Client Selection Strategy for Heterogeneous Federated Learning | Duke University                                              | CVPR  | 2022 | FedCor [^FedCor]                            | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Tang_FedCor_Correlation-Based_Active_Client_Selection_Strategy_for_Heterogeneous_Federated_Learning_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Tang_FedCor_Correlation-Based_Active_CVPR_2022_supplemental.zip)] [[PDF](http://arxiv.org/abs/2103.13822)] |
| Layer-Wised Model Aggregation for Personalized Federated Learning | PolyU                                                        | CVPR  | 2022 | pFedLA [^pFedLA]                            | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Ma_Layer-Wised_Model_Aggregation_for_Personalized_Federated_Learning_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Ma_Layer-Wised_Model_Aggregation_CVPR_2022_supplemental.pdf)] [[PDF](http://arxiv.org/abs/2205.03993)] |
| Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning | University of Central Florida                                | CVPR  | 2022 | FedAlign[^FedAlign]                         | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Mendieta_Local_Learning_Matters_Rethinking_Data_Heterogeneity_in_Federated_Learning_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Mendieta_Local_Learning_Matters_CVPR_2022_supplemental.pdf)] [[PDF](http://arxiv.org/abs/2111.14213)] [[Code]](https://github.com/mmendiet/FedAlign) |
| Federated Learning With Position-Aware Neurons               | Nanjing University                                           | CVPR  | 2022 | PANs [^PANs]                                | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Federated_Learning_With_Position-Aware_Neurons_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Li_Federated_Learning_With_CVPR_2022_supplemental.pdf)] [[PDF](http://arxiv.org/abs/2203.14666)] |
| RSCFed: Random Sampling Consensus Federated Semi-Supervised Learning | HKUST                                                        | CVPR  | 2022 | RSCFed [^RSCFed]                            | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Liang_RSCFed_Random_Sampling_Consensus_Federated_Semi-Supervised_Learning_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Liang_RSCFed_Random_Sampling_CVPR_2022_supplemental.pdf)] [[PDF](http://arxiv.org/abs/2203.13993)] [[Code]](https://github.com/xmed-lab/rscfed) |
| Learn From Others and Be Yourself in Heterogeneous Federated Learning | Wuhan University                                             | CVPR  | 2022 | FCCL[^FCCL]                                 | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Huang_Learn_From_Others_and_Be_Yourself_in_Heterogeneous_Federated_Learning_CVPR_2022_paper.html).] [[code](https://github.com/wenkehuang/fccl)] [video](https://www.youtube.com/watch?v=zZoASA71qwQ&ab_channel=HuangWenke) |
| Robust Federated Learning With Noisy and Heterogeneous Clients | Wuhan University                                             | CVPR  | 2022 | RHFL [^RHFL]                                | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Fang_Robust_Federated_Learning_With_Noisy_and_Heterogeneous_Clients_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Fang_Robust_Federated_Learning_CVPR_2022_supplemental.pdf)] [[Code]](https://github.com/FangXiuwen/Robust_FL) |
| ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning | Arizona State University                                     | CVPR  | 2022 | ResSFL [^ResSFL]                            | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Li_ResSFL_A_Resistance_Transfer_Framework_for_Defending_Model_Inversion_Attack_CVPR_2022_paper.html).] [[supp](https://openaccess.thecvf.com/content/CVPR2022/supplemental/Li_ResSFL_A_Resistance_CVPR_2022_supplemental.pdf)] [[PDF](http://arxiv.org/abs/2205.04007)] [[Code]](https://github.com/zlijingtao/ResSFL) |
| FedDC: Federated Learning With Non-IID Data via Local Drift Decoupling and Correction | National University of Defense Technology                    | CVPR  | 2022 | FedDC [^FedDC]                              | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Gao_FedDC_Federated_Learning_With_Non-IID_Data_via_Local_Drift_Decoupling_CVPR_2022_paper.html).] [[PDF](http://arxiv.org/abs/2203.11751)] [[Code](https://github.com/gaoliang13/FedDC)] [[解读](https://zhuanlan.zhihu.com/p/505889549)] |
| Federated Class-Incremental Learning                         | CAS; Northwestern University; UTS                            | CVPR  | 2022 | GLFC [^GLFC]                                | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Dong_Federated_Class-Incremental_Learning_CVPR_2022_paper.html).] [[PDF](http://arxiv.org/abs/2203.11473)] [[Code]](https://github.com/conditionWang/FCIL) |
| Fine-Tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning | PKU;  JD Explore Academy;  The University of Sydney          | CVPR  | 2022 | FedFTG [^FedFTG]                            | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Fine-Tuning_Global_Model_via_Data-Free_Knowledge_Distillation_for_Non-IID_Federated_CVPR_2022_paper.html).] [[PDF](http://arxiv.org/abs/2203.09249)] |
| Differentially Private Federated Learning With Local Regularization and Sparsification | CAS                                                          | CVPR  | 2022 | DP-FedAvg +BLUR + LUS [^DP-FedAvg+BLUR+LUS] | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Differentially_Private_Federated_Learning_With_Local_Regularization_and_Sparsification_CVPR_2022_paper.html).] [[PDF](http://arxiv.org/abs/2203.03106)] |
| Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage | University of Tennessee; Oak Ridge National Laboratory; Google Research | CVPR  | 2022 | GGL [^GGL]                                  | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Auditing_Privacy_Defenses_in_Federated_Learning_via_Generative_Gradient_Leakage_CVPR_2022_paper.html).] [[PDF](http://arxiv.org/abs/2203.15696)] [[Code]](https://github.com/zhuohangli/GGL) [video](https://www.youtube.com/watch?v=rphFSGDlGPY&ab_channel=MoSISLab) |
| CD2-pFed: Cyclic Distillation-Guided Channel Decoupling for Model Personalization in Federated Learning | SJTU                                                         | CVPR  | 2022 | CD2-pFed [^CD2-pFed]                        | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Shen_CD2-pFed_Cyclic_Distillation-Guided_Channel_Decoupling_for_Model_Personalization_in_Federated_CVPR_2022_paper.html).] [[PDF](https://arxiv.org/abs/2204.03880)] |
| Closing the Generalization Gap of Cross-Silo Federated Medical Image Segmentation | Univ. of Pittsburgh; NVIDIA                                  | CVPR  | 2022 | FedSM [^FedSM]                              | [[PUB](https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Closing_the_Generalization_Gap_of_Cross-Silo_Federated_Medical_Image_Segmentation_CVPR_2022_paper.html).] [[PDF](http://arxiv.org/abs/2203.10144)] |
| Multi-Institutional Collaborations for Improving Deep Learning-Based Magnetic Resonance Image Reconstruction Using Federated Learning | Johns Hopkins University                                     | CVPR  | 2021 | FL-MRCM [^FL-MRCM]                          | [[PUB](https://openaccess.thecvf.com/content/CVPR2021/html/Guo_Multi-Institutional_Collaborations_for_Improving_Deep_Learning-Based_Magnetic_Resonance_Image_Reconstruction_CVPR_2021_paper.html).] [[PUB](https://ieeexplore.ieee.org/document/9578476).] [PDF](https://arxiv.org/abs/2103.02148) [[Code]](https://github.com/guopengf/FL-MRCM) |
| Model-Contrastive Federated Learning :fire:                  | NUS; UC Berkeley                                             | CVPR  | 2021 | MOON [^MOON]                                | [[PUB](https://openaccess.thecvf.com/content/CVPR2021/html/Li_Model-Contrastive_Federated_Learning_CVPR_2021_paper.html).] [[PUB](https://ieeexplore.ieee.org/document/9578660).] [PDF](https://arxiv.org/abs/2103.16257) [[Code]](https://github.com/QinbinLi/MOON) [[解读](https://weisenhui.top/posts/17666.html)] |
| FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space :fire: | CUHK                                                         | CVPR  | 2021 | FedDG-ELCFS [^FedDG-ELCFS]                  | [[PUB](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_FedDG_Federated_Domain_Generalization_on_Medical_Image_Segmentation_via_Episodic_CVPR_2021_paper.html).] [[PUB](https://ieeexplore.ieee.org/document/9577482).] [PDF](https://arxiv.org/abs/2103.06030) [[Code]](https://github.com/liuquande/FedDG-ELCFS) |
| Soteria: Provable Defense Against Privacy Leakage in Federated Learning From Representation Perspective | Duke University                                              | CVPR  | 2021 | Soteria [^Soteria]                          | [[PUB](https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Soteria_Provable_Defense_Against_Privacy_Leakage_in_Federated_Learning_From_CVPR_2021_paper.html).] [[PUB](https://ieeexplore.ieee.org/document/9578192).] [PDF](https://arxiv.org/abs/2012.06043) [[Code]](https://github.com/jeremy313/Soteria) |
| Federated Learning for Non-IID Data via Unified Feature Learning and Optimization Objective Alignment | PKU                                                          | ICCV  | 2021 | FedUFO [^FedUFO]                            | [[PUB](https://ieeexplore.ieee.org/document/9710573).]       |
| Ensemble Attention Distillation for Privacy-Preserving Federated Learning | University at Buffalo                                        | ICCV  | 2021 | FedAD [^FedAD]                              | [[PUB](https://ieeexplore.ieee.org/document/9710586).] [[PDF](https://openaccess.thecvf.com/content/ICCV2021/papers/Gong_Ensemble_Attention_Distillation_for_Privacy-Preserving_Federated_Learning_ICCV_2021_paper.pdf)] |
| Collaborative Unsupervised Visual Representation Learning from Decentralized Data | NTU; SenseTime                                               | ICCV  | 2021 | FedU [^FedU]                                | [[PUB](https://ieeexplore.ieee.org/document/9710366).] [PDF](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhuang_Collaborative_Unsupervised_Visual_Representation_Learning_From_Decentralized_Data_ICCV_2021_paper.pdf) [PDF](https://arxiv.org/abs/2108.06492) |
| Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification | NTU                                                          | MM    | 2021 | FedUReID [^FedUReID]                        | [[PUB](https://dl.acm.org/doi/10.1145/3474085.3475182).] [[PDF](https://arxiv.org/abs/2108.06493)] |
| Federated Visual Classification with Real-World Data Distribution | MIT; Google                                                  | ECCV  | 2020 | FedVC+FedIR [^FedVC+FedIR]                  | [[PUB](https://link.springer.com/chapter/10.1007/978-3-030-58607-2_5).] [PDF](https://arxiv.org/abs/2003.08082) [Video](https://www.youtube.com/watch?v=Rc67rZzPDDY&ab_channel=TzuMingHsu) |
| InvisibleFL: Federated Learning over Non-Informative Intermediate Updates against Multimedia Privacy Leakages |                                                              | MM    | 2020 | InvisibleFL [^InvisibleFL]                  | [[PUB](https://dl.acm.org/doi/10.1145/3394171.3413923).]     |
| Performance Optimization of Federated Person Re-identification via Benchmark Analysis **`data.`** | NTU                                                          | MM    | 2020 | FedReID [^FedReID]                          | [[PUB](https://dl.acm.org/doi/10.1145/3394171.3413814).] [[PDF](https://arxiv.org/abs/2008.11560)] [Code ](https://github.com/cap-ntu/FedReID) [[解读](https://zhuanlan.zhihu.com/p/265987079)] |



## FL in top NLP conference and journal

In this section, we will summarize Federated Learning papers accepted by top AI and NLP conference and journal, including [ACL](https://dblp.uni-trier.de/db/conf/acl/index.html)(Annual Meeting of the Association for Computational Linguistics), [NAACL](https://dblp.uni-trier.de/db/conf/naacl/index.html)(North American Chapter of the Association for Computational Linguistics), [EMNLP](https://dblp.uni-trier.de/db/conf/emnlp/index.html)(Conference on Empirical Methods in Natural Language Processing) and [COLING](https://dblp.uni-trier.de/db/conf/coling/index.html)(International Conference on Computational Linguistics).

- ACL [2022](https://aclanthology.org/events/acl-2022/), [2021](https://aclanthology.org/events/acl-2021/), [2019](https://aclanthology.org/events/acl-2019/)
- [NAACL](https://dblp.uni-trier.de/search?q=federate%20venue%3ANAACL-HLT%3A) [2022](https://aclanthology.org/events/naacl-2022/), [2021](https://aclanthology.org/events/naacl-2021/)
- [EMNLP](https://dblp.uni-trier.de/search?q=federate%20venue%3AEMNLP%3A) [2021](https://aclanthology.org/events/emnlp-2021/), [2020](https://aclanthology.org/events/emnlp-2020/)
- [COLING](https://dblp.uni-trier.de/search?q=federate%20venue%3ACOLING%3A) [2020](https://aclanthology.org/events/coling-2020/)



| Title                                                        | Affiliation                                       | Venue          | Year | TL;DR                               | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------------------------- | -------------- | ---- | ----------------------------------- | ------------------------------------------------------------ |
| Scaling Language Model Size in Cross-Device Federated Learning | Google                                            | ACL workshop   | 2022 |                                     | [[PUB](https://aclanthology.org/2022.fl4nlp-1.2/).] [PDF](https://arxiv.org/abs/2204.09715) |
| Intrinsic Gradient Compression for Scalable and Efficient Federated Learning | Oxford                                            | ACL workshop   | 2022 |                                     | [[PUB](https://aclanthology.org/2022.fl4nlp-1.4/).] [PDF](https://arxiv.org/abs/2112.02656) |
| ActPerFL: Active Personalized Federated Learning             | Amazon                                            | ACL workshop   | 2022 | ActPerFL[^ActPerFL]                 | [[PUB](https://aclanthology.org/2022.fl4nlp-1.1).] [Page](https://www.amazon.science/publications/actperfl-active-personalized-federated-learning) |
| FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks :fire: | USC                                               | NAACL          | 2022 | FedNLP[^FedNLP]                     | [[PUB](https://aclanthology.org/2022.findings-naacl.13/).] [PDF](https://arxiv.org/abs/2104.08815) [Code](https://github.com/FedML-AI/FedNLP) |
| Federated Learning with Noisy User Feedback                  | USC; Amazon                                       | NAACL          | 2022 |                                     | [[PUB](https://aclanthology.org/2022.naacl-main.196/).] [PDF](https://arxiv.org/abs/2205.03092) |
| Training Mixed-Domain Translation Models via Federated Learning | Amazon                                            | NAACL          | 2022 |                                     | [[PUB](https://aclanthology.org/2022.naacl-main.186).] [Page](https://www.amazon.science/publications/training-mixed-domain-translation-models-via-federated-learning) [PDF](https://arxiv.org/abs/2205.01557) |
| Pretrained Models for Multilingual Federated Learning        | Johns Hopkins University                          | NAACL          | 2022 |                                     | [[PUB](https://aclanthology.org/2022.naacl-main.101).] [PDF](https://arxiv.org/abs/2206.02291) [Code](https://github.com/orionw/multilingual-federated-learning) |
| Training Mixed-Domain Translation Models via Federated Learning | Amazon                                            | NAACL          | 2022 |                                     | [[PUB](https://aclanthology.org/2022.naacl-main.186/).] [Page](https://www.amazon.science/publications/training-mixed-domain-translation-models-via-federated-learning) [PDF](https://arxiv.org/abs/2205.01557) |
| Federated Chinese Word Segmentation with Global Character Associations | University of Washington                          | ACL workshop   | 2021 |                                     | [[PUB](https://aclanthology.org/2021.findings-acl.376).] [code](https://github.com/cuhksz-nlp/GCASeg) |
| Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation | USTC                                              | EMNLP          | 2021 | Efficient-FedRec[^Efficient-FedRec] | [[PUB](https://aclanthology.org/2021.emnlp-main.223).] [PDF](https://arxiv.org/abs/2109.05446) [Code](https://github.com/yjw1029/Efficient-FedRec) [Video](https://aclanthology.org/2021.emnlp-main.223.mp4) |
| Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories | CUHK (Shenzhen)                                   | EMNLP          | 2021 |                                     | [[PUB](https://aclanthology.org/2021.emnlp-main.321/).] [Code](https://github.com/cuhksz-nlp/ASA-TM) [Video](https://aclanthology.org/2021.emnlp-main.321.mp4) |
| A Secure and Efficient Federated Learning Framework for NLP  | University of Connecticut                         | EMNLP          | 2021 |                                     | [[PUB](https://aclanthology.org/2021.emnlp-main.606).] [PDF](https://arxiv.org/abs/2201.11934) [Video](https://aclanthology.org/2021.emnlp-main.606.mp4) |
| Distantly Supervised Relation Extraction in Federated Settings | UCAS                                              | EMNLP workshop | 2021 |                                     | [[PUB](https://aclanthology.org/2021.findings-emnlp.52).] [PDF](https://arxiv.org/abs/2008.05049) [Code](https://github.com/DianboWork/FedDS) |
| Federated Learning with Noisy User Feedback                  | USC; Amazon                                       | NAACL workshop | 2021 |                                     | [[PUB](https://aclanthology.org/2022.naacl-main.196).] [PDF](https://arxiv.org/abs/2205.03092) |
| An Investigation towards Differentially Private Sequence Tagging in a Federated Framework | Universität Hamburg                               | NAACL workshop | 2021 |                                     | [[PUB](https://aclanthology.org/2021.privatenlp-1.4).]       |
| Understanding Unintended Memorization in Language Models Under Federated Learning | Google                                            | NAACL workshop | 2021 |                                     | [[PUB](https://aclanthology.org/2021.privatenlp-1.1).] [PDF](https://arxiv.org/abs/2006.07490) |
| FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction | CAS                                               | EMNLP          | 2020 |                                     | [[PUB](https://aclanthology.org/2020.emnlp-main.165).] [Video](https://slideslive.com/38939230) [[解读](https://zhuanlan.zhihu.com/p/539347225)] |
| Empirical Studies of Institutional Federated Learning For Natural Language Processing | Ping An Technology                                | EMNLP workshop | 2020 |                                     | [[PUB](https://aclanthology.org/2020.findings-emnlp.55).]    |
| Federated Learning for Spoken Language Understanding         | PKU                                               | COLING         | 2020 |                                     | [[PUB](https://aclanthology.org/2020.coling-main.310/).]     |
| Two-stage Federated Phenotyping and Patient Representation Learning | Boston Children’s Hospital Harvard Medical School | ACL workshop   | 2019 |                                     | [[PUB](https://aclanthology.org/W19-5030).] [PDF](https://arxiv.org/abs/1908.05596) [Code](https://github.com/kaiyuanmifen/FederatedNLP) [UC.](https://github.com/MarcioPorto/federated-phenotyping) |



## FL in top IR conference and journal

In this section, we will summarize Federated Learning papers accepted by top Information Retrieval conference and journal, including [SIGIR](https://dblp.org/db/conf/sigir/index.html)(Annual International ACM SIGIR Conference on Research and Development in Information Retrieval).

- [SIGIR](https://dblp.uni-trier.de/search?q=federate%20venue%3ASIGIR%3A) [2022](https://dl.acm.org/doi/proceedings/10.1145/3477495), [2021](https://dl.acm.org/doi/proceedings/10.1145/3404835), [2020](https://dl.acm.org/doi/proceedings/10.1145/3397271)

| Title                                                        | Affiliation                     | Venue | Year | TL;DR                       | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------- | ----- | ---- | --------------------------- | ------------------------------------------------------------ |
| Is Non-IID Data a Threat in Federated Online Learning to Rank? | The University of Queensland    | SIGIR | 2022 | noniid-foltr[^noniid-foltr] | [[PUB](https://dl.acm.org/doi/10.1145/3477495.3531709).] [Code](https://github.com/ielab/2022-SIGIR-noniid-foltr) |
| FedCT: Federated Collaborative Transfer for Recommendation   | Rutgers University              | SIGIR | 2021 | FedCT[^FedCT]               | [[PUB](https://dl.acm.org/doi/10.1145/3404835.3462825).] [PDF](http://yongfeng.me/attach/liu-sigir2021.pdf) [Code](https://github.com/CharlieMat/EdgeCDR) |
| On the Privacy of Federated Pipelines                        | Technical University of Munich  | SIGIR | 2021 | FedGWAS[^FedGWAS]           | [[PUB](https://dl.acm.org/doi/10.1145/3404835.3462996).]     |
| FedCMR: Federated Cross-Modal Retrieval.                     | Dalian University of Technology | SIGIR | 2021 | FedCMR[^FedCMR]             | [[PUB](https://dl.acm.org/doi/10.1145/3404835.3462989).] [Code](https://github.com/hasakiXie123/FedCMR) |
| Meta Matrix Factorization for Federated Rating Predictions.  | SDU                             | SIGIR | 2020 | MetaMF[^MetaMF]             | [[PUB](https://dl.acm.org/doi/10.1145/3397271.3401081).] [PDF](https://arxiv.org/abs/1910.10086) |



## FL in top DB conference and journal

In this section, we will summarize Federated Learning papers accepted by top Database conference and journal, including [SIGMOD](https://dblp.uni-trier.de/db/conf/sigmod/index.html)(ACM SIGMOD Conference) , [ICDE](https://dblp.uni-trier.de/db/conf/icde/index.html)(IEEE International Conference on Data Engineering) and [VLDB](https://dblp.uni-trier.de/db/conf/vldb/index.html)(Very Large Data Bases Conference).

- [SIGMOD](https://dblp.uni-trier.de/search?q=federate%20venue%3ASIGMOD%20Conference%3A) [2022](https://2022.sigmod.org/sigmod_research_list.shtml), [2021](https://2021.sigmod.org/sigmod_research_list.shtml)
- [ICDE](https://dblp.uni-trier.de/search?q=federate%20venue%3AICDE%3A) [2022](https://icde2022.ieeecomputer.my/accepted-research-track/), [2021](https://ieeexplore.ieee.org/xpl/conhome/9458599/proceeding)
- [VLDB](https://dblp.org/search?q=federate%20venue%3AProc%20VLDB%20Endow%3A) [2021](https://vldb.org/pvldb/vol15-volume-info/), [2021](http://www.vldb.org/pvldb/vol14/), [2020](http://vldb.org/pvldb/vol13-volume-info/)

| Title                                                        | Affiliation                     | Venue           | Year | TL;DR                               | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------- | --------------- | ---- | ----------------------------------- | ------------------------------------------------------------ |
| Improving Fairness for Data Valuation in Horizontal Federated Learning | The UBC                         | ICDE            | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9835382).] [PDF](https://arxiv.org/abs/2109.09046) |
| FedADMM: A Robust Federated Deep Learning Framework with Adaptivity to System Heterogeneity | USTC                            | ICDE            | 2022 | FedADMM[^FedADMM]                   | [[PUB](https://ieeexplore.ieee.org/document/9835545).] [PDF](https://arxiv.org/abs/2204.03529) [Code](https://github.com/YonghaiGong/FedADMM) |
| FedMP: Federated Learning through Adaptive Model Pruning in Heterogeneous Edge Computing. | USTC                            | ICDE            | 2022 | FedMP[^FedMP]                       | [[PUB](https://ieeexplore.ieee.org/document/9835327).]       |
| Federated Learning on Non-IID Data Silos: An Experimental Study. :fire: | NUS                             | ICDE            | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9835537).] [PDF](https://arxiv.org/abs/2102.02079) [Code](https://github.com/Xtra-Computing/NIID-Bench) |
| Enhancing Federated Learning with Intelligent Model Migration in Heterogeneous Edge Computing | USTC                            | ICDE            | 2022 | FedMigr[^FedMigr]                   | [[PUB](https://ieeexplore.ieee.org/document/9835657).]       |
| Samba: A System for Secure Federated Multi-Armed Bandits     |                                 | ICDE            | 2022 | Samba[^Samba]                       | [[PUB](https://ieeexplore.ieee.org/document/9835585).] [Code](https://github.com/gamarcad/samba-demo) |
| FedRecAttack: Model Poisoning Attack to Federated Recommendation | ZJU                             | ICDE            | 2022 | FedRecAttack[^FedRecAttack]         | [[PUB](https://ieeexplore.ieee.org/document/9835228).] [PDF](https://arxiv.org/abs/2204.01499) [Code](https://github.com/rdz98/fedrecattack) |
| Enhancing Federated Learning with In-Cloud Unlabeled Data    | USTC                            | ICDE            | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9835163).]       |
| Efficient Participant Contribution Evaluation for Horizontal and Vertical Federated Learning | USTC                            | ICDE            | 2022 | DIG-FL[^DIG-FL]                     | [[PUB](https://ieeexplore.ieee.org/document/9835159).]       |
| An Introduction to Federated Computation                     | University of Warwick; Facebook | SIGMOD          | 2022 |                                     | [[PUB](https://dl.acm.org/doi/10.1145/3514221.3522561).]     |
| BlindFL: Vertical Federated Machine Learning without Peeking into Your Data | PKU; Tencent                    | SIGMOD          | 2022 | BlindFL[^BlindFL]                   | [[PUB](https://dl.acm.org/doi/10.1145/3514221.3526127).] [PDF](https://arxiv.org/abs/2206.07975) |
| An Efficient Approach for Cross-Silo Federated Learning to Rank |                                 | ICDE            | 2021 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9458704).] [[Related Paper(zh)](https://kns.cnki.net/kcms/detail/detail.aspx?doi=10.13328/j.cnki.jos.006174)] |
| Feature Inference Attack on Model Predictions in Vertical Federated Learning |                                 | ICDE            | 2021 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9458672/).] [PDF](https://arxiv.org/abs/2010.10152) [Code](https://github.com/xj231/featureinference-vfl) |
| Efficient Federated-Learning Model Debugging                 |                                 | ICDE            | 2021 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9458829).]       |
| Federated Matrix Factorization with Privacy Guarantee        | Purdue                          | VLDB            | 2021 |                                     | [[PUB](https://www.vldb.org/pvldb/vol15/p900-li.pdf).]       |
| Projected Federated Averaging with Heterogeneous Differential Privacy. | Renmin University of China      | VLDB            | 2021 |                                     | [[PUB](https://dl.acm.org/doi/10.14778/3503585.3503592).] [Code](https://github.com/Emory-AIMS/PFA) |
| Enabling SQL-based Training Data Debugging for Federated Learning | Simon Fraser University         | VLDB            | 2021 | FedRain-and-Frog[^FedRain-and-Frog] | [[PUB](http://www.vldb.org/pvldb/vol15/p388-wu.pdf).] [PDF](https://arxiv.org/abs/2108.11884) [Code](https://github.com/sfu-db/FedRain-and-Frog) |
| Refiner: A Reliable Incentive-Driven Federated Learning System Powered by Blockchain | ZJU                             | VLDB            | 2021 |                                     | [[PUB](http://vldb.org/pvldb/vol14/p2659-jiang.pdf).]        |
| Tanium Reveal: A Federated Search Engine for Querying Unstructured File Data on Large Enterprise Networks | Tanium Inc.                     | VLDB            | 2021 |                                     | [[PUB](http://www.vldb.org/pvldb/vol14/p3096-stoddard.pdf).] [Video](https://www.bilibili.com/video/BV1Wg411j7aA) |
| VF2Boost: Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning | PKU                             | SIGMOD          | 2021 | VF2Boost[^VF2Boost]                 | [[PUB](https://dl.acm.org/doi/10.1145/3448016.3457241).]     |
| ExDRa: Exploratory Data Science on Federated Raw Data        |                                 | SIGMOD          | 2021 |                                     | [[PUB](https://dl.acm.org/doi/10.1145/3448016.3457549).]     |
| Joint blockchain and federated learning-based offloading in harsh edge computing environments |                                 | SIGMOD workshop | 2021 |                                     | [[PUB](https://dl.acm.org/doi/10.1145/3460866.3461765).]     |
| Privacy Preserving Vertical Federated Learning for Tree-based Models | NUS                             | VLDB            | 2020 | Pivot-DT[^Pivot-DT]                 | [[PUB](http://vldb.org/pvldb/vol13/p2090-wu.pdf).] [PDF](https://arxiv.org/abs/2008.06170) [[Video](https://www.youtube.com/watch?v=sjii8oVCqiY)] [Code](https://github.com/nusdbsystem/pivot) |



## FL in top Network conference and journal

In this section, we will summarize Federated Learning papers accepted by top Database conference and journal, including [SIGCOMM](https://dblp.org/db/conf/sigcomm/index.html)(Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication), [INFOCOM](https://dblp.org/db/conf/infocom/index.html)(IEEE Conference on Computer Communications), [MobiCom](https://dblp.org/db/conf/mobicom/index.html)(ACM/IEEE International Conference on Mobile Computing and Networking), [NSDI](https://dblp.org/db/conf/nsdi/index.html)(Symposium on Networked Systems Design and Implementation) and [WWW](https://dblp.org/db/conf/www/index.html)(The Web Conference).

- [SIGCOMM](https://dblp.uni-trier.de/search?q=federate%20venue%3ASIGCOMM%3A) NULL

- [INFOCOM](https://dblp.uni-trier.de/search?q=federate%20venue%3AINFOCOM%3A) [2022](https://infocom2022.ieee-infocom.org/program/accepted-paper-list-main-conference)([Page](https://infocom.info/day/3/track/Track%20B#B-7)), [2021](https://infocom2021.ieee-infocom.org/accepted-paper-list-main-conference.html)([Page](https://duetone.org/infocom21)), [2020](https://infocom2020.ieee-infocom.org/accepted-paper-list-main-conference.html)([Page](https://duetone.org/infocom20)), [2019](https://infocom2019.ieee-infocom.org/accepted-paper-list-main-conference.html), 2018
- [MobiCom](https://dblp.uni-trier.de/search?q=federate%20venue%3AMobiCom%3A) [2021](https://www.sigmobile.org/mobicom/2021/accepted.html), [2020](https://www.sigmobile.org/mobicom/2020/accepted.php)
- [NSDI](https://dblp.uni-trier.de/search?q=federate%20venue%3ANSDI%3A) NULL
- [WWW](https://dblp.uni-trier.de/search?q=federate%20venue%3AWWW%3A) [2022](https://www2022.thewebconf.org/accepted-papers/), [2021](https://www2021.thewebconf.org/program/papers-program/links/index.html)

| Title                                                        | Affiliation                                                  | Venue      | Year | TL;DR                                     | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ---------- | ---- | ----------------------------------------- | ------------------------------------------------------------ |
| Joint Superposition Coding and Training for Federated Learning over Multi-Width Neural Networks | Korea University                                             | INFOCOM    | 2022 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9796733).]       |
| Towards Optimal Multi-Modal Federated Learning on Non-IID Data with Hierarchical Gradient Blending | University of Toronto                                        | INFOCOM    | 2022 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9796724).]       |
| Optimal Rate Adaption in Federated Learning with Compressed Communications | SZU                                                          | INFOCOM    | 2022 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9796982).] [PDF](https://arxiv.org/abs/2112.06694) |
| The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining. | CityU                                                        | INFOCOM    | 2022 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9796721).] [PDF](https://arxiv.org/abs/2203.07320) |
| Tackling System and Statistical Heterogeneity for Federated Learning with Adaptive Client Sampling. | CUHK; AIRS ;Yale University                                  | INFOCOM    | 2022 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9796935).] [PDF](https://arxiv.org/abs/2112.11256) |
| Communication-Efficient Device Scheduling for Federated Learning Using Stochastic Optimization | Army Research Laboratory, Adelphi                            | INFOCOM    | 2022 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9796818).] [PDF](https://arxiv.org/abs/2201.07912) |
| FLASH: Federated Learning for Automated Selection of High-band mmWave Sectors | NEU                                                          | INFOCOM    | 2022 | FLASH[^FLASH]                             | [[PUB](https://ieeexplore.ieee.org/document/9796865).] [Code](https://github.com/Batool-Salehi/FL-based-Sector-Selection) |
| A Profit-Maximizing Model Marketplace with Differentially Private Federated Learning | CUHK; AIRS                                                   | INFOCOM    | 2022 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9796833).]       |
| Protect Privacy from Gradient Leakage Attack in Federated Learning | PolyU                                                        | INFOCOM    | 2022 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9796841/).] [Slides](https://jxiao.wang/slides/INFOCOM22.pdf) |
| FedFPM: A Unified Federated Analytics Framework for Collaborative Frequent Pattern Mining. | SJTU                                                         | INFOCOM    | 2022 | FedFPM[^FedFPM]                           | [[PUB](https://ieeexplore.ieee.org/document/9796719).] [Code](https://github.com/HuskyW/FFPA) |
| An Accuracy-Lossless Perturbation Method for Defending Privacy Attacks in Federated Learning | SWJTU;THU                                                    | WWW        | 2022 | PBPFL[^PBPFL]                             | [[PUB](https://dl.acm.org/doi/10.1145/3485447.3512233).] [PDF](https://arxiv.org/abs/2002.09843) [Code](https://github.com/Kira0096/PBPFL) |
| LocFedMix-SL: Localize, Federate, and Mix for Improved Scalability, Convergence, and Latency in Split Learning | Yonsei University                                            | WWW        | 2022 | LocFedMix-SL[^LocFedMix-SL]               | [[PUB](https://dl.acm.org/doi/10.1145/3485447.3512153).]     |
| Federated Unlearning via Class-Discriminative Pruning        | PolyU                                                        | WWW        | 2022 |                                           | [[PUB](https://dl.acm.org/doi/10.1145/3485447.3512222).] [PDF ](https://arxiv.org/abs/2110.11794)[Code](https://github.com/wangjunxiao/unlearning) [Code](https://github.com/MoonkeyBoy/Federated-Unlearning-via-Class-Discriminative-Pruning) |
| FedKC: Federated Knowledge Composition for Multilingual Natural Language Understanding | Purdue                                                       | WWW        | 2022 | FedKC[^FedKC]                             | [[PUB](https://dl.acm.org/doi/10.1145/3485447.3511988).]     |
| Federated Bandit: A Gossiping Approach                       | University of California                                     | SIGMETRICS | 2021 | Federated Bandit<br />[^Federated-Bandit] | [[PUB](https://dl.acm.org/doi/10.1145/3447380).] [PDF](https://arxiv.org/abs/2010.12763) |
| Hermes: an efficient federated learning framework for heterogeneous mobile clients | Duke University                                              | MobiCom    | 2021 | Hermes[^Hermes]                           | [[PUB](https://dl.acm.org/doi/10.1145/3447993.3483278).]     |
| Federated mobile sensing for activity recognition            | Samsung AI Center                                            | MobiCom    | 2021 |                                           | [[PUB](https://dl.acm.org/doi/10.1145/3447993.3488031).] [Page](https://federatedsensing.gitlab.io/) [Talks](https://federatedsensing.gitlab.io/talks/) [Video](https://federatedsensing.gitlab.io/program/) |
| Learning for Learning: Predictive Online Control of Federated Learning with Edge Provisioning. | Nanjing University                                           | INFOCOM    | 2021 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9488733/).]      |
| Device Sampling for Heterogeneous Federated Learning: Theory, Algorithms, and Implementation. | Purdue                                                       | INFOCOM    | 2021 | D2D-FedL[^D2D-FedL]                       | [[PUB](https://ieeexplore.ieee.org/document/9488906).] [PDF](https://arxiv.org/abs/2101.00787) |
| FAIR: Quality-Aware Federated Learning with Precise User Incentive and Model Aggregation | THU                                                          | INFOCOM    | 2021 | FAIR[^FAIR]                               | [[PUB](https://ieeexplore.ieee.org/document/9488743).]       |
| Sample-level Data Selection for Federated Learning           | USTC                                                         | INFOCOM    | 2021 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9488723).]       |
| To Talk or to Work: Flexible Communication Compression for Energy Efficient Federated Learning over Heterogeneous Mobile Edge Devices | Xidian University; CAS                                       | INFOCOM    | 2021 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9488839).] [PDF](https://arxiv.org/abs/2012.11804) |
| Cost-Effective Federated Learning Design                     | CUHK; AIRS; Yale University                                  | INFOCOM    | 2021 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9488679).] [PDF](https://arxiv.org/abs/2012.08336) |
| An Incentive Mechanism for Cross-Silo Federated Learning: A Public Goods Perspective | The UBC                                                      | INFOCOM    | 2021 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9488705).]       |
| Resource-Efficient Federated Learning with Hierarchical Aggregation in Edge Computing | USTC                                                         | INFOCOM    | 2021 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9488756/).]      |
| FedServing: A Federated Prediction Serving Framework Based on Incentive Mechanism. | Jinan University; CityU                                      | INFOCOM    | 2021 | FedServing[^FedServing]                   | [[PUB](https://ieeexplore.ieee.org/document/9488807).] [PDF](https://arxiv.org/abs/2012.10566) |
| Federated Learning over Wireless Networks: A Band-limited Coordinated Descent Approach | Arizona State University                                     | INFOCOM    | 2021 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9488818).] [PDF](https://arxiv.org/abs/2102.07972) |
| Dual Attention-Based Federated Learning for Wireless Traffic Prediction | King Abdullah University of Science and Technology           | INFOCOM    | 2021 | FedDA[^FedDA]                             | [[PUB](https://ieeexplore.ieee.org/document/9488883).] [PDF](https://arxiv.org/abs/2110.05183) [Code](https://codeocean.com/capsule/4884085/) [Code](https://github.com/chuanting/fedda) |
| FedSens: A Federated Learning Approach for Smart Health Sensing with Class Imbalance in Resource Constrained Edge Computing | University of Notre Dame                                     | INFOCOM    | 2021 | FedSens[^FedSens]                         | [[PUB](https://ieeexplore.ieee.org/document/9488776/).]      |
| P-FedAvg: Parallelizing Federated Learning with Theoretical Guarantees | SYSU; Guangdong Key Laboratory of Big Data Analysis and Processing | INFOCOM    | 2021 | P-FedAvg[^P-FedAvg]                       | [[PUB](https://ieeexplore.ieee.org/document/9488877).]       |
| Meta-HAR: Federated Representation Learning for Human Activity Recognition. | University of Alberta                                        | WWW        | 2021 | Meta-HAR[^Meta-HAR]                       | [[PUB](https://dl.acm.org/doi/10.1145/3442381.3450006).] [PDF](https://arxiv.org/abs/2106.00615) [Code](https://github.com/Chain123/Meta-HAR) |
| PFA: Privacy-preserving Federated Adaptation for Effective Model Personalization | PKU                                                          | WWW        | 2021 | PFA[^PFA]                                 | [[PUB](https://dl.acm.org/doi/10.1145/3442381.3449847).] [PDF](https://arxiv.org/abs/2103.01548) [Code](https://github.com/lebyni/PFA) |
| Communication Efficient Federated Generalized Tensor Factorization for Collaborative Health Data Analytics | Emory                                                        | WWW        | 2021 | FedGTF-EF-PC[^FedGTF-EF-PC]               | [[PUB](https://dl.acm.org/doi/10.1145/3442381.3449832).] [Code](https://github.com/jma78/FedGTF-EF) |
| Hierarchical Personalized Federated Learning for User Modeling | USTC                                                         | WWW        | 2021 |                                           | [[PUB](https://dl.acm.org/doi/10.1145/3442381.3449926).]     |
| Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data | PKU                                                          | WWW        | 2021 | Heter-aware[^Heter-aware]                 | [[PUB](https://dl.acm.org/doi/10.1145/3442381.3449851).] [PDF](https://arxiv.org/abs/2006.06983) [Slides](https://qipengwang.github.io/files/www21.slides.pdf) [Code](https://github.com/PKU-Chengxu/FLASH) |
| Incentive Mechanism for Horizontal Federated Learning Based on Reputation and Reverse Auction | SYSU                                                         | WWW        | 2021 |                                           | [[PUB](https://dl.acm.org/doi/10.1145/3442381.3449888).]     |
| Physical-Layer Arithmetic for Federated Learning in Uplink MU-MIMO Enabled Wireless Networks. | Nanjing University                                           | INFOCOM    | 2020 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9155479).]       |
| Optimizing Federated Learning on Non-IID Data with Reinforcement Learning :fire: | University of Toronto                                        | INFOCOM    | 2020 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9155494).] [Slides](https://workshoputrgv.github.io/slides/hao_wang.pdf) [Code](https://github.com/iQua/flsim) [[解读](https://zhuanlan.zhihu.com/p/458716656)] |
| Enabling Execution Assurance of Federated Learning at Untrusted Participants | THU                                                          | INFOCOM    | 2020 |                                           | [[PUB](https://ieeexplore.ieee.org/document/9155414).] [Code](https://github.com/zeyu-zh/TrustFL) |
| Billion-scale federated learning on mobile clients: a submodel design with tunable privacy | SJTU                                                         | MobiCom    | 2020 |                                           | [[PUB](https://dl.acm.org/doi/10.1145/3372224.3419188).]     |
| Federated Learning over Wireless Networks: Optimization Model Design and Analysis | The University of Sydney                                     | INFOCOM    | 2019 |                                           | [[PUB](https://ieeexplore.ieee.org/document/8737464).] [Code](https://github.com/nhatminh/FEDL-INFOCOM) |
| Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning | Wuhan University                                             | INFOCOM    | 2019 |                                           | [[PUB](https://ieeexplore.ieee.org/document/8737416).] [PDF](https://arxiv.org/abs/1812.00535) [UC.](https://github.com/JonasGeiping/breaching) |
| InPrivate Digging: Enabling Tree-based Distributed Data Mining with Differential Privacy | Collaborative Innovation Center of Geospatial Technology     | INFOCOM    | 2018 | TFL[^TFL]                                 | [[PUB](https://ieeexplore.ieee.org/document/8486352).]       |



## FL in top System conference and journal

In this section, we will summarize Federated Learning papers accepted by top Database conference and journal, including [OSDI](https://dblp.org/db/conf/osdi/index.html)(USENIX Symposium on Operating Systems Design and Implementation), [SOSP](https://dblp.org/db/conf/sosp/index.html)(Symposium on Operating Systems Principles), [ISCA](https://dblp.org/db/conf/isca/index.html)(International Symposium on Computer Architecture), [MLSys](https://dblp.org/db/conf/mlsys/index.html)(Conference on Machine Learning and Systems), [TPDS](https://dblp.uni-trier.de/db/journals/tpds/index.html)(IEEE Transactions on Parallel and Distributed Systems).

- [OSDI](https://dblp.org/search?q=federated%20venue%3AOSDI%3A) 2021
- [SOSP](https://dblp.org/search?q=federated%20venue%3ASOSP%3A) 2021
- [ISCA](https://dblp.org/search?q=federated%20venue%3AISCA%3A) NULL
- [MLSys](https://dblp.org/search?q=federated%20venue%3AMLSys%3A) 2022, 2020, 2019
- [TPDS](https://dblp.uni-trier.de/search?q=federate%20venue%3AIEEE%20Trans.%20Parallel%20Distributed%20Syst.%3A) 2022, 2021, 2020


| Title                                                        | Affiliation               | Venue                     | Year | TL;DR                               | Materials                                                    |
| ------------------------------------------------------------ | ------------------------- | ------------------------- | ---- | ----------------------------------- | ------------------------------------------------------------ |
| FedGraph: Federated Graph Learning with Intelligent Sampling | UoA                       | TPDS                      | 2022 | FedGraph[^FedGraph]                 | [[PUB.]](https://ieeexplore.ieee.org/abstract/document/9606516/) [Code](https://github.com/cfh19980612/FedGraph) [[解读](https://zhuanlan.zhihu.com/p/442233479)] |
| AUCTION: Automated and Quality-Aware Client Selection Framework for Efficient Federated Learning. | THU                       | TPDS                      | 2022 | AUCTION[^AUCTION]                   | [[PUB](https://ieeexplore.ieee.org/document/9647925).]       |
| DONE: Distributed Approximate Newton-type Method for Federated Edge Learning. | University of Sydney      | TPDS                      | 2022 | DONE[^DONE]                         | [[PUB](https://ieeexplore.ieee.org/document/9695269).] [PDF](https://arxiv.org/abs/2012.05625) [Code](https://github.com/dual-grp/DONE) |
| Flexible Clustered Federated Learning for Client-Level Data Distribution Shift. | CQU                       | TPDS                      | 2022 | FlexCFL[^FlexCFL]                   | [[PUB](https://ieeexplore.ieee.org/document/9647969).] [PDF](https://arxiv.org/abs/2108.09749) [Code](https://github.com/morningd/flexcfl) |
| Min-Max Cost Optimization for Efficient Hierarchical Federated Learning in Wireless Edge Networks. | Xidian University         | TPDS                      | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9629331).]       |
| LightFed: An Efficient and Secure Federated Edge Learning System on Model Splitting. | CSU                       | TPDS                      | 2022 | LightFed[^LightFed]                 | [[PUB](https://ieeexplore.ieee.org/document/9613755).]       |
| On the Benefits of Multiple Gossip Steps in Communication-Constrained Decentralized Federated Learning. | Purdue                    | TPDS                      | 2022 | Deli-CoCo[^Deli-CoCo]               | [[PUB](https://ieeexplore.ieee.org/document/9664349).] [PDF](https://arxiv.org/abs/2011.10643) [Code](https://github.com/anishacharya/DeLiCoCo) |
| Incentive-Aware Autonomous Client Participation in Federated Learning. | Sun Yat-sen University    | TPDS                      | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9705080).]       |
| Communicational and Computational Efficient Federated Domain Adaptation. | HKUST                     | TPDS                      | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9757821).]       |
| Decentralized Edge Intelligence: A Dynamic Resource Allocation Framework for Hierarchical Federated Learning. | NTU                       | TPDS                      | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9479786).]       |
| Differentially Private Byzantine-Robust Federated Learning.  | Qufu Normal University    | TPDS                      | 2022 | DPBFL[^DPBFL]                       | [[PUB](https://ieeexplore.ieee.org/document/9757841).]       |
| Multi-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing. | University of Exeter      | TPDS                      | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9492755).] [PDF](https://arxiv.org/abs/2007.09236) [Code](https://codeocean.com/capsule/7772141/tree/v1) [Code](https://github.com/JedMills/MTFL-For-Personalised-DNNs) |
| Reputation-Aware Hedonic Coalition Formation for Efficient Serverless Hierarchical Federated Learning. | BUAA                      | TPDS                      | 2022 | SHFL[^SHFL]                         | [[PUB](https://ieeexplore.ieee.org/document/9665214).]       |
| Differentially Private Federated Temporal Difference Learning. | Stony Brook University    | TPDS                      | 2022 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9645233).]       |
| Towards Efficient and Stable K-Asynchronous Federated Learning With Unbounded Stale Gradients on Non-IID Data. | XJTU                      | TPDS                      | 2022 | WKAFL[^WKAFL]                       | [[PUB](https://ieeexplore.ieee.org/document/9712243).] [PDF](https://arxiv.org/abs/2203.01214) |
| Communication-Efficient Federated Learning With Compensated Overlap-FedAvg. | SCU                       | TPDS                      | 2022 | Overlap-FedAvg[^Overlap-FedAvg]     | [[PUB](https://ieeexplore.ieee.org/document/9459540).] [PDF](https://arxiv.org/abs/2012.06706) [Code](https://github.com/Soptq/Overlap-FedAvg) |
| PAPAYA: Practical, Private, and Scalable Federated Learning. | Meta AI                   | MLSys                     | 2022 | PAPAYA[^PAPAYA]                     | [[PDF](https://arxiv.org/abs/2111.04877)] [[PUB](https://proceedings.mlsys.org/paper/2022/hash/f340f1b1f65b6df5b5e3f94d95b11daf-Abstract.html).] |
| LightSecAgg: a Lightweight and Versatile Design for Secure Aggregation in Federated Learning | USC                       | MLSys                     | 2022 | LightSecAgg[^LightSecAgg]           | [[PDF](https://arxiv.org/abs/2109.14236)] [[PUB](https://proceedings.mlsys.org/paper/2022/hash/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html).] [Code](https://github.com/LightSecAgg/MLSys2022_anonymous) |
| Oort: Efficient Federated Learning via Guided Participant Selection | University of Michigan    | OSDI                      | 2021 | Oort[^Oort]                         | [[PUB](https://www.usenix.org/conference/osdi21/presentation/lai).] [[PDF](https://arxiv.org/abs/2010.06081)] [Code](https://github.com/SymbioticLab/Oort) [Slides](https://www.usenix.org/system/files/osdi21_slides_lai.pdf) [Video](https://www.youtube.com/watch?v=5npOel4T4Mw) |
| Towards Efficient Scheduling of Federated Mobile Devices Under Computational and Statistical Heterogeneity. | Old Dominion University   | TPDS                      | 2021 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9195793).] [PDF](https://arxiv.org/abs/2005.12326) |
| Self-Balancing Federated Learning With Global Imbalanced Data in Mobile Systems. | CQU                       | TPDS                      | 2021 | Astraea[^Astraea]                   | [[PUB](https://ieeexplore.ieee.org/document/9141436).] [Code](https://github.com/mtang724/Self-Balancing-Federated-Learning) |
| An Efficiency-Boosting Client Selection Scheme for Federated Learning With Fairness Guarantee | SCUT                      | TPDS                      | 2021 | RBCS-F[^RBCS-F]                     | [[PUB](https://ieeexplore.ieee.org/document/9272649/).] [[PDF](https://arxiv.org/abs/2011.01783)] [[解读](https://zhuanlan.zhihu.com/p/456101770)] |
| Proof of Federated Learning: A Novel Energy-Recycling Consensus Algorithm. | Beijing Normal University | TPDS                      | 2021 | PoFL[^PoFL]                         | [[PUB](https://ieeexplore.ieee.org/document/9347812).] [PDF](https://arxiv.org/abs/1912.11745) |
| Biscotti: A Blockchain System for Private and Secure Federated Learning. | UBC                       | TPDS                      | 2021 | Biscotti[^Biscotti]                 | [[PUB](https://ieeexplore.ieee.org/document/9292450).]       |
| Mutual Information Driven Federated Learning.                | Deakin University         | TPDS                      | 2021 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9272656).]       |
| Accelerating Federated Learning Over Reliability-Agnostic Clients in Mobile Edge Computing Systems. | University of Warwick     | TPDS                      | 2021 |                                     | [[PUB](https://ieeexplore.ieee.org/document/9272671).] [PDF](https://arxiv.org/abs/2007.14374) |
| FedSCR: Structure-Based Communication Reduction for Federated Learning. | HKU                       | TPDS                      | 2021 | FedSCR[^FedSCR]                     | [[PUB](https://ieeexplore.ieee.org/document/9303442).]       |
| FedScale: Benchmarking Model and System Performance of Federated Learning :fire: | University of Michigan    | SOSP workshop / ICML 2022 | 2021 | FedScale[^FedScale]                 | [[PUB](https://dl.acm.org/doi/10.1145/3477114.3488760).] [[PUB](https://proceedings.mlr.press/v162/lai22a.html).] [PDF](https://arxiv.org/abs/2105.11367) [code](https://github.com/SymbioticLab/FedScale) [[解读](https://zhuanlan.zhihu.com/p/520020117)] |
| Redundancy in cost functions for Byzantine fault-tolerant federated learning |                           | SOSP workshop             | 2021 |                                     | [[PUB](https://dl.acm.org/doi/10.1145/3477114.3488761).]     |
| Towards an Efficient System for Differentially-private, Cross-device Federated Learning |                           | SOSP workshop             | 2021 |                                     | [[PUB](https://dl.acm.org/doi/10.1145/3477114.3488762).]     |
| GradSec: a TEE-based Scheme Against Federated Learning Inference Attacks |                           | SOSP workshop             | 2021 |                                     | [[PUB](https://dl.acm.org/doi/10.1145/3477114.3488763).]     |
| Community-Structured Decentralized Learning for Resilient EI. |                           | SOSP workshop             | 2021 |                                     | [[PUB](https://dl.acm.org/doi/10.1145/3477114.3488764).]     |
| Separation of Powers in Federated Learning (Poster Paper)    | IBM Research              | SOSP workshop             | 2021 | TRUDA[^TRUDA]                       | [[PUB](https://dl.acm.org/doi/10.1145/3477114.3488765).] [PDF](https://arxiv.org/abs/2105.09400) |
| Accelerating Federated Learning via Momentum Gradient Descent. | USTC                      | TPDS                      | 2020 | MFL[^MFL]                           | [[PUB](https://ieeexplore.ieee.org/document/9003425).] [PDF](https://arxiv.org/abs/1910.03197) |
| Towards Fair and Privacy-Preserving Federated Deep Models.   | NUS                       | TPDS                      | 2020 | FPPDL[^FPPDL]                       | [[PUB](https://ieeexplore.ieee.org/document/9098045).] [PDF](https://arxiv.org/abs/1906.01167) [Code](https://github.com/lingjuanlv/FPPDL) |
| Federated Optimization in Heterogeneous Networks :fire:      | CMU                       | MLSys                     | 2020 | FedProx[^FedProx]                   | [[PUB](https://proceedings.mlsys.org/paper/2020/hash/38af86134b65d0f10fe33d30dd76442e-Abstract.html).] [[PDF](https://arxiv.org/abs/1812.06127)] [Code](https://github.com/litian96/FedProx) |
| Towards Federated Learning at Scale: System Design           | Google                    | MLSys                     | 2019 | System_Design<br />[^System_Design] | [[PUB](https://proceedings.mlsys.org/paper/2019/hash/bd686fd640be98efaae0091fa301e613-Abstract.html).] [[PDF](https://arxiv.org/abs/1902.01046)] [[解读](https://zhuanlan.zhihu.com/p/450993635)] |



# Framework

## Federated Learning Framework

### Table

*Note: **SG** means Support for Graph data and algorithms, **ST** means Support for Tabular data and algorithms.*

| Platform                                                     | Papers                                                       | Affiliations                                                 |                  SG                  |                  ST                  | Materials                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | :----------------------------------: | :----------------------------------: | ------------------------------------------------------------ |
| [PySyft](https://github.com/OpenMined/PySyft)<br />[![Stars](https://img.shields.io/github/stars/OpenMined/PySyft.svg?color=red)](https://github.com/OpenMined/PySyft/stargazers)<br />![](https://img.shields.io/github/last-commit/OpenMined/PySyft) | [A generic framework for privacy preserving deep learning](https://arxiv.org/abs/1811.04017) | [OpenMined](https://www.openmined.org/)                      |                                      |                                      | [Doc](https://pysyft.readthedocs.io/en/latest/installing.html) |
| [FATE](https://github.com/FederatedAI/FATE)<br />[![Stars](https://img.shields.io/github/stars/FederatedAI/FATE.svg?color=red)](https://github.com/FederatedAI/FATE/stargazers)<br />![](https://img.shields.io/github/last-commit/FederatedAI/FATE) | [FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection](https://www.jmlr.org/papers/volume22/20-815/20-815.pdf) | [WeBank](https://fedai.org/)                                 |                                      | :white_check_mark::white_check_mark: | [Doc](https://fate.readthedocs.io/en/latest/)<br />[Doc(zh)](https://fate.readthedocs.io/en/latest/zh/) |
| [MindSpore Federated](https://github.com/mindspore-ai/mindspore/tree/master/tests/st/fl)<br />[![Stars](https://img.shields.io/github/stars/mindspore-ai/mindspore.svg?color=red)](https://github.com/mindspore-ai/mindspore/stargazers)<br />![](https://img.shields.io/github/last-commit/mindspore-ai/mindspore) |                                                              | HUAWEI                                                       |                                      |                                      | [Doc](https://mindspore.cn/federated/docs/zh-CN/r1.6/index.html) <br />[Page](https://mindspore.cn/federated) |
| [TFF(Tensorflow-Federated)](https://github.com/tensorflow/federated) <br />[![Stars](https://img.shields.io/github/stars/tensorflow/federated.svg?color=red)](https://github.com/tensorflow/federated/stargazers)<br />![](https://img.shields.io/github/last-commit/tensorflow/federated) | [Towards Federated Learning at Scale: System Design](https://arxiv.org/abs/1902.01046) | Google                                                       |                                      |                                      | [Doc](https://www.tensorflow.org/federated) <br />[Page](https://www.tensorflow.org/federated) |
| [FedML](https://github.com/FedML-AI/FedML)<br />[![Stars](https://img.shields.io/github/stars/FedML-AI/FedML.svg?color=red)](https://github.com/FedML-AI/FedML/stargazers)<br />![](https://img.shields.io/github/last-commit/FedML-AI/FedML) | [FedML: A Research Library and Benchmark for Federated Machine Learning](https://arxiv.org/abs/2007.13518) | [FedML](https://fedml.ai/)                                   | :white_check_mark::white_check_mark: |          :white_check_mark:          | [Doc](https://doc.fedml.ai/)                                 |
| [Flower](https://github.com/adap/flower)<br />[![Stars](https://img.shields.io/github/stars/adap/flower.svg?color=red)](https://github.com/adap/flower/stargazers)<br />![](https://img.shields.io/github/last-commit/adap/flower) | [Flower: A Friendly Federated Learning Research Framework](https://arxiv.org/abs/2104.03042.pdf) | [flower.dev](https://flower.dev/) [adap](https://adap.com/en) |                                      |                                      | [Doc](https://flower.dev/docs/)                              |
| [Fedlearner](https://github.com/bytedance/fedlearner)<br />[![Stars](https://img.shields.io/github/stars/bytedance/fedlearner.svg?color=blue)](https://github.com/bytedance/fedlearner/stargazers)<br />![](https://img.shields.io/github/last-commit/bytedance/fedlearner) |                                                              | [Bytedance](https://github.com/bytedance)                    |                                      |                                      |                                                              |
| [SecretFlow](https://github.com/secretflow/secretflow) <br />[![Stars](https://img.shields.io/github/stars/secretflow/secretflow.svg?color=blue)](https://github.com/secretflow/secretflow/stargazers)<br />![](https://img.shields.io/github/last-commit/secretflow/secretflow) |                                                              | [Ant group](https://www.antgroup.com/)                       |                                      |          :white_check_mark:          | [Doc](https://secretflow.readthedocs.io/en/latest/getting_started/index.html) |
| [FederatedScope](https://github.com/alibaba/FederatedScope)<br />[![Stars](https://img.shields.io/github/stars/alibaba/FederatedScope.svg?color=blue)](https://github.com/alibaba/FederatedScope/stargazers)<br />![](https://img.shields.io/github/last-commit/alibaba/FederatedScope) | [FederatedScope: A Flexible Federated Learning Platform for Heterogeneity](https://arxiv.org/abs/2204.05011) | [Alibaba DAMO Academy](https://damo.alibaba.com/labs/data-analytics-and-intelligence) | :white_check_mark::white_check_mark: |                                      | [Doc](https://federatedscope.io/refs/index) <br />[Page](https://federatedscope.io/) |
| [LEAF](https://github.com/TalwalkarLab/leaf)<br />[![Stars](https://img.shields.io/github/stars/TalwalkarLab/leaf.svg?color=blue)](https://github.com/TalwalkarLab/leaf/stargazers)<br />![](https://img.shields.io/github/last-commit/TalwalkarLab/leaf) | [LEAF: A Benchmark for Federated Settings](https://arxiv.org/abs/1812.01097.pdf) | [CMU](https://leaf.cmu.edu/)                                 |                                      |                                      |                                                              |
| [Rosetta](https://github.com/LatticeX-Foundation/Rosetta)<br />[![Stars](https://img.shields.io/github/stars/LatticeX-Foundation/Rosetta.svg?color=blue)](https://github.com/LatticeX-Foundation/Rosetta/stargazers)<br />![](https://img.shields.io/github/last-commit/LatticeX-Foundation/Rosetta) |                                                              | [matrixelements](https://www.matrixelements.com/product/rosetta) |                                      |                                      | [Doc](https://github.com/LatticeX-Foundation/Rosetta/blob/master/doc/DEPLOYMENT.md) <br />[Page](https://github.com/LatticeX-Foundation/Rosetta) |
| [PaddleFL](https://github.com/PaddlePaddle/PaddleFL)<br />[![Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleFL.svg?color=blue)](https://github.com/PaddlePaddle/PaddleFL/stargazers)<br />![](https://img.shields.io/github/last-commit/PaddlePaddle/PaddleFL) |                                                              | Baidu                                                        |                                      |                                      | [Doc](https://paddlefl.readthedocs.io/en/latest/index.html)  |
| [OpenFL](https://github.com/intel/openfl)<br />[![Stars](https://img.shields.io/github/stars/intel/openfl.svg?color=blue)](https://github.com/intel/openfl/stargazers)<br />![](https://img.shields.io/github/last-commit/intel/openfl) | [OpenFL: An open-source framework for Federated Learning](https://arxiv.org/abs/2105.06413) | [Intel](https://github.com/intel)                            |                                      |                                      | [Doc](https://openfl.readthedocs.io/en/latest/install.html)  |
| [IBM Federated Learning](https://github.com/IBM/federated-learning-lib)<br />[![Stars](https://img.shields.io/github/stars/IBM/federated-learning-lib.svg?color=blue)](https://github.com/IBM/federated-learning-lib/stargazers)<br />![](https://img.shields.io/github/last-commit/IBM/federated-learning-lib) | [IBM Federated Learning: an Enterprise Framework White Paper](https://arxiv.org/abs/2007.10987.pdf) | [IBM](https://github.com/IBM)                                |                                      |          :white_check_mark:          | [Papers](https://github.com/IBM/federated-learning-lib/blob/main/docs/papers.md) |
| [KubeFATE](https://github.com/FederatedAI/KubeFATE)<br />[![Stars](https://img.shields.io/github/stars/FederatedAI/KubeFATE.svg?color=blue)](https://github.com/FederatedAI/KubeFATE/stargazers)<br />![](https://img.shields.io/github/last-commit/FederatedAI/KubeFATE) |                                                              | [WeBank](https://fedai.org/)                                 |                                      |                                      | [Wiki](https://github.com/FederatedAI/KubeFATE/wiki/#faqs)   |
| [Privacy Meter](https://github.com/privacytrustlab/ml_privacy_meter)<br />[![Stars](https://img.shields.io/github/stars/privacytrustlab/ml_privacy_meter.svg?color=blue)](https://github.com/PaddlePaddle/privacytrustlab/ml_privacy_meter)<br />![](https://img.shields.io/github/last-commit/privacytrustlab/ml_privacy_meter) | [Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning](https://ieeexplore.ieee.org/document/8835245) | University of Massachusetts Amherst                          |                                      |                                      |                                                              |
| [Fedlab](https://github.com/SMILELab-FL/FedLab)<br />[![Stars](https://img.shields.io/github/stars/SMILELab-FL/FedLab.svg?color=blue)](https://github.com/SMILELab-FL/FedLab/stargazers)<br />![](https://img.shields.io/github/last-commit/SMILELab-FL/FedLab) | [FedLab: A Flexible Federated Learning Framework](https://arxiv.org/abs/2107.11621) | [SMILELab](https://github.com/SMILELab-FL/)                  |                                      |                                      | [Doc](https://fedlab.readthedocs.io/en/master/)<br />[Doc(zh)](https://fedlab.readthedocs.io/zh_CN/latest/) <br />[Page](https://github.com/SMILELab-FL/FedLab-benchmarks) |
| [Differentially Private Federated Learning: A Client-level Perspective](https://github.com/SAP-samples/machine-learning-diff-private-federated-learning) <br />[![Stars](https://img.shields.io/github/stars/SAP-samples/machine-learning-diff-private-federated-learning.svg?color=blue)](https://github.comSAP-samples/machine-learning-diff-private-federated-learning/stargazers)<br />![](https://img.shields.io/github/last-commit/SAP-samples/machine-learning-diff-private-federated-learning) | [Differentially Private Federated Learning: A Client Level Perspective](https://arxiv.org/abs/1712.07557) | [SAP-samples](https://github.com/SAP-samples)                |                                      |                                      |                                                              |
| [NVFlare](https://github.com/NVIDIA/NVFlare)<br />[![Stars](https://img.shields.io/github/stars/NVIDIA/NVFlare.svg?color=blue)](https://github.com/NVIDIA/NVFlare/stargazers)<br />![](https://img.shields.io/github/last-commit/NVIDIA/NVFlare) |                                                              | [NVIDIA](https://github.com/NVIDIA)                          |                                      |                                      | [Doc](https://nvflare.readthedocs.io/en/2.1.1/)              |
| [easyFL](https://github.com/WwZzz/easyFL)<br />[![Stars](https://img.shields.io/github/stars/WwZzz/easyFL.svg?color=blue)](https://github.com/WwZzz/easyFL/stargazers)<br />![](https://img.shields.io/github/last-commit/WwZzz/easyFL) | [Federated Learning with Fair Averaging](https://www.ijcai.org/proceedings/2021/223) | XMU                                                          |                                      |                                      |                                                              |
| [FedScale](https://github.com/SymbioticLab/FedScale)<br />[![Stars](https://img.shields.io/github/stars/SymbioticLab/FedScale.svg?color=blue)](https://github.com/SymbioticLab/FedScale/stargazers)<br />![](https://img.shields.io/github/last-commit/SymbioticLab/FedScale) | [FedScale: Benchmarking Model and System Performance of Federated Learning at Scale](https://arxiv.org/abs/2105.11367.pdf) | [SymbioticLab(U-M)](https://symbioticlab.org/)               |                                      |                                      |                                                              |
| [FedNLP](https://github.com/FedML-AI/FedNLP)<br />[![Stars](https://img.shields.io/github/stars/FedML-AI/FedNLP.svg?color=blue)](https://github.com/FedML-AI/FedNLP/stargazers)<br />![](https://img.shields.io/github/last-commit/FedML-AI/FedNLP) | [FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks](https://arxiv.org/abs/2104.08815) | [FedML](https://fedml.ai/)                                   |                                      |                                      |                                                              |
| [Backdoors 101](https://github.com/ebagdasa/backdoors101)<br />[![Stars](https://img.shields.io/github/stars/ebagdasa/backdoors101.svg?color=blue)](https://github.com/ebagdasa/backdoors101/stargazers)<br />![](https://img.shields.io/github/last-commit/ebagdasa/backdoors101) | [Blind Backdoors in Deep Learning Models](https://arxiv.org/abs/2005.03823) | Cornell Tech                                                 |                                      |                                      |                                                              |
| [FedJAX](https://github.com/google/fedjax)<br />[![Stars](https://img.shields.io/github/stars/google/fedjax.svg?color=blue)](https://github.com/google/fedjax/stargazers)<br />![](https://img.shields.io/github/last-commit/google/fedjax) | [FEDJAX: Federated learning simulation with JAX](https://arxiv.org/abs/2108.02117.pdf) | [Google](https://ai.googleblog.com/2021/10/fedjax-federated-learning-simulation.html) |                                      |                                      |                                                              |
| [PFL-Non-IID](https://github.com/TsingZ0/PFL-Non-IID)<br />[![Stars](https://img.shields.io/github/stars/TsingZ0/PFL-Non-IID.svg?color=blue)](https://github.com/TsingZ0/PFL-Non-IID/stargazers)<br />![](https://img.shields.io/github/last-commit/TsingZ0/PFL-Non-IID) |                                                              | SJTU                                                         |                                      |                                      |                                                              |
| [Xaynet](https://github.com/xaynetwork/xaynet)<br />[![Stars](https://img.shields.io/github/stars/xaynetwork/xaynet.svg?color=blue)](https://github.com/xaynetwork/xaynet/stargazers)<br />![](https://img.shields.io/github/last-commit/xaynetwork/xaynet) |                                                              | [XayNet](https://www.xayn.com/)                              |                                      |                                      | [Page](https://www.xaynet.dev/) [Doc](https://docs.rs/xaynet) [Whitepaper](https://uploads-ssl.webflow.com/5f0c5c0bb18a279f0a62919e/5f157004da6585f299fa542b_XayNet%20Whitepaper%202.1.pdf) [Legal Review](https://uploads-ssl.webflow.com/5f0c5c0bb18a279f0a62919e/5fcfa8e3389ecc84a9309513_XAIN%20Legal%20Review%202020%20v1.pdf) |
| [SyferText](https://github.com/OpenMined/SyferText)<br />[![Stars](https://img.shields.io/github/stars/OpenMined/SyferText.svg?color=blue)](https://github.com/OpenMined/SyferText/stargazers)<br />![](https://img.shields.io/github/last-commit/OpenMined/SyferText) |                                                              | [OpenMined](https://www.openmined.org/)                      |                                      |                                      |                                                              |
| [Galaxy Federated Learning](https://github.com/GalaxyLearning/GFL)<br />[![Stars](https://img.shields.io/github/stars/GalaxyLearning/GFL.svg?color=blue)](https://github.com/GalaxyLearning/GFL/stargazers)<br />![](https://img.shields.io/github/last-commit/GalaxyLearning/GFL) | [GFL: A Decentralized Federated Learning Framework Based On Blockchain](https://arxiv.org/abs/2010.10996.pdf) | ZJU                                                          |                                      |                                      | [Doc](http://galaxylearning.github.io/)                      |
| [FedGraphNN](https://github.com/FedML-AI/FedGraphNN)<br />[![Stars](https://img.shields.io/github/stars/FedML-AI/FedGraphNN.svg?color=blue)](https://github.com/FedML-AI/FedGraphNN/stargazers)<br />![](https://img.shields.io/github/last-commit/FedML-AI/FedGraphNN) | [FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks](https://arxiv.org/abs/2104.07145) | [FedML](https://fedml.ai/)                                   | :white_check_mark::white_check_mark: |                                      |                                                              |
| [plato](https://github.com/TL-System/plato)<br />[![Stars](https://img.shields.io/github/stars/TL-System/plato.svg?color=blue)](https://github.com/TL-System/plato/stargazers)<br />![](https://img.shields.io/github/last-commit/TL-System/plato) |                                                              | UofT                                                         |                                      |                                      |                                                              |
| [NIID-Bench](https://github.com/Xtra-Computing/NIID-Bench)<br />[![Stars](https://img.shields.io/github/stars/Xtra-Computing/NIID-Bench.svg?color=blue)](https://github.com/Xtra-Computing/NIID-Bench/stargazers)<br />![](https://img.shields.io/github/last-commit/Xtra-Computing/NIID-Bench) | [Federated Learning on Non-IID Data Silos: An Experimental Study](https://arxiv.org/abs/2102.02079.pdf) | [Xtra Computing Group](https://github.com/Xtra-Computing)    |                                      |                                      |                                                              |
| [SWARM LEARNING](https://github.com/HewlettPackard/swarm-learning) <br />[![Stars](https://img.shields.io/github/stars/HewlettPackard/swarm-learning.svg?color=blue)](https://github.com/HewlettPackard/swarm-learning/stargazers)<br />![](https://img.shields.io/github/last-commit/HewlettPackard/swarm-learning) | [Swarm Learning for decentralized and confidential clinical machine learning](https://www.nature.com/articles/s41586-021-03583-3) |                                                              |                                      |                                      | [Video](https://github.com/HewlettPackard/swarm-learning/blob/master/docs/videos.md) |
| [substra](https://github.com/Substra/substra) <br />[![Stars](https://img.shields.io/github/stars/Substra/substra.svg?color=blue)](https://github.com/Substra/substra/stargazers)<br />![](https://img.shields.io/github/last-commit/Substra/substra) |                                                              | [Substra](https://github.com/Substra)                        |                                      |                                      | [Doc](https://doc.substra.ai/index.html)                     |
| [PhotoLabeller](https://github.com/mccorby/PhotoLabeller)<br />[![Stars](https://img.shields.io/github/stars/mccorby/PhotoLabeller.svg?color=blue)](https://github.com/mccorby/PhotoLabeller/stargazers)<br />![](https://img.shields.io/github/last-commit/mccorby/PhotoLabeller) |                                                              |                                                              |                                      |                                      | [Blog](https://proandroiddev.com/federated-learning-e79e054c33ef) |
| [FLSim](https://github.com/facebookresearch/FLSim)<br />[![Stars](https://img.shields.io/github/stars/facebookresearch/FLSim.svg?color=blue)](https://github.com/facebookresearch/FLSim/stargazers)<br />![](https://img.shields.io/github/last-commit/facebookresearch/FLSim) |                                                              | [facebook research ](https://github.com/facebookresearch)    |                                      |                                      |                                                              |
| [Primihub](https://github.com/primihub/primihub)<br />[![Stars](https://img.shields.io/github/stars/primihub/primihub.svg?color=blue)](https://github.com/primihub/primihub/stargazers)<br />![](https://img.shields.io/github/last-commit/primihub/primihub) |                                                              | [primihub](https://github.com/primihub)                      |                                      |                                      | [Doc]()                                                      |
| [PyVertical ](https://github.com/OpenMined/PyVertical)<br />[![Stars](https://img.shields.io/github/stars/OpenMined/PyVertical.svg?color=blue)](https://github.com/OpenMined/PyVertical/stargazers)<br />![](https://img.shields.io/github/last-commit/OpenMined/PyVertical) | [PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN](https://arxiv.org/abs/2104.00489.pdf) | [OpenMined](https://www.openmined.org/)                      |                                      |                                      |                                                              |
| [FedTorch](https://github.com/MLOPTPSU/FedTorch) <br />[![Stars](https://img.shields.io/github/stars/MLOPTPSU/FedTorch.svg?color=blue)](https://github.com/MLOPTPSU/FedTorch/stargazers)<br />![](https://img.shields.io/github/last-commit/MLOPTPSU/FedTorch) | [Distributionally Robust Federated Averaging](https://papers.nips.cc/paper/2020/file/ac450d10e166657ec8f93a1b65ca1b14-Paper.pdf) | Penn State                                                   |                                      |                                      |                                                              |
| [FATE-Serving](https://github.com/FederatedAI/FATE-Serving) <br />[![Stars](https://img.shields.io/github/stars/FederatedAI/FATE-Serving.svg?color=blue)](https://github.com/FederatedAI/FATE-Serving/stargazers)<br />![](https://img.shields.io/github/last-commit/FederatedAI/FATE-Serving) |                                                              | [WeBank](https://fedai.org/)                                 |                                      |                                      | [Doc](https://fate-serving.readthedocs.io/en/develop/)       |
| [PriMIA](https://github.com/gkaissis/PriMIA)<br />[![Stars](https://img.shields.io/github/stars/gkaissis/PriMIA.svg?color=blue)](https://github.com/gkaissis/PriMIA/stargazers)<br />![](https://img.shields.io/github/last-commit/gkaissis/PriMIA) | [End-to-end privacy preserving deep learning on multi-institutional medical imaging](https://www.nature.com/articles/s42256-021-00337-8) | [TUM](https://www.tum.de/en/); Imperial College London; [OpenMined](https://www.openmined.org) |                                      |                                      | [Doc](https://g-k.ai/PriMIA/)                                |
| [FLUTE](https://github.com/microsoft/msrflute)<br />[![Stars](https://img.shields.io/github/stars/microsoft/msrflute.svg?color=blue)](https://github.com/microsoft/msrflute/stargazers)<br />![](https://img.shields.io/github/last-commit/microsoft/msrflute) | [FLUTE: A Scalable, Extensible Framework for High-Performance Federated Learning Simulations](https://arxiv.org/abs/2203.13789) | microsoft                                                    |                                      |                                      | [Doc](https://microsoft.github.io/msrflute/)                 |
| [FLSim](https://github.com/iQua/flsim) <br />[![Stars](https://img.shields.io/github/stars/iQua/flsim.svg?color=blue)](https://github.com/iQua/flsim/stargazers)<br />![](https://img.shields.io/github/last-commit/iQua/flsim) | [Optimizing Federated Learning on Non-IID Data with Reinforcement Learning](https://ieeexplore.ieee.org/document/9155494/) | University of Toronto                                        |                                      |                                      |                                                              |
| [Breaching](https://github.com/JonasGeiping/breaching)<br />[![Stars](https://img.shields.io/github/stars/JonasGeiping/breaching.svg?color=blue)](https://github.com/JonasGeiping/breaching/stargazers)<br />![](https://img.shields.io/github/last-commit/JonasGeiping/breaching) | A Framework for Attacks against Privacy in Federated Learning ([papers](https://github.com/JonasGeiping/breaching)) |                                                              |                                      |                                      |                                                              |
| [9nfl](https://github.com/jd-9n/9nfl)<br />[![Stars](https://img.shields.io/github/stars/jd-9n/9nfl.svg?color=blue)](https://github.com/jd-9n/9nfl/stargazers)<br />![](https://img.shields.io/github/last-commit/jd-9n/9nfl) |                                                              | JD                                                           |                                      |                                      |                                                              |
| [EasyFL](https://github.com/EasyFL-AI/EasyFL)<br />[![Stars](https://img.shields.io/github/stars/EasyFL-AI/EasyFL.svg?color=blue)](https://github.com/EasyFL-AI/EasyFL/stargazers)<br />![](https://img.shields.io/github/last-commit/EasyFL-AI/EasyFL) | [EasyFL: A Low-code Federated Learning Platform For Dummies](https://ieeexplore.ieee.org/abstract/document/9684558) | NTU                                                          |                                      |                                      |                                                              |
| [FedLearn](https://github.com/fedlearnAI/fedlearn-algo)<br />[![Stars](https://img.shields.io/github/stars/fedlearnAI/fedlearn-algo.svg?color=blue)](https://github.com/fedlearnAI/fedlearn-algo/stargazers)<br />![](https://img.shields.io/github/last-commit/fedlearnAI/fedlearn-algo) | [Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform](https://arxiv.org/abs/2107.04129) | JD                                                           |                                      |                                      |                                                              |
| [FEDn](https://github.com/scaleoutsystems/fedn)<br />[![Stars](https://img.shields.io/github/stars/scaleoutsystems/fedn.svg?color=blue)](https://github.com/scaleoutsystems/fedn/stargazers)<br />![](https://img.shields.io/github/last-commit/scaleoutsystems/fedn) | [Scalable federated machine learning with FEDn](https://arxiv.org/abs/2103.00148) | [scaleoutsystems](http://www.scaleoutsystems.com)            |                                      |                                      | [Doc](https://scaleoutsystems.github.io/fedn/)               |
| [FedCV](https://github.com/FedML-AI/FedCV)<br />[![Stars](https://img.shields.io/github/stars/FedML-AI/FedCV.svg?color=blue)](https://github.com/FedML-AI/FedCV/stargazers)<br />![](https://img.shields.io/github/last-commit/FedML-AI/FedCV) | [FedCV: A Federated Learning Framework for Diverse Computer Vision Tasks](https://arxiv.org/abs/2111.11066) | FedML                                                        |                                      |                                      |                                                              |
| [FedTree](https://github.com/Xtra-Computing/FedTree)<br />[![Stars](https://img.shields.io/github/stars/Xtra-Computing/FedTree.svg?color=blue)](https://github.com/Xtra-Computing/FedTree/stargazers)<br />![](https://img.shields.io/github/last-commit/Xtra-Computing/FedTree) |                                                              | [Xtra Computing Group](https://github.com/Xtra-Computing)    |                                      | :white_check_mark::white_check_mark: | [Doc](https://fedtree.readthedocs.io/en/latest/index.html)   |
| [MPLC](https://github.com/LabeliaLabs/distributed-learning-contributivity)<br />[![Stars](https://img.shields.io/github/stars/LabeliaLabs/distributed-learning-contributivity.svg?color=blue)](https://github.com/LabeliaLabs/distributed-learning-contributivity/stargazers)<br />![](https://img.shields.io/github/last-commit/LabeliaLabs/distributed-learning-contributivity) |                                                              | [LabeliaLabs](https://github.com/LabeliaLabs)                |                                      |                                      | [Page](https://www.labelia.org)                              |
| [OpenHealth](https://github.com/QibingLee/OpenHealth) <br />[![Stars](https://img.shields.io/github/stars/QibingLee/OpenHealth.svg?color=blue)](https://github.com/QibingLee/OpenHealth/stargazers)<br />![](https://img.shields.io/github/last-commit/QibingLee/OpenHealth) |                                                              | ZJU                                                          |                                      |                                      |                                                              |
| [UCADI](https://github.com/HUST-EIC-AI-LAB/UCADI) <br />[![Stars](https://img.shields.io/github/stars/HUST-EIC-AI-LAB/UCADI.svg?color=blue)](https://github.com/HUST-EIC-AI-LAB/UCADI/stargazers)<br />![](https://img.shields.io/github/last-commit/HUST-EIC-AI-LAB/UCADI) | [Advancing COVID-19 diagnosis with privacy-preserving collaboration in artificial intelligence](https://www.nature.com/articles/s42256-021-00421-z) | Huazhong University of Science and Technology                |                                      |                                      |                                                              |
| [OpenFed](https://github.com/FederalLab/OpenFed/)<br />[![Stars](https://img.shields.io/github/stars/FederalLab/OpenFed.svg?color=blue)](https://github.com/FederalLab/OpenFed/stargazers)<br />![](https://img.shields.io/github/last-commit/FederalLab/OpenFed) | [OpenFed: A Comprehensive and Versatile Open-Source Federated Learning Framework](https://arxiv.org/abs/2109.07852) |                                                              |                                      |                                      | [Doc](https://openfed.readthedocs.io/README.html)            |
| [FlexCFL](https://github.com/morningD/FlexCFL)<br />[![Stars](https://img.shields.io/github/stars/morningD/FlexCFL.svg?color=blue)](https://github.com/morningD/FlexCFL/stargazers)<br />![](https://img.shields.io/github/last-commit/morningD/FlexCFL) | [Flexible Clustered Federated Learning for Client-Level Data Distribution Shift](https://arxiv.org/abs/2108.09749) | Chongqing University                                         |                                      |                                      |                                                              |
| [FedGroup](https://github.com/morningD/GrouProx)<br />[![Stars](https://img.shields.io/github/stars/morningD/GrouProx.svg?color=blue)](https://github.com/morningD/GrouProx/stargazers)<br />![](https://img.shields.io/github/last-commit/morningD/GrouProx) | [FedGroup: Efficient Clustered Federated Learning via Decomposed Data-Driven Measure](https://arxiv.org/abs/2010.06870) | Chongqing University                                         |                                      |                                      |                                                              |
| [FedEval](https://github.com/Di-Chai/FedEval)<br />[![Stars](https://img.shields.io/github/stars/Di-Chai/FedEval.svg?color=blue)](https://github.com/Di-Chai/FedEval/stargazers)<br />![](https://img.shields.io/github/last-commit/Di-Chai/FedEval) | [FedEval: A Benchmark System with a Comprehensive Evaluation Model for Federated Learning](https://arxiv.org/abs/2011.09655) | HKU                                                          |                                      |                                      | [Doc](https://di-chai.github.io/FedEval/)                    |
| [APPFL](https://github.com/APPFL/APPFL)<br />[![Stars](https://img.shields.io/github/stars/APPFL/APPFL.svg?color=blue)](https://github.com/APPFL/APPFL/stargazers)<br />![](https://img.shields.io/github/last-commit/APPFL/APPFL) |                                                              |                                                              |                                      |                                      | [Doc](https://appfl.readthedocs.io/en/stable/)               |
| [Flame](https://github.com/cisco-open/flame)<br />[![Stars](https://img.shields.io/github/stars/cisco-open/flame.svg?color=blue)](https://github.com/cisco-open/flame/stargazers)<br />![](https://img.shields.io/github/last-commit/cisco-open/flame) |                                                              | Cisco                                                        |                                      |                                      |                                                              |
| [Federated-Learning-source](https://github.com/MTC-ETH/Federated-Learning-source) <br />[![Stars](https://img.shields.io/github/stars/MTC-ETH/Federated-Learning-source.svg?color=blue)](https://github.com/MTC-ETH/Federated-Learning-source/stargazers)<br />![](https://img.shields.io/github/last-commit/MTC-ETH/Federated-Learning-source) | [A Practical Federated Learning Framework for Small Number of Stakeholders](https://dl.acm.org/doi/10.1145/3437963.3441702) | ETH Zürich                                                   |                                      |                                      | [Doc](https://github.com/MTC-ETH/Federated-Learning-source/blob/master/dashboard/README.md) |
| [Clara](https://developer.nvidia.com/clara)                  |                                                              | NVIDIA                                                       |                                      |                                      |                                                              |



### Benchmark

- UniFed leaderboard

Here's a really great Benchmark for the federated learning open source framework :+1: [UniFed leaderboard](https://unifedbenchmark.github.io/leaderboard/index.html), which present both qualitative and quantitative evaluation results of existing popular open-sourced FL frameworks, from the perspectives of **functionality, usability, and system performance**. 

![workflow-design](https://unifedbenchmark.github.io/images/workflow-design.png)



![UniFed_framework_benchmark](./assets/UniFed_framework_benchmark.png)

For more results, please refer to [Framework Functionality Support](https://unifedbenchmark.github.io/leaderboard/index.html)





# Datasets

- [LEAF](https://leaf.cmu.edu/)
- [Federated AI Dataset](https://dataset.fedai.org/#/) 



# Tutorials

- [Federated Learning on MNIST using a CNN](https://colab.research.google.com/drive/1dRG3yNAlDar3tll4VOkmoU-aLslhUS8d), AI6101, 2020 ([Demo Video](https://www.youtube.com/watch?v=XKQi-CUqCsM))
- [Federated Learning: User Privacy, Data Security and Confidentiality in Machine Learning](https://aaai.org/Conferences/AAAI-19/aaai19tutorials/), AAAI-19, Honolulu, HI, USA



# Key Conferences/Workshops/Journals

This section partially refers to [The Federated Learning Portal](https://federated-learning.org/).

## Workshops

- [[AI Technology School 2022](https://aitechnologyschool.github.io/)] Trustable, Verifiable and Auditable Artificial Intelligence, Singapore
- [[FL-NeurIPS'22](http://federated-learning.org/fl-neurips-2022/)] International Workshop on Federated Learning: Recent Advances and New Challenges  in Conjunction with NeurIPS 2022 , New Orleans, LA, USA

- [[FL-IJCAI'22](http://federated-learning.org/fl-ijcai-2022/)] International Workshop on Trustworthy Federated Learning  in Conjunction with IJCAI 2022, Vienna, Austria
- [[FL-AAAI-22](http://federated-learning.org/fl-aaai-2022/)] International Workshop on Trustable, Verifiable and Auditable Federated Learning in Conjunction with AAAI 2022, Vancouver, BC, Canada (Virtual)
- [[FL-NeurIPS'21](https://neurips2021workshopfl.github.io/NFFL-2021/)] New Frontiers in Federated Learning:  Privacy, Fairness, Robustness, Personalization and Data Ownership, (Virtual)
- [[The Federated Learning Workshop, 2021](https://sites.google.com/view/federatedlearning-workshop/home)] , Paris, France (Hybrid)
- [[PDFL-EMNLP'21](https://pdfl.iais.fraunhofer.de/)] Workshop on Parallel, Distributed, and Federated Learning, Bilbao, Spain (Virtual)
- [[FTL-IJCAI'21](https://federated-learning.org/fl-ijcai-2021/)] International Workshop on Federated and Transfer Learning for Data Sparsity and Confidentiality in Conjunction with IJCAI 2021, Montreal, QB, Canada (Virtual)

- [[DeepIPR-IJCAI'21](http://federated-learning.org/DeepIPR-IJCAI-2021/)] Toward Intellectual Property Protection on Deep Learning as a Services, Montreal, QB, Canada (Virtual)
- [[FL-ICML'21](http://federated-learning.org/fl-icml-2021/)]  International Workshop on Federated Learning for User Privacy and Data Confidentiality, (Virtual)
- [[RSEML-AAAI-21](http://federated-learning.org/rseml2021)]  Towards Robust, Secure and Efficient Machine Learning, (Virtual)
- [[NeurIPS-SpicyFL'20](http://icfl.cc/SpicyFL/2020)] Workshop on Scalability, Privacy, and Security in Federated Learning, Vancouver, BC, Canada (Virtual)
- [[FL-IJCAI'20](http://fl-ijcai20.federated-learning.org/)] International Workshop on Federated Learning for User Privacy and Data Confidentiality, Yokohama, Japan (Virtual)
- [[FL-ICML'20](http://federated-learning.org/fl-icml-2020/)] International Workshop on Federated Learning for User Privacy and Data Confidentiality, Vienna, Austria (Virtual)
- [[FL-IBM'20](https://federated-learning.bitbucket.io/ibm2020/)] Workshop on Federated Learning and Analytics, New York, NY, USA
- [[FL-NeurIPS'19](http://federated-learning.org/fl-neurips-2019/)] Workshop on Federated Learning for Data Privacy and Confidentiality (in Conjunction with NeurIPS 2019), Vancouver, BC, Canada
- [[FL-IJCAI'19](http://federated-learning.org/fl-ijcai-2019/)] International Workshop on Federated Learning for User Privacy and Data Confidentiality 
  in Conjunction with IJCAI 2019, Macau
- [[FL-Google'19](https://sites.google.com/view/federated-learning-2019/home)] Workshop on Federated Learning and Analytics, Seattle, WA, USA



## Journal Special Issues

- [Special Issue on Trustable, Verifiable, and Auditable Federated Learning](https://www.computer.org/digital-library/journals/bd/call-for-papers-special-issue-on-trustable-verifiable-and-auditable-federated-learning), *IEEE Transactions on Big Data (TBD)*, 2022.
- [Special Issue on Federated Learning: Algorithms, Systems, and Applications](https://dl.acm.org/pb-assets/static_journal_pages/tist/cfps/tist-si-cfp-12-2020-federated-learning-extended2-1617161513293.pdf), *ACM Transactions on Intelligent Systems and Technology (TIST)*, 2021.
- [Special Issue on Federated Machine Learning](https://www.computer.org/digital-library/magazines/ex/call-for-papers-federated-machine-learning), *IEEE Intelligent Systems (IS)*, 2019.



## Conference Special Tracks

- "Federated Learning" included as a new keyword in [IJCAI'20](https://ijcai20.org/), Yokohama, Japan 
- [Special Track on Federated Machine Learning](http://federated-learning.org/fl-ieeebigdata-2019/), *IEEE BigData'19*, Los Angeles, CA, USA



## How to contact us

**More items will be added to the repository**. Please feel free to suggest other key resources by opening an [issue](https://github.com/youngfish42/Awesome-Federated-Learning-on-Graph-and-Tabular-Data/issues) report, submitting a pull request, or dropping me an email @ ([im.young@foxmail.com](mailto:im.young@foxmail.com)). Enjoy reading!



## Acknowledgments

Many thanks :heart: to the other awesome list:

- Federated Learning

  - [Awesome-Federated-Learning-on-Graph-and-GNN-papers](https://github.com/huweibo/Awesome-Federated-Learning-on-Graph-and-GNN-papers) 
  - [Awesome-GNN-Research](https://github.com/XunKaiLi/Awesome-GNN-Research)
  - [Awesome-Federated-Machine-Learning](https://github.com/innovation-cat/Awesome-Federated-Machine-Learning)
  - [Awesome-Federated-Learning](https://github.com/chaoyanghe/Awesome-Federated-Learning)
  - [awesome-federated-learning](https://github.com/weimingwill/awesome-federated-learning)
  - [Federated-Learning](https://github.com/lokinko/Federated-Learning)
  - [FLsystem-paper](https://github.com/AmberLJC/FLsystem-paper)
  - [Federated Learning Framework Benchmark (UniFed)](https://github.com/AI-secure/FLBenchmark-toolkit)
- Other fields

  - [anomaly-detection-resources](https://github.com/yzhao062/anomaly-detection-resources)
  - [awesome-image-registration](https://github.com/Awesome-Image-Registration-Organization/awesome-image-registration)







[![map](https://rf.revolvermaps.com/h/m/a/0/ff0000/128/35/5zw06d5f905.png)](https://www.revolvermaps.com/livestats/5zw06d5f905/)






[^CE]:CE propose the concept of benefit graph which describes how each client can benefit from collaborating with other clients and advance a Pareto optimization approach to identify the optimal collaborators. CE提出了利益图的概念，描述了每个客户如何从与其他客户的合作中获益，并提出了帕累托优化方法来确定最佳合作者。
[^Comm-FedBiO]:Comm-FedBiO propose a learning-based reweighting approach to mitigate the effect of noisy labels in FL. Comm-FedBiO提出了一种基于学习的重加权方法，以减轻FL中噪声标签的影响。
[^FLDetector]:FLDetector detects malicious clients via checking their model-updates consistency to defend against model poisoning attacks with a large number of malicious clients. FLDetector 通过检查其模型更新的一致性来检测恶意客户，以防御大量恶意客户的模型中毒攻击。
[^FedSVD]:FedSVD, a practical lossless federated SVD method over billion-scale data, which can simultaneously achieve lossless accuracy and high efficiency. FedSVD，是一种实用的亿级数据上的无损联邦SVD方法，可以同时实现无损精度和高效率。
[^FedWalk]:FedWalk, a random-walk-based unsupervised node embedding algorithm that operates in such a node-level visibility graph with raw graph information remaining locally. FedWalk，一个基于随机行走的无监督节点嵌入算法，在这样一个节点级可见度图中操作，原始图信息保留在本地。
[^FederatedScope-GNN]:FederatedScope-GNN present an easy-to-use FGL (federated graph learning) package. FederatedScope-GNN提出了一个易于使用的FGL（联邦图学习）软件包。
[^Fed-LTD]:Federated Learning-to-Dispatch (Fed-LTD), a framework that allows effective order dispatching by sharing both dispatching models and decisions while providing privacy protection of raw data and high efficiency. 解决跨平台叫车问题，即多平台在不共享数据的情况下协同进行订单分配。
[^InclusiveFL]:InclusiveFL is to assign models of different sizes to clients with different computing capabilities, bigger models for powerful clients and smaller ones for weak clients. InclusiveFL 将不同大小的模型分配给具有不同计算能力的客户，较大的模型用于强大的客户，较小的用于弱小的客户。
[^FedAttack]:FedAttack a simple yet effective and covert poisoning attack method on federated recommendation, core idea is using globally hardest samples to subvert model training. FedAttack是一种对联邦推荐的简单而有效的隐蔽中毒攻击方法，核心思想是利用全局最难的样本来颠覆模型训练。
[^ATPFL]:ATPFL helps users federate multi-source trajectory datasets to automatically design and train a powerful TP model. ATPFL帮助用户联邦多源轨迹数据集，自动设计和训练强大的TP轨迹预测模型。
[^ViT-FL]:ViT-FL demonstrate that self-attention-based architectures (e.g., Transformers) are more robust to distribution shifts and hence improve federated learning over heterogeneous data. ViT-FL证明了基于自注意力机制架构（如 Transformers）对分布的转变更加稳健，从而改善了异构数据的联邦学习。
[^FedCorr]:FedCorr, a general multi-stage framework to tackle heterogeneous label noise in FL, without making any assumptions on the noise models of local clients, while still maintaining client data privacy. FedCorr 一个通用的多阶段框架来处理FL中的异质标签噪声，不对本地客户的噪声模型做任何假设，同时仍然保持客户数据的隐私。
[^pFedLA]:A novel pFL training framework dubbed Layer-wised Personalized Federated learning (pFedLA) that can discern the importance of each layer from different clients, and thus is able to optimize the personalized model aggregation for clients with heterogeneous data. "层级个性化联邦学习"（pFedLA），它可以从不同的客户那里分辨出每一层的重要性，从而能够为拥有异质数据的客户优化个性化的模型聚合。
[^FedAlign]:FedAlign rethinks solutions to data heterogeneity in FL with a focus on local learning generality rather than proximal restriction. 我们重新思考FL中数据异质性的解决方案，重点是本地学习的通用性(generality)而不是近似限制。
[^PANs]:Position-Aware Neurons (PANs) , fusing position-related values (i.e., position encodings) into neuron outputs, making parameters across clients pre-aligned and facilitating coordinate-based parameter averaging. 位置感知神经元（PANs）将位置相关的值（即位置编码）融合到神经元输出中，使各客户的参数预先对齐，并促进基于坐标的参数平均化。
[^GGL]:Generative Gradient Leakage (GGL) validate that the private training data can still be leaked under certain defense settings with a new type of leakage. 生成梯度泄漏（GGL）验证了在某些防御设置下，私人训练数据仍可被泄漏。
[^FedDC]:FedDC propose a novel federated learning algorithm with local drift decoupling and correction. FedDC 一种带有本地漂移解耦和校正的新型联邦学习算法。
[^RSCFed]:Federated semi-supervised learning (FSSL) aims to derive a global model by training fully-labeled and fully-unlabeled clients or training partially labeled clients. RSCFed presents a Random Sampling Consensus Federated learning, by considering the uneven reliability among models from fully-labeled clients, fully-unlabeled clients or partially labeled clients. 联邦半监督学习（FSSL）旨在通过训练有监督和无监督的客户或半监督的客户来得出一个全局模型。 随机抽样共识联邦学习，即RSCFed，考虑来自有监督的客户、无监督的客户或半监督的客户的模型之间不均匀的可靠性。
[^FCCL]:FCCL (Federated Cross-Correlation and Continual Learning) For heterogeneity problem, FCCL leverages unlabeled public data for communication and construct cross-correlation matrix to learn a generalizable representation under domain shift. Meanwhile, for catastrophic forgetting, FCCL utilizes knowledge distillation in local updating, providing inter and intra domain information without leaking privacy.  FCCL（联邦交叉相关和持续学习）对于异质性问题，FCCL利用未标记的公共数据进行交流，并构建交叉相关矩阵来学习领域转移下的可泛化表示。同时，对于灾难性遗忘，FCCL利用局部更新中的知识提炼，在不泄露隐私的情况下提供域间和域内信息。
[^GLFC]:Global-Local Forgetting Compensation (GLFC) model, to learn a global class incremental model for alleviating the catastrophic forgetting from both local and global perspectives. 全局-局部遗忘补偿（GLFC）模型，从局部和全局的角度学习一个全局类增量模型来缓解灾难性的遗忘问题。
[^DP-FedAvg+BLUR+LUS]:DP-FedAvg+BLUR+LUS study the cause of model performance degradation in federated learning under user-level DP guarantee and propose two techniques, Bounded Local Update Regularization and Local Update Sparsification, to increase model quality without sacrificing privacy. DP-FedAvg+BLUR+LUS 研究了在用户级DP保证下联邦学习中模型性能下降的原因,提出了两种技术，即有界局部更新正则化和局部更新稀疏化，以提高模型质量而不牺牲隐私。
[^RHFL]:RHFL (Robust Heterogeneous Federated Learning) simultaneously handles the label noise and performs federated learning in a single framework. RHFL（稳健模型异构联邦学习），它同时处理标签噪声并在一个框架内执行联邦学习。
[^ResSFL]:ResSFL, a Split Federated Learning Framework that is designed to be MI-resistant during training. ResSFL一个分割学习的联邦学习框架，它被设计成在训练期间可以抵抗MI模型逆向攻击。 Model Inversion (MI) attack 模型逆向攻击 。
[^FedCor]:FedCor, an FL framework built on a correlation-based client selection strategy, to boost the convergence rate of FL. FedCor 一个建立在基于相关性的客户选择策略上的FL框架，以提高FL的收敛率。
[^FedFTG]:FedFTG, a data-free knowledge distillation method to fine-tune the global model in the server, which relieves the issue of direct model aggregation. FedFTG, 一种无数据的知识蒸馏方法来微调服务器中的全局模型，它缓解了直接模型聚合的问题。
[^CD2-pFed]:CD2-pFed, a novel Cyclic Distillation-guided Channel Decoupling framework, to personalize the global model in FL, under various settings of data heterogeneity. CD2-pFed，一个新的循环蒸馏引导的通道解耦框架，在各种数据异质性的设置下，在FL中实现全局模型的个性化。
[^FedSM]:FedSM propose a novel training framework to avoid the client drift issue and successfully close the generalization gap compared with the centralized training for medical image segmentation tasks for the first time. 新的训练框架FedSM，以避免客户端漂移问题，并首次成功地缩小了与集中式训练相比在医学图像分割任务中的泛化差距。
[^FedReID]:FedReID implement federated learning to person re-identification and optimize its performance affected by statistical heterogeneity in the real-world scenario. FedReID 实现了对行人重识别任务的联邦学习，并优化了其在真实世界场景中受统计异质性影响的性能。
[^InvisibleFL]:InvisibleFL propose a privacy-preserving solution that avoids multimedia privacy leakages in federated learning. InvisibleFL 提出了一个保护隐私的解决方案，以避免联邦学习中的多媒体隐私泄漏。
[^FedUReID]:FedUReID, a federated unsupervised person ReID system to learn person ReID models without any labels while preserving privacy. FedUReID，一个联邦的无监督人物识别系统，在没有任何标签的情况下学习人物识别模型，同时保护隐私。
[^FedVC+FedIR]:Introduce two new large-scale datasets for species and landmark classification, with realistic per-user data splits that simulate real-world edge learning scenarios. We also develop two new algorithms (FedVC, FedIR) that intelligently resample and reweight over the client pool, bringing large improvements in accuracy and stability in training. 为物种和地标分类引入了两个新的大规模数据集，每个用户的现实数据分割模拟了真实世界的边缘学习场景。我们还开发了两种新的算法（FedVC、FedIR），在客户池上智能地重新取样和重新加权，在训练中带来了准确性和稳定性的巨大改进
[^FedU]:FedU a novel federated unsupervised learning framework. FedU 一个新颖的无监督联邦学习框架.
[^FedAD]:FedAD propose a new distillation-based FL frame-work that can preserve privacy by design, while also consuming substantially less network communication resources when compared to the current methods. FedAD 一个新的基于蒸馏的FL框架，它可以通过设计来保护隐私，同时与目前的方法相比，消耗的网络通信资源也大大减少
[^FedUFO]:FedUFO a Unified Feature learning and Optimization objectives alignment method for non-IID FL. FedUFO 一种针对non IID FL的统一特征学习和优化目标对齐算法。
[^FL-MRCM]:FL-MRCM propose a federated learning (FL) based solution in which we take advantage of the MR data available at different institutions while preserving patients' privacy. FL-MRCM 一个基于联邦学习（FL）的解决方案，其中我们利用了不同机构的MR数据，同时保护了病人的隐私。
[^MOON]:MOON: model-contrastive federated learning. MOON is to utilize the similarity between model representations to correct the local training of individual parties, i.e., conducting contrastive learning in model-level. MOON 模型对比学习。MOON的关键思想是利用模型表征之间的相似性来修正各方的局部训练，即在模型层面进行对比学习。
[^FedDG-ELCFS]:FedDG-ELCFS A novel problem setting of federated domain generalization (FedDG), which aims to learn a federated model from multiple distributed source domains such that it can directly generalize to unseen target domains. Episodic Learning in Continuous Frequency Space (ELCFS), for this problem by enabling each client to exploit multi-source data distributions under the challenging constraint of data decentralization. FedDG-ELCFS 联邦域泛化（FedDG）旨在从多个分布式源域中学习一个联邦模型，使其能够直接泛化到未见过的目标域中。连续频率空间中的偶发学习（ELCFS），使每个客户能够在数据分散的挑战约束下利用多源数据分布。
[^Soteria]:Soteria propose a defense against model inversion attack in FL, learning to perturb data representation such that the quality of the reconstructed data is severely degraded, while FL performance is maintained. Soteria 一种防御FL中模型反转攻击的方法,关键思想是学习扰乱数据表示，使重建数据的质量严重下降，而FL性能保持不变。

[^LC-Fed]:LC-Fed propose a personalized federated framework with Local Calibration, to leverage the inter-site in-consistencies in both feature- and prediction- levels to boost the segmentation. LC-Fed提出了一个带有本地校准的个性化联邦学习框架，以利用特征和预测层面的站点间不一致来提高分割效果。
[^FedSAM]:Models trained in federated settings often suffer from degraded performances and fail at generalizing, especially when facing heterogeneous scenarios. FedSAM investigate such behavior through the lens of geometry of the loss and Hessian eigenspectrum, linking the model's lack of generalization capacity to the sharpness of the solution. 联邦学习环境下训练的模型经常会出现性能下降和泛化失败的情况，特别是在面对异质场景时。FedSAM 通过损失和Hessian特征谱的几何角度来研究这种行为，将模型缺乏泛化能力与解决方案的锐度联系起来

[^FGML]:FGML a comprehensive review of the literature in Federated Graph Machine Learning. FGML 对图联邦机器学习的文献进行了全面回顾的综述文章。

[^GAMF]:GAMF formulate the model fusion problem as a graph matching task, considering the second-order similarity of model weights instead of previous work merely formulating model fusion as a linear assignment problem. For the rising problem scale and multi-model consistency issues, GAMF propose an efficient graduated assignment-based model fusion method, iteratively updates the matchings in a consistency-maintaining manner. GAMF将模型融合问题表述为图形匹配任务，考虑了模型权重的二阶相似性，而不是之前的工作仅仅将模型融合表述为一个线性赋值问题。针对问题规模的扩大和多模型的一致性问题，GAMF提出了一种高效的基于分级赋值的模型融合方法，以保持一致性的方式迭代更新匹配结果。
[^SuPerFed]:SuPerFed, a personalized federated learning method that induces an explicit connection between the optima of the local and the federated model in weight space for boosting each other. SuPerFed，一种个性化联邦学习方法，该方法在本地模型和联邦模型的权重空间中诱导出一个明确的连接，以促进彼此的发展。
[^FedMSplit]:FedMSplit framework, which allows federated training over multimodal distributed data without assuming similar active sensors in all clients. The key idea is to employ a dynamic and multi-view graph structure to adaptively capture the correlations amongst multimodal client models. FedMSplit框架，该框架允许在多模态分布式数据上进行联邦训练，而不需要假设所有客户端都有类似的主动传感器。其关键思想是采用动态和多视图图结构来适应性地捕捉多模态客户模型之间的相关性。
[^Felicitas]:Felicitas is a distributed cross-device Federated Learning (FL) framework to solve the industrial difficulties of FL in large-scale device deployment scenarios. Felicitas是一个分布式的跨设备联邦学习（FL）框架，以解决FL在大规模设备部署场景中的工业困难。
[^PipAttack]:PipAttack present a systematic approach to backdooring federated recommender systems for targeted item promotion. The core tactic is to take advantage of the inherent popularity bias that commonly exists in data-driven recommenders. PipAttack 提出了一种系统化的方法，为联邦推荐系统提供后门，以实现目标项目的推广。其核心策略是利用数据驱动的推荐器中普遍存在的固有的流行偏见。
[^Fed2]:Fed2, a feature-aligned federated learning framework to resolve this issue by establishing a firm structure-feature alignment across the collaborative models. Fed2是一个特征对齐的联邦学习框架，通过在协作模型之间建立牢固的结构-特征对齐来解决这个问题。
[^FedRS]:FedRS focus on a special kind of non-iid scene, i.e., label distribution skew, where each client can only access a partial set of the whole class set. Considering top layers of neural networks are more task-specific, we advocate that the last classification layer is more vulnerable to the shift of label distribution. Hence, we in-depth study the classifier layer and point out that the standard softmax will encounter several problems caused by missing classes. As an alternative, we propose “Restricted Softmax" to limit the update of missing classes’ weights during the local procedure. FedRS专注于一种特殊的非iid场景，即标签分布倾斜，每个客户端只能访问整个类集的部分集合。考虑到神经网络的顶层更具有任务针对性，我们主张最后一个分类层更容易受到标签分布偏移的影响。因此，我们深入研究了分类器层，并指出标准的softmax会遇到由缺失类引起的一些问题。作为一个替代方案，提出了 "限制性Softmax"，以限制在本地程序中对缺失类的权重进行更新。
[^AsySQN]:To address the challenges of communication and computation resource utilization, we propose an asynchronous stochastic quasi-Newton (AsySQN) framework for Vertical federated learning(VFL), under which three algorithms, i.e. AsySQN-SGD, -SVRG and -SAGA, are proposed. The proposed AsySQN-type algorithms making descent steps scaled by approximate (without calculating the inverse Hessian matrix explicitly) Hessian information convergence much faster than SGD-based methods in practice and thus can dramatically reduce the number of communication rounds. Moreover, the adopted asynchronous computation can make better use of the computation resource. We theoretically prove the convergence rates of our proposed algorithms for strongly convex problems. 为了解决通信和计算资源利用的挑战，我们提出了一个异步随机准牛顿（AsySQN）的纵和联邦学习VFL框架，在这个框架下，我们提出了三种算法，即AsySQN-SGD、-SVRG和-SAGA。所提出的AsySQN型算法使下降步骤按近似（不明确计算逆Hessian矩阵）Hessian信息收敛的速度比基于SGD的方法在实践中快得多，因此可以极大地减少通信轮数。此外，采用异步计算可以更好地利用计算资源。我们从理论上证明了我们提出的算法在强凸问题上的收敛率。
[^FLOP]:A simple yet effective algorithm, named Federated Learning on Medical Datasets using Partial Networks (FLOP), that shares only a partial model between the server and clients. 一种简单而有效的算法，被命名为使用部分网络的医学数据集的联邦学习（FLOP），该算法在服务器和客户之间只共享部分模型。
[^FedFast]:FedFast accelerates distributed learning which achieves good accuracy for all users very early in the training process. We achieve this by sampling from a diverse set of participating clients in each training round and applying an active aggregation method that propagates the updated model to the other clients. Consequently, with FedFast the users benefit from far lower communication costs and more accurate models that can be consumed anytime during the training process even at the very early stages. FedFast加速了分布式学习，在训练过程的早期为所有用户实现了良好的准确性。我们通过在每轮训练中从不同的参与客户中取样，并应用主动聚合方法，将更新的模型传播给其他客户来实现这一目标。因此，有了FedFast，用户可以从更低的通信成本和更准确的模型中受益，这些模型可以在训练过程中随时使用，即使是在最早期阶段。

[^FDSKL]:FDSKL, a federated doubly stochastic kernel learning algorithm for vertically partitioned data. Specifically, we use random features to approximate the kernel mapping function and use doubly stochastic gradients to update the solutions, which are all computed federatedly without the disclosure of data. FDSKL，一个针对纵向分割数据的联邦双随机核学习算法。具体来说，我们使用随机特征来近似核映射函数，并使用双重随机梯度来更新解决方案，这些都是在不透露数据的情况下联邦计算的。

[^FADE]:While adversarial learning is commonly used in centralized learning for mitigating bias, there are significant barriers when extending it to the federated framework. In this work, we study these barriers and address them by proposing a novel approach Federated Adversarial DEbiasing (FADE). FADE does not require users' sensitive group information for debiasing and offers users the freedom to opt-out from the adversarial component when privacy or computational costs become a concern. 虽然对抗性学习通常用于集中式学习以减轻偏见，但当把它扩展到联邦式框架中时，会有很大的障碍。 在这项工作中，我们研究了这些障碍，并通过提出一种新的方法 Federated Adversarial DEbiasing（FADE）来解决它们。FADE不需要用户的敏感群体信息来进行去偏，并且当隐私或计算成本成为一个问题时，用户可以自由地选择退出对抗性部分。
[^CNFGNN]:Cross-Node Federated Graph Neural Network (CNFGNN) , a federated spatio-temporal model, which explicitly encodes the underlying graph structure using graph neural network (GNN)-based architecture under the constraint of cross-node federated learning, which requires that data in a network of nodes is generated locally on each node and remains decentralized. CNFGNN operates by disentangling the temporal dynamics modeling on devices and spatial dynamics on the server, utilizing alternating optimization to reduce the communication cost, facilitating computations on the edge devices. 跨节点联邦图神经网络（CNFGNN），是一个联邦时空模型，在跨节点联邦学习的约束下，使用基于图神经网络（GNN）的架构对底层图结构进行显式编码，这要求节点网络中的数据是在每个节点上本地生成的，并保持分散。CNFGNN通过分解设备上的时间动态建模和服务器上的空间动态来运作，利用交替优化来降低通信成本，促进边缘设备的计算。
[^Federated-Learning-source]:This paper have built a framework that enables Federated Learning (FL) for a small number of stakeholders. and described the framework architecture, communication protocol, and algorithms.  本文建立了一个框架，为少数利益相关者实现联邦学习（FL），并描述了框架架构、通信协议和算法。
[^FDKT]:A novel Federated Deep Knowledge Tracing (FDKT) framework to collectively train high-quality Deep Knowledge Tracing (DKT) models for multiple silos.  一个新颖的联邦深度知识追踪（FDKT）框架，为多个筒仓集体训练高质量的深度知识追踪（DKT）模型。
[^FOLtR-ES]:Federated Online Learning to Rank setup (FOLtR) where on-mobile ranking models are trained in a way that respects the users' privacy. FOLtR-ES that satisfies these requirement: (a) preserving the user privacy, (b) low communication and computation costs, (c) learning from noisy bandit feedback, and (d) learning with non-continuous ranking quality measures. A part of FOLtR-ES is a privatization procedure that allows it to provide ε-local differential privacy guarantees, i.e. protecting the clients from an adversary who has access to the communicated messages. This procedure can be applied to any absolute online metric that takes finitely many values or can be discretized to a finite domain. 联邦在线学习排名设置（FOLtR）中，移动端排名模型是以尊重用户隐私的方式来训练的。FOLtR-ES满足这些要求：(a)保护用户隐私，(b)低通信和计算成本，(c)从嘈杂的强盗反馈中学习，以及(d)用非连续的排名质量指标学习。FOLtR-ES的一部分是一个私有化程序，使其能够提供ε-local差异化的隐私保证，即保护客户不受能够接触到通信信息的对手的伤害。 这个程序可以应用于任何绝对在线度量，其取值有限，或者可以离散到一个有限域。

[^MetaMF]:A federated matrix factorization (MF) framework, named meta matrix factorization (MetaMF) for rating prediction (RP) for mobile environments. 一个联邦矩阵分解（MF）框架，命名为元矩阵分解（MetaMF），用于移动环境的评级预测（RP）。
[^FedCMR]:The federated cross-modal retrieval (FedCMR), which learns the model with decentralized multi-modal data. 联邦跨模式检索（FedCMR），它用分散的多模式数据学习模型。
[^FedGWAS]:Under some circumstances, the private data can be reconstructed from the model parameters, which implies that data leakage can occur in FL.In this paper, we draw attention to another risk associated with FL: Even if federated algorithms are individually privacy-preserving, combining them into pipelines is not necessarily privacy-preserving. We provide a concrete example from genome-wide association studies, where the combination of federated principal component analysis and federated linear regression allows the aggregator to retrieve sensitive patient data by solving an instance of the multidimensional subset sum problem. This supports the increasing awareness in the field that, for FL to be truly privacy-preserving, measures have to be undertaken to protect against data leakage at the aggregator. 在某些情况下，私人数据可以从模型参数中重建，这意味着在联邦学习中可能发生数据泄漏。 在本文中，我们提请注意与FL相关的另一个风险。即使联邦算法是单独保护隐私的，将它们组合成管道也不一定是保护隐私的。我们提供了一个来自全基因组关联研究的具体例子，其中联邦主成分分析和联邦线性回归的组合允许聚合器通过解决多维子集和问题的实例来检索敏感的病人数据。这支持了该领域日益增长的意识，即为了使FL真正保护隐私，必须采取措施防止聚合器的数据泄漏。
[^FedCT]:The cross-domain recommendation problem is formalized under a decentralized computing environment with multiple domain servers. And we identify two key challenges for this setting: the unavailability of direct transfer and the heterogeneity of the domain-specific user representations. We then propose to learn and maintain a decentralized user encoding on each user's personal space. The optimization follows a variational inference framework that maximizes the mutual information between the user's encoding and the domain-specific user information from all her interacted domains. 跨域推荐问题在具有多个域服务器的去中心化计算环境下被形式化。我们确定了这种情况下的两个关键挑战：直接传输的不可用性和特定领域用户表征的异质性。然后，我们建议在每个用户的个人空间上学习和维护一个分散的用户编码。优化遵循一个变分推理框架，使用户的编码和来自她所有互动领域的特定用户信息之间的互信息最大化。
[^noniid-foltr]:In this perspective paper we study the effect of non independent and identically distributed (non-IID) data on federated online learning to rank (FOLTR) and chart directions for future work in this new and largely unexplored research area of Information Retrieval. 在这篇前瞻论文中，我们研究了非独立和相同分布（非IID）数据对联邦在线学习排名（FOLTR）的影响，并为这个新的、基本上未被开发的信息检索研究领域的未来工作指明了方向。
[^FedScale]:FedScale, a federated learning (FL) benchmarking suite with realistic datasets and a scalable runtime to enable reproducible FL research. FedScale是一个联邦学习（FL）基准测试套件，具有现实的数据集和可扩展的运行时间，以实现可重复的FL研究。
[^PAPAYA]:PAPAYA outline a production asynchronous FL system design. Empirically, we demonstrate that asynchronous FL converges faster than synchronous FL when training across nearly one hundred million devices. In particular, in high concurrency settings, asynchronous FL is 5x faster and has nearly 8x less communication overhead than synchronous FL. PAPAYA概述了一个生产性的异步联邦系统设计。根据经验，我们证明了在近一亿台设备上进行训练时，异步FL比同步FL收敛得更快。特别是，在高并发环境下，异步FL比同步FL快5倍，通信开销少8倍。
[^LightSecAgg]:State-of-the-art secure aggregation protocols rely on secret sharing of the random-seeds used for mask generations at the users to enable the reconstruction and cancellation of those belonging to the dropped users. The complexity of such approaches, however, grows substantially with the number of dropped users. LightSecAgg, to overcome this bottleneck by changing the design from "random-seed reconstruction of the dropped users" to "one-shot aggregate-mask reconstruction of the active users via mask encoding/decoding". 最先进的安全聚合协议依赖于在用户处秘密共享用于掩码生成的随机种子，以便能够重建和取消属于被放弃用户的随机种子。然而，这种方法的复杂性随着被放弃的用户数量的增加而大大增加。LightSecAgg 通过将设计从 "被放弃用户的随机种子重建 "改为 "通过掩码编码/解码对活跃用户进行一次性聚合掩码重建 "来克服这个瓶颈。
[^Oort]:Oort, improve the performance of federated training and testing with guided participant selection. With an aim to improve time-to-accuracy performance in model training, Oort prioritizes the use of those clients who have both data that offers the greatest utility in improving model accuracy and the capability to run training quickly. To enable FL developers to interpret their results in model testing, Oort enforces their requirements on the distribution of participant data while improving the duration of federated testing by cherry-picking clients. Oort，通过指导性的参与者选择来提高联邦训练和测试的性能。为了提高模型训练的时间-精度性能，Oort优先使用那些既拥有对提高模型精度有最大作用的数据又有能力快速运行训练的客户。为了使FL开发者能够解释他们在模型测试中的结果，Oort强制执行他们对参与者数据分布的要求，同时通过挑选客户来改善联邦测试的持续时间。
[^FedProx]:FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. FedProx，解决联邦网络中的异质性问题。FedProx可以被看作是FedAvg的概括和重新参数化，FedAvg是目前最先进的联邦学习方法。虽然这种重新参数化只对方法本身做了微小的修改，但这些修改在理论和实践上都有重要的影响。在理论上，我们为我们的框架提供了收敛保证，当对来自非相同分布的数据进行学习时（统计异质性），同时通过允许每个参与的设备执行不同数量的工作（系统异质性）来遵守设备级别的系统约束。在实践中，我们证明了FedProx比FedAvg在一系列现实的联邦数据集中能实现更稳健的收敛。
[^System_Design]:We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions. 我们已经为移动设备领域的联邦学习建立了一个可扩展的生产系统，基于TensorFlow。在本文中，我们描述了由此产生的高层次设计，概述了一些挑战和它们的解决方案，并谈到了开放的问题和未来的方向。
[^TRUDA]:TRUDA, a new cross-silo FL system, employing a trustworthy and decentralized aggregation architecture to break down information concentration with regard to a single aggregator. Based on the unique computational properties of model-fusion algorithms, all exchanged model updates in TRUDA are disassembled at the parameter-granularity and re-stitched to random partitions designated for multiple TEE-protected aggregators. TRUDA是一个新的跨机构FL系统，采用了一个可信的、分散的聚合架构，以打破对单一聚合器的信息集中。基于模型融合算法的独特计算特性，TRUDA中所有交换的模型更新都在参数粒度上被分解，并重新缝合到指定给多个受TEE保护的聚合器的随机分区。
[^FedX]:
[^SIMC]:
[^FLAME]:
[^DeepSight]:
[^FedCRI]:
[^PBPFL]:
[^FedKC]:
[^LocFedMix-SL]:
[^FLASH]:
[^Hermes]:
[^FedServing]:
[^FedGTF-EF-PC]:
[^PFA]:
[^Meta-HAR]:
[^P-FedAvg]:
[^FedSens]:
[^FedDA]:
[^FAIR]:
[^FedFPM]:
[^Federated-Bandit]:
[^ActPerFL]:
[^FedNLP]:
[^Efficient-FedRec]:
[^FedADMM]:
[^FedMP]:
[^FedMigr]:
[^Samba]:
[^FedRecAttack]:
[^DIG-FL]:
[^BlindFL]:
[^FedRain-and-Frog]:
[^FLIX]:
[^DP-SCAFFOLD]:
[^SparseFed]:
[^QLSD]:
[^Fed-ET]:
[^CReFF]:
[^FedCG]:
[^FedDUAP]:
[^FedSpeech]:
[^FedKT]:
[^FEDMD-NFDP]:
[^LDP-FL]:
[^FedFV]:
[^H-FL]:
[^FedGame]:
[^SmartIdx]:
[^FedFIM]:
[^FedProto]:
[^FedSoft]:
[^FedFR]:
[^SplitFed]:
[^FlyNNFL]:
[^FedRec++]:
[^FLAME_D]:
[^FedAMP]:
[^FedUL]:
[^FedChain]:
[^FedReg]:
[^Fed-RoD]:
[^HeteroFL]:
[^FedMix]:
[^FedFomo]:
[^FedBN]:
[^FedBE]:
[^FedMA]:
[^fair-flearn]:
[^FedNew]:
[^DisPFL]:
[^DAdaQuant]:
[^FedNL]:
[^FedNest]:
[^EDEN]:
[^ProgFed]:
[^PPSGD]:
[^PBM]:
[^FedMLB]:
[^FedPU]:
[^Orchestra]:
[^DFL]:
[^FedHeNN]:
[^KNN-PER]:
[^ProxRR]:
[^VFL]:
[^breaching]:
[^QSFL]:
[^Neurotoxin]:
[^FL-NTK]:
[^FedBoost]:
[^FetchSGD]:
[^SCAFFOLD]:
[^Sageflow]:
[^CAFE]:
[^FedSplit]:
[^QuPeD]:
[^GradAttack]:
[^FedDR]:
[^Federated-EM]:
[^FL-WBC]:
[^FjORD]:
[^FedEx]:
[^Large-Cohort]:
[^DeepReduce]:
[^PartialFed]:
[^FCFL ]:
[^fbo]:
[^RobustFL]:
[^FedDF]:
[^DRFA]:
[^FedAc]:
[^FedNova]:
[^FedGKT]:
[^Per-FedAvg]:
[^SpreadGNN]:SpreadGNN, a novel multi-task federated training framework capable of operating in the presence of partial labels and absence of a central server for the first time in the literature. We provide convergence guarantees and empirically demonstrate the efficacy of our framework on a variety of non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels. SpreadGNN首次提出一个新颖的多任务联邦训练框架，能够在存在部分标签和没有中央服务器的情况下运行。我们提供了收敛保证，并在各种具有部分标签的非I.I.D.分布式图级分子特性预测数据集上实证了我们框架的功效。我们的研究结果表明，SpreadGNN优于通过依赖中央服务器的联邦学习系统训练的GNN模型，即使在受限的拓扑结构中也是如此。
[^MaKEr]:We study the knowledge extrapolation problem to embed new components (i.e., entities and relations) that come with emerging knowledge graphs (KGs) in the federated setting. In this problem, a model trained on an existing KG needs to embed an emerging KG with unseen entities and relations. To solve this problem, we introduce the meta-learning setting, where a set of tasks are sampled on the existing KG to mimic the link prediction task on the emerging KG. Based on sampled tasks, we meta-train a graph neural network framework that can construct features for unseen components based on structural information and output embeddings for them. 我们研究了知识外推问题，以嵌入新的组件（即实体和关系），这些组件来自于联邦设置的新兴知识图（KGs）。在这个问题上，一个在现有KG上训练的模型需要嵌入一个带有未见过的实体和关系的新兴KG。为了解决这个问题，我们引入了元学习设置，在这个设置中，一组任务在现有的KG上被抽样，以模拟新兴KG上的链接预测任务。基于抽样任务，我们对图神经网络框架进行元训练，该框架可以根据结构信息为未见过的组件构建特征，并为其输出嵌入。
[^SFL]:A novel structured federated learning (SFL) framework to enhance the knowledge-sharing process in PFL by leveraging the graph-based structural information among clients and learn both the global and personalized models simultaneously using client-wise relation graphs and clients' private data. We cast SFL with graph into a novel optimization problem that can model the client-wise complex relations and graph-based structural topology by a unified framework. Moreover, in addition to using an existing relation graph, SFL could be expanded to learn the hidden relations among clients. 一个新的结构化联邦学习（SFL）框架通过利用客户之间基于图的结构信息来加强PFL中的知识共享过程，并使用客户的关系图和客户的私人数据同时学习全局和个性化的模型。我们把带图的SFL变成一个新的优化问题，它可以通过一个统一的框架对客户的复杂关系和基于图的结构拓扑进行建模。此外，除了使用现有的关系图之外，SFL还可以扩展到学习客户之间的隐藏关系。
[^VFGNN]:VFGNN, a federated GNN learning paradigm for privacy-preserving node classification task under data vertically partitioned setting, which can be generalized to existing GNN models. Specifically, we split the computation graph into two parts. We leave the private data (i.e., features, edges, and labels) related computations on data holders, and delegate the rest of computations to a semi-honest server. We also propose to apply differential privacy to prevent potential information leakage from the server. VFGNN是一种联邦的GNN学习范式，适用于数据纵向分割情况下的隐私保护节点分类任务，它可以被推广到现有的GNN模型。具体来说，我们将计算图分成两部分。我们将私有数据（即特征、边和标签）相关的计算留给数据持有者，并将其余的计算委托给半诚实的服务器。我们还提议应用差分隐私来防止服务器的潜在信息泄露。
[^GCFL]:Graphs can also be regarded as a special type of data samples. We analyze real-world graphs from different domains to confirm that they indeed share certain graph properties that are statistically significant compared with random graphs. However, we also find that different sets of graphs, even from the same domain or same dataset, are non-IID regarding both graph structures and node features. A graph clustered federated learning (GCFL) framework that dynamically finds clusters of local systems based on the gradients of GNNs, and theoretically justify that such clusters can reduce the structure and feature heterogeneity among graphs owned by the local systems. Moreover, we observe the gradients of GNNs to be rather fluctuating in GCFL which impedes high-quality clustering, and design a gradient sequence-based clustering mechanism based on dynamic time warping (GCFL+). 图也可以看作是一种特殊类型的数据样本。我们分析来自不同领域的真实图，以确认它们确实共享某些与随机图形相比具有统计意义的图属性。然而，我们也发现不同的图集，即使来自相同的域或相同的数据集，在图结构和节点特性方面都是非IID的。图聚类联邦学习(GCFL)框架，基于GNNs的梯度动态地找到本地系统的集群，并从理论上证明这样的集群可以减少本地系统所拥有的图之间的结构和特征异构性。此外，我们观察到GNNs的梯度在GCFL中波动较大，阻碍了高质量的聚类，并设计了基于动态时间规整的梯度序列聚类机制(GCFL+)。
[^D2D-FedL]:Two important characteristics of contemporary wireless networks: (i) the network may contain heterogeneous communication/computation resources, while (ii) there may be significant overlaps in devices' local data distributions. In this work, we develop a novel optimization methodology that jointly accounts for these factors via intelligent device sampling complemented by device-to-device (D2D) offloading. Our optimization aims to select the best combination of sampled nodes and data offloading configuration to maximize FedL training accuracy subject to realistic constraints on the network topology and device capabilities. Theoretical analysis of the D2D offloading subproblem leads to new FedL convergence bounds and an efficient sequential convex optimizer. Using this result, we develop a sampling methodology based on graph convolutional networks (GCNs) which learns the relationship between network attributes, sampled nodes, and resulting offloading that maximizes FedL accuracy. 当代无线网络的两个重要特征：( i )网络中可能包含异构的通信/计算资源( ii )设备的本地数据分布可能存在显著的重叠。在这项工作中，我们开发了一种新的优化方法，通过智能设备采样和设备到设备(D2D)卸载来共同考虑这些因素。我们的优化目标是在网络拓扑和设备能力的现实约束下，选择采样节点和数据卸载配置的最佳组合，以最大化FedL训练精度。对D2D卸载子问题的理论分析得到了新的FedL收敛界和一个有效的序列凸优化器。利用这一结果，我们开发了一种基于图卷积网络(GCN)的采样方法，该方法学习网络属性、采样节点和结果卸载之间的关系，从而最大化FedL的准确性。
[^FedSage]:In this work, towards the novel yet realistic setting of subgraph federated learning, we propose two major techniques: (1) FedSage, which trains a GraphSage model based on FedAvg to integrate node features, link structures, and task labels on multiple local subgraphs; (2) FedSage+, which trains a missing neighbor generator along FedSage to deal with missing links across local subgraphs. 在本工作中，针对子图联邦学习的新颖而现实的设置，我们提出了两个主要技术：(1) FedSage，它基于FedAvg训练一个GraphSage模型，以整合多个局部子图上的节点特征、链接结构和任务标签；(2) FedSage +，它沿着FedSage训练一个缺失的邻居生成器，以处理跨本地子图的缺失链接。
[^RBCS-F]:Model the fairness guaranteed client selection as a Lyapunov optimization problem and then a C2MAB-based method is proposed for estimation of the model exchange time between each client and the server, based on which we design a fairness guaranteed algorithm termed RBCS-F for problem-solving.  我们将保证公平性的客户选择建模为一个Lyapunov优化问题，然后提出一个基于C2MAB的方法来估计每个客户和服务器之间的模型交换时间，在此基础上，我们设计了一个保证公平性的算法，即RBCS-F来解决问题。
[^VF2Boost]:VF2Boost, a novel and efficient vertical federated GBDT system. First, to handle the deficiency caused by frequent mutual-waiting in federated training, we propose a concurrent training protocol to reduce the idle periods. Second, to speed up the cryptography operations, we analyze the characteristics of the algorithm and propose customized operations. Empirical results show that our system can be 12.8-18.9 times faster than the existing vertical federated implementations and support much larger datasets. VF2Boost，一个新颖而高效的纵向联邦GBDT系统。首先，为了处理联邦训练中频繁的相互等待造成的缺陷，我们提出了一个并发训练协议来减少空闲期。第二，为了加快密码学操作，我们分析了算法的特点，并提出了定制的操作。经验结果表明，我们的系统可以比现有的纵向联邦实现快12.8-18.9倍，并支持更大的数据集。我们将保证公平性的客户选择建模为一个Lyapunov优化问题，然后提出一个基于C2MAB的方法来估计每个客户和服务器之间的模型交换时间，在此基础上，我们设计了一个保证公平性的算法，即RBCS-F来解决问题。
[^SimFL]:A practical horizontal federated environment with relaxed privacy constraints. In this environment, a dishonest party might obtain some information about the other parties' data, but it is still impossible for the dishonest party to derive the actual raw data of other parties. Specifically, each party boosts a number of trees by exploiting similarity information based on locality-sensitive hashing. 一个具有宽松隐私约束的实用横向联邦环境。在这种环境中，不诚实的一方可能会获得其他方数据的一些信息，但不诚实的一方仍然不可能得出其他方的实际原始数据。具体来说，每一方通过利用基于位置敏感散列的相似性信息来提升一些树。
[^Pivot-DT]:Pivot, a novel solution for privacy preserving vertical decision tree training and prediction, ensuring that no intermediate information is disclosed other than those the clients have agreed to release (i.e., the final tree model and the prediction output). Pivot does not rely on any trusted third party and provides protection against a semi-honest adversary that may compromise m - 1 out of m clients. We further identify two privacy leakages when the trained decision tree model is released in plain-text and propose an enhanced protocol to mitigate them. The proposed solution can also be extended to tree ensemble models, e.g., random forest (RF) and gradient boosting decision tree (GBDT) by treating single decision trees as building blocks. Pivot，一个用于保护隐私的纵向决策树训练和预测的新颖解决方案，确保除了客户同意发布的信息（即最终的树模型和预测输出）外，没有任何中间信息被披露。Pivot不依赖任何受信任的第三方，并提供保护，防止半诚实的对手可能损害m个客户中的m-1。我们进一步确定了当训练好的决策树模型以明文形式发布时的两个隐私泄漏，并提出了一个增强的协议来缓解这些泄漏。通过将单个决策树作为构建块，所提出的解决方案也可以扩展到集成树模型，如随机森林（RF）和梯度提升决策树（GBDT）。
[^FKGE]:A novel decentralized scalable learning framework, Federated Knowledge Graphs Embedding (FKGE), where embeddings from different knowledge graphs can be learnt in an asynchronous and peer-to-peer manner while being privacy-preserving. FKGE exploits adversarial generation between pairs of knowledge graphs to translate identical entities and relations of different domains into near embedding spaces. In order to protect the privacy of the training data, FKGE further implements a privacy-preserving neural network structure to guarantee no raw data leakage. 一种新颖的去中心化可扩展学习框架，联邦知识图谱嵌入(FKGE)，其中来自不同知识图谱的嵌入可以以异步和对等的方式学习，同时保持隐私。FKGE利用成对知识图谱之间的对抗生成，将不同领域的相同实体和关系转换到临近嵌入空间。为了保护训练数据的隐私，FKGE进一步实现了一个保护隐私的神经网络结构，以保证原始数据不会泄露。
[^Glint]: 
[^PPSGCN]: 
[^FedNI]: FedNI, to leverage network inpainting and inter-institutional data via FL. Specifically, we first federatively train missing node and edge predictor using a graph generative adversarial network (GAN) to complete the missing information of local networks. Then we train a global GCN node classifier across institutions using a federated graph learning platform. The novel design enables us to build more accurate machine learning models by leveraging federated learning and also graph learning approaches. FedNI，通过 FL 来利用网络补全和机构间数据。 具体来说，我们首先使用图生成对抗网络（GAN）对缺失节点和边缘预测器进行联邦训练，以完成局部网络的缺失信息。 然后，我们使用联邦图学习平台跨机构训练全局 GCN 节点分类器。 新颖的设计使我们能够通过利用联邦学习和图学习方法来构建更准确的机器学习模型。
[^FedVGCN]: 
[^FedSGC]: We study the vertical and horizontal settings for federated learning on graph data. We propose FedSGC to train the Simple Graph Convolution model under three data split scenarios. 我们研究了图数据上联邦学习的横向和纵向设置。我们提出FedSGC在三种数据分割场景下训练简单图卷积模型。
[^SGNN]: 
[^FedGL]: 
[^FL-AGCNS]: 
[^FL-DISCO]: A holistic collaborative and privacy-preserving FL framework, namely FL-DISCO, which integrates GAN and GNN to generate molecular graphs. 集成GAN和GNN生成分子图的整体协作和隐私保护FL框架FL-DISCO。
[^DAG-FL]: 
[^FedE]: 
[^FedGraph]: FedGraph for federated graph learning among multiple computing clients, each of which holds a subgraph. FedGraph provides strong graph learning capability across clients by addressing two unique challenges. First, traditional GCN training needs feature data sharing among clients, leading to risk of privacy leakage. FedGraph solves this issue using a novel cross-client convolution operation. The second challenge is high GCN training overhead incurred by large graph size. We propose an intelligent graph sampling algorithm based on deep reinforcement learning, which can automatically converge to the optimal sampling policies that balance training speed and accuracy.  FedGraph 用于多个计算客户端之间的联邦图学习，每个客户端都有一个子图。FedGraph通过解决两个独特的挑战，跨客户端提供了强大的图形学习能力。首先，传统的GCN训练需要在客户之间进行功能数据共享，从而导致隐私泄露的风险。FedGraph使用一种新的跨客户端卷积操作来解决了这个问题。第二个挑战是大图所产生的高GCN训练开销。提出了一种基于深度强化学习的智能图采样算法，该算法可以自动收敛到最优的平衡训练速度和精度的采样策略。
[^GraFeHTy]: 
[^ASFGNN]: 
[^FedGNN]: 
[^FD-GATDR]: 
[^FedGCN]: 
[^GraphFL]: 
[^eFL-Boost]: Efficient FL for GBDT (eFL-Boost), which minimizes accuracy loss, communication costs, and information leakage. The proposed scheme focuses on appropriate allocation of local computation (performed individually by each organization) and global computation (performed cooperatively by all organizations) when updating a model. A tree structure is determined locally at one of the organizations, and leaf weights are calculated globally by aggregating the local gradients of all organizations. Specifically, eFL-Boost requires only three communications per update, and only statistical information that has low privacy risk is leaked to other organizations. 针对GBDT的高效FL（eFL-Boost），将精度损失、通信成本和信息泄露降到最低。该方案的重点是在更新模型时适当分配局部计算（由每个组织单独执行）和全局计算（由所有组织合作执行）。树状结构由其中一个组织在本地确定，而叶子的权重则由所有组织的本地梯度汇总后在全局计算。具体来说，eFL-Boost每次更新只需要三次通信，而且只有具有低隐私风险的统计信息才会泄露给其他组织。
[^Fed-EINI]: Decision tree ensembles such as gradient boosting decision trees (GBDT) and random forest are widely applied powerful models with high interpretability and modeling efficiency. However, state-of-art framework for decision tree ensembles in vertical federated learning frameworks adapt anonymous features to avoid possible data breaches, makes the interpretability of the model compromised. Fed-EINI make a problem analysis about the necessity of disclosure meanings of feature to Guest Party in vertical federated learning. Fed-EINI protect data privacy and allow the disclosure of feature meaning by concealing decision paths and adapt a communication-efficient secure computation method for inference outputs. 集成决策树，如梯度提升决策树（GBDT）和随机森林，是被广泛应用的强大模型，具有较高的可解释性和建模效率。然而，纵向联邦学习框架中的决策树群的先进框架适应匿名特征以避免可能的数据泄露，使得模型的可解释性受到影响。Fed-EINI对纵向联邦学习中向客人方披露特征含义的必要性进行了问题分析。Fed-EINI通过隐藏决策路径来保护数据隐私，并允许披露特征含义，同时为推理输出适应一种通信效率高的安全计算方法。
[^GBF-Cen]: Propose a new tree-boosting method, named Gradient Boosting Forest (GBF), where the single decision tree in each gradient boosting round of GBDT is replaced by a set of trees trained from different subsets of the training data (referred to as a forest), which enables training GBDT in Federated Learning scenarios. We empirically prove that GBF outperforms the existing GBDT methods in both centralized (GBF-Cen) and federated (GBF-Fed) cases. 我们提出了一种新的提升树方法，即梯度提升森林（GBF），在GBDT的每一轮梯度提升中，单一的决策树被一组从训练数据的不同子集训练出来的树（称为森林）所取代，这使得在联邦学习场景中可以训练GBDT。我们通过经验证明，GBF在集中式（GBF-Cen）和联邦式（GBF-Fed）情况下都优于现有的GBDT方法。
[^AF-DNDF]: AF-DNDF which extends DNDF (Deep Neural Decision Forests, which unites classification trees with the representation learning functionality from deep convolutional neural networks) with an asynchronous federated aggregation protocol. Based on the local quality of each classification tree, our architecture can select and combine the optimal groups of decision trees from multiple local devices. AF-DNDF，它将DNDF（深度神经决策森林，它将分类树与深度卷积神经网络的表征学习功能结合起来）与一个异步的联邦聚合协议进行了扩展。基于每个分类树的本地质量，我们的架构可以选择和组合来自多个本地设备的最佳决策树组。
[^FedCluster]: FedCluster, a novel federated learning framework with improved optimization efficiency, and investigate its theoretical convergence properties. The FedCluster groups the devices into multiple clusters that perform federated learning cyclically in each learning round. FedCluster是一个具有改进的优化效率的新型联邦学习框架，并研究其理论收敛特性。FedCluster将设备分成多个集群，在每一轮学习中循环进行联邦学习。
[^Hercules]: 
[^SecureBoost+]: 
[^FederBoost]: FederBoost for private federated learning of gradient boosting decision trees (GBDT). It supports running GBDT over both horizontally and vertically partitioned data. The key observation for designing FederBoost is that the whole training process of GBDT relies on the order of the data instead of the values. Consequently, vertical FederBoost does not require any cryptographic operation and horizontal FederBoost only requires lightweight secure aggregation. FederBoost用于梯度提升决策树（GBDT）的私有联邦学习。它支持在横向和纵向分区的数据上运行GBDT。设计FederBoost的关键是，GBDT的整个训练过程依赖于数据的顺序而不是数值。因此，纵向FederBoost不需要任何加密操作，横向FederBoost只需要轻量级的安全聚合。
[^FedR]: In this paper, we first develop a novel attack that aims to recover the original data based on embedding information, which is further used to evaluate the vulnerabilities of FedE. Furthermore, we propose a Federated learning paradigm with privacy-preserving Relation embedding aggregation (FedR) to tackle the privacy issue in FedE. Compared to entity embedding sharing, relation embedding sharing policy can significantly reduce the communication cost due to its smaller size of queries. 在本文中，我们首先开发了一个新颖的攻击，旨在基于嵌入信息恢复原始数据，并进一步用于评估FedE的漏洞。此外，我们提出了一种带有隐私保护的关系嵌入聚合（FedR）的联邦学习范式，以解决FedE的隐私问题。与实体嵌入共享相比，关系嵌入共享策略由于其较小的查询规模，可以大大降低通信成本。
[^FedPerGNN]: FedPerGNN, a federated GNN framework for both effective and privacy-preserving personalization. Through a privacy-preserving model update method, we can collaboratively train GNN models based on decentralized graphs inferred from local data. To further exploit graph information beyond local interactions, we introduce a privacy-preserving graph expansion protocol to incorporate high-order information under privacy protection.  FedPerGNN是一个既有效又保护隐私的GNN联盟框架。通过一个保护隐私的模型更新方法，我们可以根据从本地数据推断出的分散图来协作训练GNN模型。为了进一步利用本地互动以外的图信息，我们引入了一个保护隐私的图扩展协议，在保护隐私的前提下纳入高阶信息。
[^FeSoG]: 
[^FedAlign-KG]: 
[^FLIT+]: Federated learning allows end users to build a global model collaboratively while keeping their training data isolated. We ﬁrst simulate a heterogeneous federated-learning benchmark (FedChem) by jointly performing scaffold splitting and latent Dirichlet allocation on existing datasets. Our results on FedChem show that signiﬁcant learning challenges arise when working with heterogeneous molecules across clients. We then propose a method to alleviate the problem: Federated Learning by Instance reweighTing (FLIT+). FLIT+ can align local training across clients. Experiments conducted on FedChem validate the advantages of this method. 联邦学习允许最终用户协同构建全局模型，同时保持他们的训练数据是孤立的。我们首先通过在现有数据集上联合执行支架拆分和隐狄利克雷分配来模拟一个异构的联邦学习基准FedChem 。我们在FedChem上的研究结果表明，在跨客户端处理异构分子时，会出现显著的学习挑战。然后，我们提出了一种缓解该问题的方法：实例重加权联邦学习FLIT + 。FLIT+可以跨客户对齐本地训练。在FedChem上进行的实验验证了这种方法的优势。
[^FML-ST]: A privacy-preserving spatial-temporal prediction technique via federated learning (FL). Due to inherent non-independent identically distributed (non-IID) characteristic of spatial-temporal data, the basic FL-based method cannot deal with this data heterogeneity well by sharing global model; furthermore, we propose the personalized federated learning methods based on meta-learning. We automatically construct the global spatial-temporal pattern graph under a data federation. This global pattern graph incorporates and memorizes the local learned patterns of all of the clients, and each client leverages those global patterns to customize its own model by evaluating the difference between global and local pattern graph. Then, each client could use this customized parameters as its model initialization parameters for spatial-temporal prediction tasks. 一种通过联邦学习(FL)保护隐私的时空预测技术。由于时空数据固有的非独立同分布(non-IID)特性，基本的基于FL的方法无法通过共享全局模型很好地处理这种数据异构性；此外，我们提出了基于元学习的个性化联邦学习方法。我们在数据联邦下自动构建全局时空模式图。这个全局模式图包含并记忆了所有客户机的本地学习模式，每个客户机利用这些全局模式通过评估全局模式图和本地模式图之间的差异来定制自己的模型。然后，每个客户端可以使用这个定制的参数作为其时空预测任务的模型初始化参数。
[^FedGCN-NES]: A Federated Learning-Based Graph Convolutional Network (FedGCN). First, we propose a Graph Convolutional Network (GCN) as a local model of FL. Based on the classical graph convolutional neural network, TopK pooling layers and full connection layers are added to this model to improve the feature extraction ability. Furthermore, to prevent pooling layers from losing information, cross-layer fusion is used in the GCN, giving FL an excellent ability to process non-Euclidean spatial data. Second, in this paper, a federated aggregation algorithm based on an online adjustable attention mechanism is proposed. The trainable parameter ρ is introduced into the attention mechanism. The aggregation method assigns the corresponding attention coefficient to each local model, which reduces the damage caused by the inefficient local model parameters to the global model and improves the fault tolerance and accuracy of the FL algorithm. 基于联邦学习的图卷积网络(Fedgcn)。首先，我们提出了一个图卷积网络(GCN)作为FL的局部模型。该模型在经典图卷积神经网络的基础上，增加了Top K池化层和全连接层，提高了特征提取能力。此外，为了防止池化层丢失信息，在GCN中使用跨层融合，使FL具有处理非欧几里得空间数据的出色能力。其次，本文提出了一种基于在线可调注意力机制的联合聚合算法。可训练参数ρ被引入注意力机制。聚合方法为每个局部模型分配相应的注意力系数，减少了低效的局部模型参数对全局模型造成的破坏，提高了FL算法的容错性和准确性。
[^CTFL]: C lustering-based hierarchical and T wo-step- optimized FL (CTFL) employs a divide-and-conquer strategy, clustering clients based on the closeness of their local model parameters. Furthermore, we incorporate the particle swarm optimization algorithm in CTFL, which employs a two-step strategy for optimizing local models. This technique enables the central server to upload only one representative local model update from each cluster, thus reducing the communication overhead associated with model update transmission in the FL. 基于聚类的层次化和两步优化的FL ( CTFL )采用分治策略，根据本地模型参数的接近程度对客户端进行聚类。此外，我们将粒子群优化算法集成到CTFL中，该算法采用两步策略优化局部模型。此技术使中心服务器能够仅从每个集群上载一个有代表性的本地模型更新，从而减少与FL中模型更新传输相关的通信开销。 
[^FASTGNN]: We introduce a differential privacy-based adjacency matrix preserving approach for protecting the topological information. We also propose an adjacency matrix aggregation approach to allow local GNN-based models to access the global network for a better training effect. Furthermore, we propose a GNN-based model named attention-based spatial-temporal graph neural networks (ASTGNN) for traffic speed forecasting. We integrate the proposed federated learning framework and ASTGNN as FASTGNN for traffic speed forecasting. 我们提出了一种基于差分隐私的邻接矩阵保护方法来保护拓扑信息。我们还提出了一种邻接矩阵聚合方法，允许基于局部GNN的模型访问全局网络，以获得更好的训练效果。此外，我们提出了一个基于GNN的模型，称为基于注意力的时空图神经网络(ASTGNN)的交通速度预测。我们将提出的联邦学习框架和ASTGNN集成为FASTGNN用于交通速度预测。
[^BiG-Fed]: 
[^D-FedGNN]: A new Decentralized Federated Graph Neural Network (D-FedGNN for short) which allows multiple participants to train a graph neural network model without a centralized server. Specifically, D-FedGNN uses a decentralized parallel stochastic gradient descent algorithm DP-SGD to train the graph neural network model in a peer-to-peer network structure. To protect privacy during model aggregation, D-FedGNN introduces the Diffie-Hellman key exchange method to achieve secure model aggregation between clients. 一个新的去中心化的联邦图神经网络(简称D-FedGNN)允许多个参与者在没有中心化服务器的情况下训练一个图神经网络模型。具体地，D-FedGNN采用去中心化的并行随机梯度下降算法DP-SGD在对等网络结构中训练图神经网络模型。为了保护模型聚合过程中的隐私，D-FedGNN引入了Diffie-Hellman密钥交换方法来实现客户端之间的安全模型聚合。
[^FedEgo]: FedEgo, a federated graph learning framework based on ego-graphs, where each client will train their local models while also contributing to the training of a global model. FedEgo applies GraphSAGE over ego-graphs to make full use of the structure information and utilizes Mixup for privacy concerns. To deal with the statistical heterogeneity, we integrate personalization into learning and propose an adaptive mixing coefficient strategy that enables clients to achieve their optimal personalization. FedEgo是一个基于自中心图的联邦图学习框架，每个客户端将训练他们的本地模型，同时也为全局模型的训练作出贡献。FedEgo在自中心图上应用GraphSAGE来充分利用结构信息，并利用Mixup来解决隐私问题。为了处理统计上的异质性，我们将个性化整合到学习中，并提出了一个自适应混合系数策略，使客户能够实现其最佳的个性化。
[^DFL-PENS]: 
[^P2P-FLG]: 
[^GFL]: 
[^FL-DSGD]: 
[^DSGD]: 
[^cPDS]: 
[^dFedU]: 
[^EF-HC]: 
[^FRF]: Federated Random Forests (FRF) models, focusing particularly on the heterogeneity within and between datasets. 联邦随机森林（FRF）模型，特别关注数据集内部和之间的异质性。
[^FF]: Federated Forest , which is a lossless learning model of the traditional random forest method, i.e., achieving the same level of accuracy as the non-privacy-preserving approach. Based on it, we developed a secure cross-regional machine learning system that allows a learning process to be jointly trained over different regions’ clients with the same user samples but different attribute sets, processing the data stored in each of them without exchanging their raw data. A novel prediction algorithm was also proposed which could largely reduce the communication overhead.  Federated Forest ，是传统随机森林方法的无损学习模型，即达到与非隐私保护方法相同的准确度。在此基础上，我们开发了一个安全的跨区域机器学习系统，允许在具有相同用户样本但不同属性集的不同区域的客户端上联邦训练一个学习过程，处理存储在每个客户端的数据，而不交换其原始数据。还提出了一种新的预测算法，可以在很大程度上减少通信开销。
[^FFGB]: Federated functional gradient boosting (FFGB). Under appropriate assumptions on the weak learning oracle, the FFGB algorithm is proved to efficiently converge to certain neighborhoods of the global optimum. The radii of these neighborhoods depend upon the level of heterogeneity measured via the total variation distance and the much tighter Wasserstein-1 distance, and diminish to zero as the setting becomes more homogeneous.
[^Fed-GBM]: Fed-GBM (Federated Gradient Boosting Machines), a cost-effective collaborative learning framework, consisting of two-stage voting and node-level parallelism, to address the problems in co-modelling for Non-intrusive load monitoring (NILM). Fed-GBM(联邦梯度提升)是一个具有成本效益的协作学习框架，由两阶段投票和节点级并行组成，用于解决非侵入式负载监测(NILM)中的协同建模问题。
[^TFL]: 
[^FedGBF]: 
[^HFL-XGBoost]: A hybrid federated learning framework based on XGBoost, for distributed power prediction from real-time external features. In addition to introducing boosted trees to improve accuracy and interpretability, we combine horizontal and vertical federated learning, to address the scenario where features are scattered in local heterogeneous parties and samples are scattered in various local districts. Moreover, we design a dynamic task allocation scheme such that each party gets a fair share of information, and the computing power of each party can be fully leveraged to boost training efficiency. 一个基于XGBoost的混合联邦学习框架，用于从实时外部特征进行分布式电力预测。除了引入提升树来提高准确性和可解释性，我们还结合了横向和纵向的联邦学习，以解决特征分散在本地异质方和样本分散在不同本地区的情况。此外，我们设计了一个动态的任务分配方案，使每一方都能获得公平的信息份额，并能充分利用每一方的计算能力来提高训练效率。
[^Fed-TGAN]: 
[^MP-FedXGB]: MP-FedXGB, a lossless multi-party federated XGB learning framework is proposed with a security guarantee, which reshapes the XGBoost's split criterion calculation process under a secret sharing setting and solves the leaf weight calculation problem by leveraging distributed optimization.  MP-FedXGB是一个无损的多方联邦XGB学习框架，它在秘密共享的环境下重塑了XGBoost的分割准则计算过程，并通过利用分布式优化解决了叶子权重计算问题。
[^FedXGBoost]: Two variants of federated XGBoost with privacy guarantee: FedXGBoost-SMM and FedXGBoost-LDP. Our first protocol FedXGBoost-SMM deploys enhanced secure matrix multiplication method to preserve privacy with lossless accuracy and lower overhead than encryption-based techniques. Developed independently, the second protocol FedXGBoost-LDP is heuristically designed with noise perturbation for local differential privacy. 两种具有隐私保护的联邦XGBoost的变体：FedXGBoost-SMM和FedXGBoost-LDP。FedXGBoost-SMM部署了增强的安全矩阵乘法，以无损的精度和低于基于加密的技术的开销来保护隐私。第二个协议FedXGBoost-LDP以启发式方法设计的，带有噪声扰动，用于保护局部差分隐私。
[^EBHE-VFXGB]: Efficient XGBoost vertical federated learning. we proposed a novel batch homomorphic encryption method to cut the cost of encryption-related computation and transmission in nearly half. This is achieved by encoding the first-order derivative and the second-order derivative into a single number for encryption, ciphertext transmission, and homomorphic addition operations. The sum of multiple first-order derivatives and second-order derivatives can be simultaneously decoded from the sum of encoded values.  高效的XGBoost纵向联邦学习。我们提出了一种新颖的批量同态加密方法，将加密相关的计算和传输成本减少了近一半。这是通过将一阶导数和二阶导数编码为一个数字来实现的，用于加密、密码文本传输和同态加法操作。多个一阶导数和二阶导数的总和可以同时从编码值的总和中解密。
[^FL-XGBoost]: The proposed FL-XGBoost can train a sensitive task to be solved among different entities without revealing their own data. The proposed FL-XGBoost can achieve significant reduction in the number of communications between entities by exchanging decision tree models.  FL-XGBoost可以训练一个敏感的任务，在不同的实体之间解决，而不透露他们自己的数据。所提出的FL-XGBoost可以通过交换决策树模型实现实体之间通信数量的大幅减少。
[^F-XGBoost]: A horizontal federated XGBoost algorithm to solve the federated anomaly detection problem, where the anomaly detection aims to identify abnormalities from extremely unbalanced datasets and can be considered as a special classification problem. Our proposed federated XGBoost algorithm incorporates data aggregation and sparse federated update processes to balance the tradeoff between privacy and learning performance. In particular, we introduce the virtual data sample by aggregating a group of users' data together at a single distributed node. 一个横向联邦XGBoost算法来解决联邦异常检测问题，其中异常检测的目的是从极不平衡的数据集中识别异常，可以被视为一个特殊的分类问题。我们提出的联邦XGBoost算法包含了数据聚合和稀疏的联邦更新过程，以平衡隐私和学习性能之间的权衡。特别是，我们通过将一组用户的数据聚集在一个分布式节点上，引入虚拟数据样本。
[^FL-RF]: Random Forest Based on Federated Learning for Intrusion Detection 使用联邦随机森林做入侵检测
[^PEA]: We are motivated to resolve the above issue by proposing a solution, referred to as PEA (Private, Efficient, Accurate), which consists of a secure differentially private stochastic gradient descent (DPSGD for short) protocol and two optimization methods. First, we propose a secure DPSGD protocol to enforce DPSGD, which is a popular differentially private machine learning algorithm, in secret sharing-based MPL frameworks. Second, to reduce the accuracy loss led by differential privacy noise and the huge communication overhead of MPL, we propose two optimization methods for the training process of MPL. 提出一个安全差分隐私随机梯度下降协议以在基于秘密共享的安全多方学习框架中实现差分隐私随机梯度下降算法。为了降低差分隐私带来的精度损失并提升安全多方学习的效率，从安全多方学习训练过程的角度提出了两项优化方法，多方可以在MPL模型训练过程中平衡。做到隐私、效率和准确性三者之间的权衡。
[^FL-DT]: A federated decision tree-based random forest algorithm where a small number of organizations or industry companies collaboratively build models. 一个基于联邦决策树的随机森林算法，由少数组织或行业公司合作建立模型。
[^FL-ST]: We explore the threat of collusion attacks from multiple malicious clients who pose targeted attacks (e.g., label flipping) in a federated learning configuration. By leveraging client weights and the correlation among them, we develop a graph-based algorithm to detect malicious clients. 我们探讨了来自多个恶意客户的串通攻击的威胁，这些客户在联邦学习配置中提出了有针对性的攻击（例如，标签翻转）。通过利用客户端的权重和它们之间的关联性，我们开发了一种基于图的算法来检测恶意客户端。
[^FedSCR]: 
[^AUCTION]: 
[^DONE]: 
[^FlexCFL]: 
[^LightFed]: 
[^Deli-CoCo]: 
[^DPBFL]: 
[^SHFL]: 
[^WKAFL]: 
[^Overlap-FedAvg]: 
[^Astraea]: 
[^PoFL]: 
[^Biscotti]: 
[^MFL]: 
[^FPPDL]: 
[^BFF-IDS]: A Blockchain-Based Federated Forest for SDN-Enabled In-Vehicle Network Intrusion Detection System 基于区块链的联邦森林用于支持SDN的车载网络入侵检测系统
[^I-GBDT]: An improved gradient boosting decision tree (GBDT) federated ensemble learning method is proposed, which takes the average gradient of similar samples and its own gradient as a new gradient to improve the accuracy of the local model. Different ensemble learning methods are used to integrate the parameters of the local model, thus improving the accuracy of the updated global model. 提出了一种改进的梯度提升决策树（GBDT）联邦集合学习方法，该方法将相似样本的平均梯度和自身的梯度作为新的梯度来提高局部模型的精度。采用不同的集合学习方法来整合局部模型的参数，从而提高更新的全局模型的精度。
[^SecureBoost]: SecureBoost, a novel lossless privacy-preserving tree-boosting system. SecureBoost first conducts entity alignment under a privacy-preserving protocol and then constructs boosting trees across multiple parties with a carefully designed encryption strategy. This federated learning system allows the learning process to be jointly conducted over multiple parties with common user samples but different feature sets, which corresponds to a vertically partitioned data set. SecureBoost是一种新型的无损隐私保护的提升树系统。SecureBoost首先在一个保护隐私的协议下进行实体对齐，然后通过精心设计的加密策略在多方之间构建提升树。这种联邦学习系统允许学习过程在具有共同用户样本但不同特征集的多方联邦进行，这相当于一个纵向分割的数据集。
[^KA-FL]: A privacy-preserving framework using Mondrian k-anonymity with decision trees for the horizontally partitioned data. 使用Mondrian K-匿名化的隐私保护框架，对横向分割的数据使用决策树建模。
[^CB-DP]: Differential Privacy is used to obtain theoretically sound privacy guarantees against such inference attacks by noising the exchanged update vectors. However, the added noise is proportional to the model size which can be very large with modern neural networks. This can result in poor model quality. Compressive sensing is used to reduce the model size and hence increase model quality without sacrificing privacy.  差分隐私是通过对交换的更新向量进行噪声处理来获得理论上合理的隐私保证，以抵御这种推断攻击。然而，增加的噪声与模型大小成正比，而现代神经网络的模型大小可能非常大。这可能会导致模型质量不佳。压缩感知被用来减少模型大小，从而在不牺牲隐私的情况下提高模型质量。
[^FEDXGB]: FEDXGB, a federated extreme gradient boosting (XGBoost) scheme supporting forced aggregation. First, FEDXGB involves a new HE(homomorphic encryption) based secure aggregation scheme for FL. Then, FEDXGB extends FL to a new machine learning model by applying the secure aggregation scheme to the classification and regression tree building of XGBoost. FEDXGB，一个支持强制聚合的联邦极端梯度提升（XGBoost）方案。首先，FEDXGB涉及一个新的基于HE（同态加密）的FL的安全聚合方案。然后，FEDXGB通过将安全聚合方案应用于XGBoost的分类和回归树构建，将FL扩展到一个新的机器学习模型。
[^DFedForest]: A distributed machine learning system based on local random forest algorithms created with shared decision trees through the blockchain. 一个基于本地随机森林算法的分布式机器学习系统通过区块链创建了共享决策树。
[^FL-PON]: A bandwidth slicing algorithm in PONs(passive optical network) is introduced for efficient FL, in which bandwidth is reserved for the involved ONUs(optical network units) collaboratively and mapped into each polling cycle. 在PONs（无源光网络）中引入了一种高效的FL算法，即为参与的ONU（光网络单元）协同保留带宽并映射到每个轮询周期。
[^DRC-tree]: A decentralized redundant n-Cayley tree (DRC-tree) for federated learning. Explore the hierarchical structure of the n-Cayley tree to enhance the redundancy rate in federated learning to mitigate the impact of stragglers. In the DRC- tree structure, the fusion node serves as the root node, while all the worker devices are the intermediate tree nodes and leaves that formulated through a distributed message passing interface. the redundancy of workers is constructed layer by layer with a given redundancy branch degree. 用于联邦学习的分散冗余n-Cayley树（DRC-tree）。探索n-Cayley树的分层结构，提高联邦学习中的冗余率，以减轻散兵游勇的影响。在DRC-树结构中，融合节点作为根节点，而所有客户端设备是通过分布式消息传递接口制定的中间树节点和叶子。客户端的冗余度是以给定的冗余分支度逐层构建的。
[^Fed-sGBM]: Fed-sGBM, a federated soft gradient boosting machine framework applicable on the streaming data. Compared with traditional gradient boosting methods, where base learners are trained sequentially, each base learner in the proposed framework can be efficiently trained in a parallel and distributed fashion. Fed-sGBM是一个适用于流数据的联邦软梯度提升机框架。与传统的梯度提升方法相比，传统的梯度提升方法中的基础学习器是按顺序训练的，而拟议的框架中的每个基础学习器可以以平行和分布的方式有效地训练。
[^FL-DNDF]: Deep neural decision forests (DNDF), combine the divide-and-conquer principle together with the property representation learning. By parameterizing the probability distributions in the prediction nodes of the forest, and include all trees of the forest in the loss function, a gradient of the whole forest can be computed which some/several federated learning algorithms utilize. 深度神经决策森林（DNDF），将分治策略与属性表示学习结合起来。通过对森林预测节点的概率分布进行参数化，并将森林中的所有树木纳入损失函数中，可以计算出整个森林的梯度，一些/一些联邦学习算法利用了这一梯度。
[^SemiGraphFL]: This work focuses on the graph classification task with partially labeled data. (1) Enhancing the collaboration processes: We propose a new personalized FL framework to deal with Non-IID data. Clients with more similar data have greater mutual influence, where the similarities can be evaluated via unlabeled data. (2) Enhancing the local training process: We introduce auxiliary loss for unlabeled data that restrict the training process. We propose a new pseudo-label strategy for our SemiGraphFL framework to make more effective predictions. 这项工作专注于具有部分标记数据的图分类任务。(1) 加强合作过程。我们提出了一个新的个性化的FL框架来处理非IID数据。拥有更多相似数据的客户有更大的相互影响，其中的相似性可以通过未标记的数据进行评估。(2) 加强本地训练过程。我们为未标记的数据引入了辅助损失，限制了训练过程。我们为我们的SemiGraphFL框架提出了一个新的伪标签策略，以做出更有效的预测。
[^FedEC]: FedEC framework, a local training procedure is responsible for learning knowledge graph embeddings on each client based on a specific embedding learner. We apply embedding-contrastive learning to limit the embedding update for tackling data heterogeneity. Moreover, a global update procedure is used for sharing and averaging entity embeddings on the master server.  在FedEC框架中，一个本地训练程序负责在每个客户端上基于特定的嵌入学习者学习知识图的嵌入。我们应用嵌入对比学习来限制嵌入的更新，以解决数据的异质性问题。此外，全局更新程序被用于共享和平均主服务器上的实体嵌入。
[^DP-FedRec]: The DP-based federated GNN has not been well investigated, especially in the sub-graph-level setting, such as the scenario of recommendation system. DP-FedRec, a DP-based federated GNN to fill the gap. Private Set Intersection (PSI) is leveraged to extend the local graph for each client, and thus solve the non-IID problem. Most importantly, DP(differential privacy) is applied not only on the weights but also on the edges of the intersection graph from PSI to fully protect the privacy of clients. 基于DP的联合GNN还没有得到很好的研究，特别是在子图层面的设置，如推荐系统的场景。DP-FedRec，一个基于DP的联盟式GNN来填补这一空白。隐私集合求交（PSI）被用来扩展每个客户端的本地图，从而解决非IID问题。最重要的是，DP（差分隐私）不仅适用于权重，也适用于PSI中交集图的边，以充分保护客户的隐私。
[^wirelessfl-pdgnet]: A data-driven approach for power allocation in the context of federated learning (FL) over interference-limited wireless networks. The power policy is designed to maximize the transmitted information during the FL process under communication constraints, with the ultimate objective of improving the accuracy and efficiency of the global FL model being trained. The proposed power allocation policy is parameterized using a graph convolutional network and the associated constrained optimization problem is solved through a primal-dual algorithm. 在干扰有限的无线网络上联邦学习（FL）的背景下，一种数据驱动的功率分配方法。功率策略的设计是为了在通信约束下的联邦学习过程中最大化传输信息，其最终目的是提高正在训练的全局联邦学习模型的准确性和效率。所提出的功率分配策略使用图卷积网络进行参数化，相关的约束性优化问题通过原始-双重算法进行解决。
[^multitask-fusion]: We investigate multi-task learning (MTL), where multiple learning tasks are performed jointly rather than separately to leverage their similarities and improve performance. We focus on the federated multi-task linear regression setting, where each machine possesses its own data for individual tasks and sharing the full local data between machines is prohibited. Motivated by graph regularization, we propose a novel fusion framework that only requires a one-shot communication of local estimates. Our method linearly combines the local estimates to produce an improved estimate for each task, and we show that the ideal mixing weight for fusion is a function of task similarity and task difficulty. 我们研究了多任务学习（MTL），其中多个学习任务被关联而不是单独执行，以利用它们的相似性并提高性能。我们专注于联邦多任务线性回归的设置，其中每台机器拥有自己的个别任务的数据，并且禁止在机器之间共享完整的本地数据。在图正则化的启发下，我们提出了一个新的融合框架，只需要一次本地估计的交流。我们的方法线性地结合本地估计，为每个任务产生一个改进的估计，我们表明，融合的理想混合权重是任务相似性和任务难度的函数。
[^PNS-FGL]: Existing FL paradigms are inefficient for geo-distributed GCN training since neighbour sampling across geo-locations will soon dominate the whole training process and consume large WAN bandwidth. We derive a practical federated graph learning algorithm, carefully striking the trade-off among GCN convergence error, wall-clock runtime, and neighbour sampling interval. Our analysis is divided into two cases according to the budget for neighbour sampling. In the unconstrained case, we obtain the optimal neighbour sampling interval, that achieves the best trade-off between convergence and runtime; in the constrained case, we show that determining the optimal sampling interval is actually an online problem and we propose a novel online algorithm with bounded competitive ratio to solve it. Combining the two cases, we propose a unified algorithm to decide the neighbour sampling interval in federated graph learning, and demonstrate its effectiveness with extensive simulation over graph datasets. 现有的FL范式对于地理分布式的GCN训练是低效的，因为跨地理位置的近邻采样很快将主导整个训练过程，并消耗大量的广域网带宽。我们推导了一个实用的联邦图学习算法，仔细权衡了GCN收敛误差、wall - clock运行时间和近邻采样间隔。我们的分析根据邻居抽样的预算分为两种情况。在无约束的情况下，我们得到了最优的近邻采样间隔，实现了收敛性和运行时间的最佳折衷；在有约束的情况下，我们证明了确定最优采样间隔实际上是一个在线问题，并提出了一个新的有界竞争比的在线算法来解决这个问题。结合这两种情况，我们提出了一个统一的算法来决定联合图学习中的近邻采样间隔，并通过在图数据集上的大量仿真证明了其有效性
[^GraphSniffer]: A graph neural network model based on federated learning named GraphSniffer to identify malicious transactions in the digital currency market. GraphSniffer leverages federated learning and graph neural networks to model graph-structured Bitcoin transaction data distributed at different worker nodes, and transmits the gradients of the local model to the server node for aggregation to update the parameters of the global model. GraphSniffer 一种基于联邦学习的图神经网络模型来识别数字货币市场中的恶意交易。GraphSniffer 利用联邦学习和图神经网络对分布在不同工作节点的图结构比特币交易数据进行建模，并将局部模型的梯度传递到服务器节点进行聚合，更新全局模型的参数。
[^ML-FGL]: Deep learning-based Wi-Fi indoor fingerprint localization, which requires a large received signal strength (RSS) dataset for training. A multi-level federated graph learning and self-attention based personalized indoor localization method is proposed to further capture the intrinsic features of RSS(received signal strength), and learn the aggregation manner of shared information uploaded by clients, with better personalization accuracy. 基于深度学习的Wi-Fi室内指纹定位，需要一个大的接收信号强度( RSS )数据集进行训练。为了进一步捕获RSS(接收信号强度)的内在特征，学习客户端上传的共享信息的聚合方式，具有更好的个性化精度，提出了一种基于多级联邦图学习和自注意力机制的个性化室内定位方法。
[^PSO-GFML]: This paper proposes a decentralized online multitask learning algorithm based on GFL (O-GFML). Clients update their local models using continuous streaming data while clients and multiple servers can train different but related models simul-taneously. Furthermore, to enhance the communication efficiency of O-GFML, we develop a partial-sharing-based O-GFML (PSO-GFML). The PSO-GFML allows participating clients to exchange only a portion of model parameters with their respective servers during a global iteration, while non-participating clients update their local models if they have access to new data. 本文提出了一种基于GFL (O-GFML)的去中心化在线多任务学习算法。客户端使用连续的流数据更新本地模型，而客户端和多个服务器可以同时训练不同但相关的模型。此外，为了提高O-GFML的通信效率，我们开发了一种基于部分共享的O-GFML (PSO-GFML)。PSO-GFML允许参与的客户端在全局迭代过程中只与各自的服务器交换部分模型参数，而非参与的客户端在有机会获得新数据的情况下更新本地模型。
[^DNG-FR]: AI healthcare applications rely on sensitive electronic healthcare  records (EHRs) that are scarcely labelled and are often distributed  across a network of the symbiont institutions. In this work,  we propose  dynamic neural graphs based federated learning framework to address these challenges. The proposed framework extends  Reptile , a model agnostic meta-learning (MAML) algorithm, to a federated  setting. However, unlike the existing MAML algorithms, this paper  proposes a dynamic variant of neural graph learning (NGL) to incorporate unlabelled examples in the supervised training setup. Dynamic NGL  computes a meta-learning update by performing supervised learning on a  labelled training example while performing metric learning on its  labelled or unlabelled neighbourhood. This neighbourhood of a labelled  example is established dynamically using local graphs built over the  batches of training examples. Each local graph is constructed by  comparing the similarity between embedding generated by the current  state of the model. The introduction of metric learning on the  neighbourhood makes this framework semi-supervised in nature. The  experimental results on the publicly available MIMIC-III dataset  highlight the effectiveness of the proposed framework for both single  and multi-task settings under data decentralisation constraints and  limited supervision. 人工智能医疗应用依赖于敏感的电子医疗记录( EHR )，这些记录几乎没有标签，而且往往分布在共生体机构的网络中。在这项工作中，我们提出了基于动态神经图的联邦学习框架来解决这些挑战。提出的框架将模型不可知元学习(MAML)算法Reptile扩展到联邦环境。然而，与现有的MAML算法不同，本文提出了神经图学习(Neural Graph Learning，NGL 的动态变体，以在有监督的训练设置中纳入未标记的示例。动态NGL通过对带标签的训练示例执行监督学习，同时对其带标签或未带标签的邻域执行度量学习来计算元学习更新。标记样本的这个邻域是使用在批量训练样本上建立的局部图动态建立的。通过比较由模型的当前状态生成的嵌入之间的相似性来构造每个局部图。在邻域上引入度量学习使得这个框架具有半监督的性质。
[^BOFRF]: A novel federated ensemble classification algorithm for horizontally partitioned data, namely Boosting-based Federated Random Forest (BOFRF), which not only increases the predictive power of all participating sites, but also provides significantly high improvement on the predictive power of sites having unsuccessful local models. We implement a federated version of random forest, which is a well-known bagging algorithm, by adapting the idea of boosting to it. We introduce a novel aggregation and weight calculation methodology that assigns weights to local classifiers based on their classification performance at each site without increasing the communication or computation cost. 一种针对横向划分数据的新型联邦集成分类算法，即基于 Boosting 的联邦随机森林 (BOFRF)，它不仅提高了所有参与站点的预测能力，而且显着提高了局部模型不成功的站点的预测能力 . 我们通过采用 boosting 的思想来实现一个联邦版本的随机森林，这是一种众所周知的 bagging 算法。 我们引入了一种新颖的聚合和权重计算方法，该方法根据本地分类器在每个站点的分类性能为它们分配权重，而不会增加通信或计算成本。
[^FedDis]: With the advent of deep learning and increasing use of brain MRIs, a great amount of interest has arisen in automated anomaly segmentation to improve clinical workflows; however, it is time-consuming and expensive to curate medical imaging. FedDis to collaboratively train an unsupervised deep convolutional autoencoder on 1,532 healthy magnetic resonance scans from four different institutions, and evaluate its performance in identifying pathologies such as multiple sclerosis, vascular lesions, and low- and high-grade tumours/glioblastoma on a total of 538 volumes from six different institutions. To mitigate the statistical heterogeneity among different institutions, we disentangle the parameter space into global (shape) and local (appearance). Four institutes jointly train shape parameters to model healthy brain anatomical structures. Every institute trains appearance parameters locally to allow for client-specific personalization of the global domain-invariant features. 随着深度学习的出现和脑 MRI 的使用越来越多，人们对自动异常分割以改善临床工作流程产生了极大的兴趣。然而，管理医学成像既耗时又昂贵。 FedDis 将在来自四个不同机构的 1,532 次健康磁共振扫描上协作训练一个无监督的深度卷积自动编码器，并评估其在总共 538 个机构中识别多发性硬化症、血管病变以及低级别和高级别肿瘤/胶质母细胞瘤等病理的性能来自六个不同机构的卷。为了减轻不同机构之间的统计异质性，我们将参数空间分解为全局（形状）和局部（外观）。四个研究所联合训练形状参数来模拟健康的大脑解剖结构。每个机构都在本地训练外观参数，以允许对全局域不变特征进行客户特定的个性化。
[^Heter-aware]: This paper carries out the first empirical study to characterize the impacts of heterogeneity in FL based on large-scale data from 136k smartphones that can faithfully reflect heterogeneity in real-world settings. This paper also builds a heterogeneity-aware FL platform that complies with the standard FL protocol but with heterogeneity in consideration.本文第一次通过通过136,000台数据收集到的数据上的实证研究来描述联邦学习中异质性的影响，并构建了一个数据异质性感知的联邦学习平台。
